"""
Task Vector Loss Landscape Explorer
Inspired by "Walks in Rotation Spaces Return Home when Doubled and Scaled"
(Eckmann & Tlusty, 2025, arXiv:2502.14367v3)

MATHEMATICAL BACKGROUND:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Eckmann & Tlusty prove that for rotation groups SO(3)/SU(2), almost any walk W
can be scaled to reach a 180¬∞ rotation, which when squared returns to identity:
[W(Œª)]¬≤ = I for abundant Œª values.

Key insight: 180¬∞ rotations are common in rotation space (f‚ÇÅ(œÄ) = 2/œÄ), and
squaring them gives identity: R(n,œÄ)¬≤ = I.

THE SIMPLE QUESTION:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
What does the loss landscape look like along the task vector direction?

THE EXPERIMENT:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
1. Create task vector: T = M_finetuned - M_base
2. Sweep Œ± from -3.0 to 3.0 (100 samples)
3. For each Œ±: compute M(Œ±) = M_base + Œ±T
4. Evaluate loss: L(Œ±) = loss of M(Œ±)
5. Plot L(Œ±) vs Œ±

WHAT WE'RE LOOKING FOR:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚Ä¢ Does L(Œ±) cross L(M_base) at any Œ± ‚â† 0? ("zero-crossings")
‚Ä¢ What is the minimum loss Œ±?
‚Ä¢ Is the landscape monotonic? Periodic? Symmetric?

If zero-crossings exist ‚Üí suggests special scaling factors (analogous to 180¬∞ rotations)
If monotonic ‚Üí task vectors lack rotation-like symmetry

CONNECTION TO PAPER:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚ö†Ô∏è  Rotations: Multiplicative group (W‚ÇÅ ‚àò W‚ÇÇ)
‚ö†Ô∏è  Task vectors: Additive vector space (v‚ÇÅ + v‚ÇÇ)
‚ö†Ô∏è  No group structure ‚Üí no proven abundance of special Œ±
‚ö†Ô∏è  This is EMPIRICAL exploration, not a theorem

PRACTICAL APPLICATIONS:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚Ä¢ Find optimal Œ± for task performance
‚Ä¢ Discover special Œ± for model merging
‚Ä¢ Understand loss landscape geometry
‚Ä¢ Guide task vector arithmetic
"""

import argparse
import json
import os
import time
from dataclasses import asdict, dataclass, field
from datetime import datetime
from typing import cast

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    DataCollatorForLanguageModeling,
    PreTrainedModel,
    Trainer,
    TrainerCallback,
    TrainingArguments,
)


def get_device_map():
    """Get appropriate device_map for model loading based on available hardware.

    Returns:
        str or dict: Device map configuration for transformers models.
        - "auto" for CUDA (multi-GPU support)
        - None for MPS or CPU (load to default device)
    """
    if torch.cuda.is_available():
        return "auto"  # CUDA supports automatic device mapping
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        return None  # MPS: load to default device, then move manually
    else:
        return None  # CPU: load to default device


def save_models_for_analysis(
    base_model: PreTrainedModel,
    finetuned_model: PreTrainedModel,
    output_dir: str,
) -> None:
    """Save base and fine-tuned models for later analysis.

    Args:
        base_model: Base model to save
        finetuned_model: Fine-tuned model to save
        output_dir: Directory to save models
    """
    base_path = os.path.join(output_dir, "saved_base_model")
    ft_path = os.path.join(output_dir, "saved_finetuned_model")

    print(f"\nSaving models for future analysis...")
    print(f"  Base model ‚Üí {base_path}")
    print(f"  Fine-tuned model ‚Üí {ft_path}")

    # Save models (saves config, weights, etc.)
    base_model.save_pretrained(base_path)
    finetuned_model.save_pretrained(ft_path)

    print("Models saved successfully!\n")


def load_models_for_analysis(
    model_name: str,
    output_dir: str,
    device: str,
) -> tuple[PreTrainedModel, PreTrainedModel]:
    """Load previously saved models for analysis.

    Args:
        model_name: Name of the base model (for tokenizer compatibility)
        output_dir: Directory containing saved models
        device: Device to load models to

    Returns:
        Tuple of (base_model, finetuned_model)
    """
    base_path = os.path.join(output_dir, "saved_base_model")
    ft_path = os.path.join(output_dir, "saved_finetuned_model")

    print(f"\nLoading previously saved models...")
    print(f"  Base model ‚Üê {base_path}")
    print(f"  Fine-tuned model ‚Üê {ft_path}")

    # Load models
    base_model = AutoModelForCausalLM.from_pretrained(
        base_path, torch_dtype=torch.bfloat16, device_map=None
    )
    finetuned_model = AutoModelForCausalLM.from_pretrained(
        ft_path, torch_dtype=torch.bfloat16, device_map=None
    )

    # Move to device
    base_model = base_model.to(device)  # type: ignore[assignment]
    finetuned_model = finetuned_model.to(device)  # type: ignore[assignment]

    print("Models loaded successfully!\n")

    return base_model, finetuned_model


def check_saved_models_exist(output_dir: str) -> bool:
    """Check if saved models exist in the output directory.

    Args:
        output_dir: Directory to check for saved models

    Returns:
        True if both base and fine-tuned models exist, False otherwise
    """
    base_path = os.path.join(output_dir, "saved_base_model")
    ft_path = os.path.join(output_dir, "saved_finetuned_model")

    base_exists = os.path.exists(os.path.join(base_path, "config.json"))
    ft_exists = os.path.exists(os.path.join(ft_path, "config.json"))

    return base_exists and ft_exists


@dataclass
class AlphaSweepResult:
    """Store results for a specific Œ± value along the line M(Œ±) = M_base + Œ±T.

    This directly answers: "What does the loss look like as we move along
    the task vector direction?"
    """

    alpha: float  # Scaling factor Œ±
    loss: float  # L(M_base + Œ±T)
    base_loss: float  # L(M_base) - reference point
    functional_return: float  # |L(M_base + Œ±T) - L(M_base)|
    task_performance: float  # Task-specific loss at M_base + Œ±T


@dataclass
class ExperimentMetrics:
    """Comprehensive metrics for the entire experiment."""

    start_time: str
    end_time: str = ""
    duration_seconds: float = 0.0

    # Model info
    model_name: str = ""
    device: str = ""
    model_parameters: int = 0

    # Fine-tuning metrics
    finetuning_start_time: str = ""
    finetuning_end_time: str = ""
    finetuning_duration_seconds: float = 0.0
    training_examples: int = 0
    num_epochs: int = 0
    learning_rate: float = 0.0
    final_training_loss: float = 0.0
    training_steps: int = 0
    training_history: list[dict] = field(default_factory=list)

    # Task vector metrics
    task_vector_magnitude: float = 0.0
    task_vector_computation_time: float = 0.0

    # Alpha sweep metrics
    sweep_start_time: str = ""
    sweep_end_time: str = ""
    sweep_duration_seconds: float = 0.0
    num_alpha_samples: int = 0
    alpha_range: tuple[float, float] = (-3.0, 3.0)
    time_per_alpha_seconds: float = 0.0

    # Results summary
    min_general_loss_alpha: float = 0.0
    min_general_loss: float = 0.0
    min_task_loss_alpha: float = 0.0
    min_task_loss: float = 0.0
    num_zero_crossings: int = 0
    zero_crossing_alphas: list[float] = field(default_factory=list)


class FineTuningProgressCallback(TrainerCallback):
    """Custom callback for detailed fine-tuning progress reporting."""

    def __init__(self):
        self.training_history = []
        self.epoch_start_time = None
        self.step_times = []

    def on_epoch_begin(self, args, state, control, **kwargs):
        self.epoch_start_time = time.time()
        epoch_num = int(state.epoch) if state.epoch else 0
        print(f"\n{'‚îÄ'*60}")
        print(f"Epoch {epoch_num + 1}/{args.num_train_epochs}")
        print(f"{'‚îÄ'*60}")

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs:
            step_time = time.time()
            self.step_times.append(step_time)

            # Calculate ETA
            if len(self.step_times) > 1 and state.max_steps:
                avg_step_time = (
                    self.step_times[-1] - self.step_times[0]
                ) / len(self.step_times)
                remaining_steps = state.max_steps - state.global_step
                eta_seconds = avg_step_time * remaining_steps
                eta_str = f"{eta_seconds / 60:.1f}m" if eta_seconds > 60 else f"{eta_seconds:.0f}s"
            else:
                eta_str = "calculating..."

            # Record history
            log_entry = {
                "step": state.global_step,
                "epoch": state.epoch,
                "timestamp": datetime.now().isoformat(),
                **logs,
            }
            self.training_history.append(log_entry)

            # Print progress
            print(f"  Step {state.global_step}/{state.max_steps} | ", end="")
            if "loss" in logs:
                print(f"Loss: {logs['loss']:.4f} | ", end="")
            if "learning_rate" in logs:
                print(f"LR: {logs['learning_rate']:.2e} | ", end="")
            if "grad_norm" in logs:
                print(f"Grad: {logs['grad_norm']:.2f} | ", end="")
            print(f"ETA: {eta_str}")

    def on_epoch_end(self, args, state, control, **kwargs):
        if self.epoch_start_time:
            epoch_duration = time.time() - self.epoch_start_time
            epoch_num = int(state.epoch) if state.epoch else 0
            print(f"{'‚îÄ'*60}")
            print(f"Epoch {epoch_num} completed in {epoch_duration:.1f}s")
            print(f"{'‚îÄ'*60}\n")


def compute_task_vector(
    base_model: PreTrainedModel, finetuned_model: PreTrainedModel
) -> dict[str, torch.Tensor]:
    """Compute task vector: T = M_finetuned - M_base

    Handles models on different devices by moving parameters to CPU before subtraction.
    """
    task_vector = {}

    for (name, base_param), (_, ft_param) in zip(
        base_model.named_parameters(), finetuned_model.named_parameters(), strict=True
    ):
        # Check for meta device - this indicates improper materialization
        if ft_param.device.type == 'meta':
            raise RuntimeError(
                f"Parameter '{name}' in fine-tuned model is on meta device. "
                "This happens with device_map='auto' and model offloading. "
                "Load the fine-tuning model with device_map=None instead."
            )
        if base_param.device.type == 'meta':
            raise RuntimeError(
                f"Parameter '{name}' in base model is on meta device. "
                "Load the model with device_map=None instead."
            )

        # Move both to CPU first to handle device mismatches (e.g., cuda vs cpu)
        base_cpu = base_param.detach().cpu()
        ft_cpu = ft_param.detach().cpu()
        task_vector[name] = (ft_cpu - base_cpu).clone()

    return task_vector


def evaluate_model(
    model: nn.Module, tokenizer, eval_texts: list[str], device: str = "cuda"
) -> float:
    """Evaluate model perplexity on evaluation texts"""
    model.eval()
    total_loss = 0.0
    count = 0

    with torch.no_grad():
        for text in eval_texts:
            inputs = tokenizer(
                text, return_tensors="pt", truncation=True, max_length=512, padding=True
            )
            inputs = {k: v.to(device) for k, v in inputs.items()}

            outputs = model(**inputs, labels=inputs["input_ids"])
            total_loss += outputs.loss.item()
            count += 1

    return total_loss / count


def evaluate_task_performance(
    model: nn.Module, tokenizer, task_eval_texts: list[str], device: str = "cuda"
) -> float:
    """
    Evaluate how well the model performs on the specific task.
    Lower is better for loss-based metrics.
    """
    return evaluate_model(model, tokenizer, task_eval_texts, device)


def fine_tune_model(
    base_model: PreTrainedModel,
    tokenizer,
    train_texts: list[str],
    output_dir: str = "./finetuned_model",
    num_epochs: int = 3,
    learning_rate: float = 5e-5,
) -> tuple[PreTrainedModel, dict]:
    """Fine-tune model on task to create a structured task vector.

    Args:
        base_model: Pre-trained base model
        tokenizer: Associated tokenizer
        train_texts: Training examples for the task
        output_dir: Directory for checkpoints
        num_epochs: Number of training epochs
        learning_rate: Learning rate for fine-tuning

    Returns:
        Tuple of (Fine-tuned model, metrics dict)
    """
    ft_start_time = time.time()

    print("\n" + "="*70)
    print("FINE-TUNING MODEL")
    print("="*70)
    print(f"  Training examples: {len(train_texts)}")
    print(f"  Epochs: {num_epochs}")
    print(f"  Learning rate: {learning_rate:.2e}")
    print(f"  Batch size: 1")
    print(f"  Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # Prepare dataset
    class TextDataset(Dataset):
        """Dataset wrapper for tokenizing text inputs for language model training."""

        def __init__(self, texts, tokenizer, max_length=512):
            self.encodings = tokenizer(
                texts,
                truncation=True,
                padding=True,
                max_length=max_length,
                return_tensors="pt",
            )

        def __len__(self):
            return len(self.encodings.input_ids)

        def __getitem__(self, idx):
            return {
                "input_ids": self.encodings.input_ids[idx],
                "attention_mask": self.encodings.attention_mask[idx],
            }

    dataset = TextDataset(train_texts, tokenizer)

    # Training arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=num_epochs,
        per_device_train_batch_size=1,  # Reduced from 4 for 12B model
        learning_rate=learning_rate,
        save_strategy="no",  # Disable all checkpoint saving
        logging_steps=10,  # More frequent logging for better progress tracking
        report_to=[],  # Disable wandb/tensorboard
        remove_unused_columns=False,
        gradient_checkpointing=True,  # Save memory for large model
        fp16=False,  # Using bfloat16 instead
        bf16=True,  # Match model's native precision
    )

    # Data collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, mlm=False  # Causal LM, not masked LM
    )

    # Progress callback
    progress_callback = FineTuningProgressCallback()

    # Trainer
    trainer = Trainer(
        model=base_model,
        args=training_args,
        train_dataset=dataset,
        data_collator=data_collator,
        callbacks=[progress_callback],
    )

    # Train
    train_result = trainer.train()

    ft_end_time = time.time()
    ft_duration = ft_end_time - ft_start_time

    # Collect metrics
    metrics = {
        "start_time": datetime.fromtimestamp(ft_start_time).isoformat(),
        "end_time": datetime.fromtimestamp(ft_end_time).isoformat(),
        "duration_seconds": ft_duration,
        "training_examples": len(train_texts),
        "num_epochs": num_epochs,
        "learning_rate": learning_rate,
        "final_loss": train_result.training_loss,
        "training_steps": train_result.global_step,
        "training_history": progress_callback.training_history,
    }

    print(f"\n{'='*70}")
    print("FINE-TUNING COMPLETE")
    print(f"{'='*70}")
    print(f"  Duration: {ft_duration / 60:.1f} minutes ({ft_duration:.0f}s)")
    print(f"  Final loss: {train_result.training_loss:.4f}")
    print(f"  Total steps: {train_result.global_step}")
    print(f"  Avg time/step: {ft_duration / train_result.global_step:.2f}s")
    print(f"{'='*70}\n")

    return cast(PreTrainedModel, trainer.model), metrics


def sweep_alpha_values(
    base_model: PreTrainedModel,
    task_vector: dict[str, torch.Tensor],
    tokenizer,
    general_eval_texts: list[str],
    task_eval_texts: list[str],
    alpha_range: tuple[float, float] = (-3.0, 3.0),
    num_samples: int = 100,
    device: str = "cuda",
) -> list[AlphaSweepResult]:
    """Sweep Œ± values and plot loss landscape L(M_base + Œ±T).

    MEMORY-EFFICIENT VERSION: Reuses a single model instance and modifies
    parameters in-place. This is 10-100x faster than reloading the model
    for each Œ± value, especially for large models (e.g., 12B parameters).

    Args:
        base_model: Base model M_base
        task_vector: Task vector T = M_finetuned - M_base
        tokenizer: Tokenizer for evaluation
        general_eval_texts: Neutral evaluation texts
        task_eval_texts: Task-specific evaluation texts
        alpha_range: Range of Œ± values to test
        num_samples: Number of Œ± samples
        device: Device for computation

    Returns:
        List of AlphaSweepResult objects with loss for each Œ±
    """

    sweep_start_time = time.time()

    print(f"\n{'='*70}")
    print("LOSS LANDSCAPE SWEEP: L(M_base + Œ±T)")
    print(f"{'='*70}")
    print(f"Range: Œ± ‚àà [{alpha_range[0]:.1f}, {alpha_range[1]:.1f}]")
    print(f"Samples: {num_samples}")
    print(f"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("\nQuestion: Does the loss curve cross L(M_base) at any Œ± ‚â† 0?\n")

    alpha_values = np.linspace(alpha_range[0], alpha_range[1], num_samples)
    results = []
    alpha_times = []

    # Move model to device and set to eval mode
    base_model = base_model.to(device)  # type: ignore[assignment]
    base_model.eval()

    # Clone original parameters once (memory-efficient: only stores base params)
    print("Cloning base model parameters...")
    original_params = {}
    for name, param in base_model.named_parameters():
        if name in task_vector:
            original_params[name] = param.clone().detach()

    # Compute base model loss for reference
    print("Computing base model loss...")
    base_loss = evaluate_model(base_model, tokenizer, general_eval_texts, device)
    print(f"Base model loss L(M_base): {base_loss:.4f}\n")

    for i, alpha in enumerate(alpha_values):
        alpha_start = time.time()

        # Calculate ETA
        if alpha_times:
            avg_time = sum(alpha_times) / len(alpha_times)
            remaining = num_samples - i
            eta_seconds = avg_time * remaining
            eta_str = f"ETA: {eta_seconds / 60:.1f}m" if eta_seconds > 60 else f"ETA: {eta_seconds:.0f}s"
        else:
            eta_str = "ETA: calculating..."

        # Progress percentage
        progress_pct = (i / num_samples) * 100

        print(f"[{i+1:3d}/{num_samples}] ({progress_pct:5.1f}%) Œ± = {alpha:+.3f} | ", end="", flush=True)

        # Modify model parameters in-place: M_alpha = M_base + Œ±T
        with torch.no_grad():
            for name, param in base_model.named_parameters():
                if name in task_vector:
                    # Set param to: original + Œ± * task_vector
                    param.copy_(
                        original_params[name]
                        + alpha * task_vector[name].to(param.device)
                    )

        # Evaluate L(M_alpha)
        loss_alpha = evaluate_model(base_model, tokenizer, general_eval_texts, device)

        # Compute functional return |L(M_alpha) - L(M_base)|
        functional_return = abs(loss_alpha - base_loss)

        # Task performance
        task_performance = evaluate_task_performance(
            base_model, tokenizer, task_eval_texts, device
        )

        result = AlphaSweepResult(
            alpha=alpha,
            loss=loss_alpha,
            base_loss=base_loss,
            functional_return=functional_return,
            task_performance=task_performance,
        )
        results.append(result)

        alpha_elapsed = time.time() - alpha_start
        alpha_times.append(alpha_elapsed)

        print(f"L(Œ±)={loss_alpha:.4f}, |ŒîL|={functional_return:.4f} | {alpha_elapsed:.1f}s | {eta_str}")

    # Restore original parameters at the end
    print("\nRestoring base model parameters...")
    with torch.no_grad():
        for name, param in base_model.named_parameters():
            if name in original_params:
                param.copy_(original_params[name])

    sweep_end_time = time.time()
    sweep_duration = sweep_end_time - sweep_start_time
    avg_time_per_sample = sweep_duration / num_samples if num_samples > 0 else 0.0

    print(f"\n{'='*70}")
    print("ALPHA SWEEP COMPLETE")
    print(f"{'='*70}")
    print(f"  Duration: {sweep_duration / 60:.1f} minutes ({sweep_duration:.0f}s)")
    print(f"  Samples: {num_samples}")
    print(f"  Avg time/sample: {avg_time_per_sample:.2f}s")
    print(f"{'='*70}\n")

    # Return results with timing metadata
    return results, {
        "start_time": datetime.fromtimestamp(sweep_start_time).isoformat(),
        "end_time": datetime.fromtimestamp(sweep_end_time).isoformat(),
        "duration_seconds": sweep_duration,
        "time_per_alpha_seconds": avg_time_per_sample,
    }


def analyze_results(results: list[AlphaSweepResult]) -> dict:
    """Analyze loss landscape sweep results.

    Find:
    - Minimum general loss and corresponding Œ±
    - Minimum task-specific loss and corresponding Œ±
    - Zero-crossings: where L(Œ±) ‚âà L(M_base) for Œ± ‚â† 0
    - Special Œ± values with small functional return

    Returns dict with analysis results.
    """

    # Sort by general loss, task loss, and functional return
    sorted_by_loss = sorted(results, key=lambda r: r.loss)
    sorted_by_task_loss = sorted(results, key=lambda r: r.task_performance)
    sorted_by_return = sorted(results, key=lambda r: r.functional_return)

    # Find zero-crossings (where L(Œ±) crosses L_base, excluding Œ± near 0)
    zero_crossings = []
    threshold = 0.1  # Functional return threshold for "crossing"

    for result in results:
        if abs(result.alpha) > 0.15 and result.functional_return < threshold:
            zero_crossings.append(result)

    print(f"\n{'='*70}")
    print("LOSS LANDSCAPE ANALYSIS")
    print(f"{'='*70}\n")

    # Minimum general loss
    min_general_result = sorted_by_loss[0]
    print("Minimum General Loss (best general knowledge):")
    print(f"  Œ± = {min_general_result.alpha:+.4f}")
    print(f"  L(Œ±) = {min_general_result.loss:.4f}")
    print(f"  L(M_base) = {min_general_result.base_loss:.4f}")
    print(f"  Œî = {min_general_result.loss - min_general_result.base_loss:+.4f}\n")

    # Minimum task-specific loss
    min_task_result = sorted_by_task_loss[0]
    print("Minimum Task-Specific Loss (best task performance):")
    print(f"  Œ± = {min_task_result.alpha:+.4f}")
    print(f"  Task L(Œ±) = {min_task_result.task_performance:.4f}")
    print(f"  General L(Œ±) = {min_task_result.loss:.4f}")
    print(
        f"  Œî from base = {min_task_result.task_performance - min_task_result.base_loss:+.4f}\n"
    )

    # Best functional return (smallest |L(Œ±) - L_base|)
    print("Best Functional Return (smallest |L(Œ±) - L_base|):")
    for i, result in enumerate(sorted_by_return[:5], 1):
        print(f"  {i}. Œ± = {result.alpha:+.4f}, |ŒîL| = {result.functional_return:.6f}")

    # Zero-crossings
    print("\nZero-Crossings (where L(Œ±) ‚âà L_base for Œ± ‚â† 0):")
    if zero_crossings:
        print(f"  Found {len(zero_crossings)} crossing(s):")
        for i, result in enumerate(zero_crossings[:5], 1):
            print(
                f"  {i}. Œ± = {result.alpha:+.4f}, L(Œ±) = {result.loss:.4f}, "
                f"|ŒîL| = {result.functional_return:.6f} ‚òÖ"
            )
    else:
        print(f"  No zero-crossings found (threshold: |ŒîL| < {threshold})")
        print("  ‚Üí Loss is monotonic along task vector direction")

    return {
        "min_general_loss": min_general_result,
        "min_task_loss": min_task_result,
        "best_return": sorted_by_return[0],
        "zero_crossings": zero_crossings,
        "sorted_by_return": sorted_by_return[:10],
        "all_results": results,
    }


def generate_markdown_report(
    experiment_metrics: ExperimentMetrics,
    analysis: dict,
    results: list[AlphaSweepResult],
    output_path: str = "experiment_report.md",
) -> None:
    """Generate a comprehensive Markdown report for easy sharing and analysis."""

    min_general = analysis["min_general_loss"]
    min_task = analysis["min_task_loss"]
    best_return = analysis["best_return"]
    zero_crossings = analysis["zero_crossings"]

    report = f"""# Task Vector Loss Landscape Experiment Report

**Inspired by:** Eckmann & Tlusty (2025), "Walks in Rotation Spaces Return Home when Doubled and Scaled" (arXiv:2502.14367v3)

**Generated:** {experiment_metrics.end_time}

---

## Executive Summary

This experiment explores the loss landscape along the task vector direction for neural language models. We compute a task vector T = M_finetuned - M_base and sweep along the line M(Œ±) = M_base + Œ±T to understand how loss behaves as we scale the task vector.

**Key Question:** Does the loss curve cross L(M_base) at any Œ± ‚â† 0?

---

## Experiment Configuration

### Model
- **Name:** {experiment_metrics.model_name}
- **Parameters:** {experiment_metrics.model_parameters:,}
- **Device:** {experiment_metrics.device}

### Task
- **Type:** Sentiment Analysis (Positive sentiment fine-tuning)
- **Training examples:** {experiment_metrics.training_examples}
- **Epochs:** {experiment_metrics.num_epochs}
- **Learning rate:** {experiment_metrics.learning_rate:.2e}

### Alpha Sweep
- **Range:** Œ± ‚àà [{experiment_metrics.alpha_range[0]:.1f}, {experiment_metrics.alpha_range[1]:.1f}]
- **Samples:** {experiment_metrics.num_alpha_samples}

---

## Timing Summary

**Total Duration:** {experiment_metrics.duration_seconds / 60:.1f} minutes ({experiment_metrics.duration_seconds:.0f}s)

| Phase | Duration | Percentage |
|-------|----------|------------|
| Fine-tuning | {experiment_metrics.finetuning_duration_seconds / 60:.1f}m ({experiment_metrics.finetuning_duration_seconds:.0f}s) | {experiment_metrics.finetuning_duration_seconds / experiment_metrics.duration_seconds * 100:.1f}% |
| Alpha Sweep | {experiment_metrics.sweep_duration_seconds / 60:.1f}m ({experiment_metrics.sweep_duration_seconds:.0f}s) | {experiment_metrics.sweep_duration_seconds / experiment_metrics.duration_seconds * 100:.1f}% |
| Other (loading, computation) | {(experiment_metrics.duration_seconds - experiment_metrics.finetuning_duration_seconds - experiment_metrics.sweep_duration_seconds) / 60:.1f}m | {(experiment_metrics.duration_seconds - experiment_metrics.finetuning_duration_seconds - experiment_metrics.sweep_duration_seconds) / experiment_metrics.duration_seconds * 100:.1f}% |

**Performance Metrics:**
- Fine-tuning: {f"{experiment_metrics.finetuning_duration_seconds / experiment_metrics.training_steps:.2f}s per training step" if experiment_metrics.training_steps > 0 else "N/A (analysis-only mode)"}
- Alpha sweep: {experiment_metrics.time_per_alpha_seconds:.2f}s per sample

---

## Fine-Tuning Results

**Training Configuration:**
- Training steps: {experiment_metrics.training_steps}
- Final training loss: {experiment_metrics.final_training_loss:.4f}
- Batch size: 1
- Gradient checkpointing: Enabled
- Precision: bfloat16

**Task Vector:**
- Magnitude ||T||: {experiment_metrics.task_vector_magnitude:.4f}
- Computation time: {experiment_metrics.task_vector_computation_time:.2f}s

---

## Loss Landscape Analysis

### 1. Minimum General Loss (Best General Knowledge Preservation)

The Œ± value that minimizes general loss while incorporating task knowledge:

- **Œ± = {min_general.alpha:+.4f}**
- L(Œ±) = {min_general.loss:.4f}
- L(M_base) = {min_general.base_loss:.4f}
- Œî from base = {min_general.loss - min_general.base_loss:+.4f}
- Functional return |ŒîL| = {min_general.functional_return:.4f}

**Interpretation:** {"This Œ± value improves upon the base model" if min_general.loss < min_general.base_loss else "This Œ± value degrades the base model"} for general knowledge.

### 2. Minimum Task-Specific Loss (Best Task Performance)

The Œ± value that optimizes task-specific performance:

- **Œ± = {min_task.alpha:+.4f}**
- Task L(Œ±) = {min_task.task_performance:.4f}
- General L(Œ±) = {min_task.loss:.4f}
- Œî from base = {min_task.task_performance - min_task.base_loss:+.4f}

**Interpretation:** This is the optimal scaling factor for the specific task.

### 3. Trade-off Analysis

"""

    if abs(min_general.alpha - min_task.alpha) > 0.1:
        report += f"""
**‚ö†Ô∏è TRADE-OFF DETECTED**

There is a significant difference between the optimal Œ± values:
- Best general knowledge: Œ± = {min_general.alpha:+.4f}
- Best task performance: Œ± = {min_task.alpha:+.4f}
- ŒîŒ± = {abs(min_task.alpha - min_general.alpha):.4f}

This suggests a trade-off between preserving general knowledge and maximizing task performance. The choice of Œ± depends on your use case:
- Use Œ± = {min_general.alpha:+.4f} to preserve general capabilities
- Use Œ± = {min_task.alpha:+.4f} to maximize task-specific performance
"""
    else:
        report += f"""
**‚úì NO SIGNIFICANT TRADE-OFF**

Both optimal values occur at similar Œ± values (ŒîŒ± = {abs(min_task.alpha - min_general.alpha):.4f}), suggesting you can achieve both good general knowledge preservation and strong task performance at Œ± ‚âà {min_general.alpha:+.4f}.
"""

    report += f"""
### 4. Zero-Crossings (Functional Return Points)

Points where L(Œ±) ‚âà L(M_base) for Œ± ‚â† 0 (threshold: |ŒîL| < 0.1):

"""

    if zero_crossings:
        report += f"""**Found {len(zero_crossings)} zero-crossing(s):**

| # | Œ± | L(Œ±) | |ŒîL| |
|---|---|------|------|
"""
        for i, zc in enumerate(zero_crossings[:10], 1):
            report += f"| {i} | {zc.alpha:+.4f} | {zc.loss:.4f} | {zc.functional_return:.6f} |\n"

        report += f"""
**Interpretation:** These Œ± values return the model to approximately the base loss level. This suggests special scaling factors where the task vector's impact "cancels out" for general knowledge, potentially useful for model merging scenarios.

**Connection to paper:** Analogous to the 180¬∞ rotations in SO(3) that square to identity, these Œ± values represent "functional return" points in the neural loss landscape.
"""
    else:
        report += """**No zero-crossings found.**

The loss is monotonic along the task vector direction. This indicates that the task vector does not exhibit the rotation-like symmetry properties proven by Eckmann & Tlusty for rotation groups.

**Interpretation:** Unlike rotation groups where [W(Œª)]¬≤ = I has abundant solutions, the neural loss landscape does not show similar self-inverse properties under task vector scaling.
"""

    report += f"""
### 5. Best Functional Return (Smallest |L(Œ±) - L_base|)

Top 10 Œ± values closest to base loss:

| Rank | Œ± | L(Œ±) | |ŒîL| |
|------|---|------|------|
"""

    for i, result in enumerate(analysis["sorted_by_return"][:10], 1):
        report += f"| {i} | {result.alpha:+.4f} | {result.loss:.4f} | {result.functional_return:.6f} |\n"

    report += """
---

## Statistical Summary

"""

    # Calculate statistics
    all_losses = [r.loss for r in results]
    all_functional_returns = [r.functional_return for r in results]
    all_task_perfs = [r.task_performance for r in results]

    report += f"""
### Loss Distribution (General)
- Mean: {np.mean(all_losses):.4f}
- Std Dev: {np.std(all_losses):.4f}
- Min: {np.min(all_losses):.4f} (at Œ± = {results[np.argmin(all_losses)].alpha:+.4f})
- Max: {np.max(all_losses):.4f} (at Œ± = {results[np.argmax(all_losses)].alpha:+.4f})

### Functional Return Distribution
- Mean: {np.mean(all_functional_returns):.4f}
- Std Dev: {np.std(all_functional_returns):.4f}
- Min: {np.min(all_functional_returns):.6f} (at Œ± = {results[np.argmin(all_functional_returns)].alpha:+.4f})
- Max: {np.max(all_functional_returns):.4f} (at Œ± = {results[np.argmax(all_functional_returns)].alpha:+.4f})

### Task Performance Distribution
- Mean: {np.mean(all_task_perfs):.4f}
- Std Dev: {np.std(all_task_perfs):.4f}
- Min: {np.min(all_task_perfs):.4f} (at Œ± = {results[np.argmin(all_task_perfs)].alpha:+.4f})
- Max: {np.max(all_task_perfs):.4f} (at Œ± = {results[np.argmax(all_task_perfs)].alpha:+.4f})

---

## Methodology

### Task Vector Computation
```
T = M_finetuned - M_base
```

The task vector T captures the parameter-space direction learned during fine-tuning.

### Alpha Sweep
```
For Œ± in [{experiment_metrics.alpha_range[0]}, {experiment_metrics.alpha_range[1]}]:
    M(Œ±) = M_base + Œ±¬∑T
    L(Œ±) = loss of M(Œ±)
```

We evaluate the model at {experiment_metrics.num_alpha_samples} evenly-spaced Œ± values to map the loss landscape.

### Evaluation Metrics
- **General Loss:** Evaluated on neutral/general texts
- **Task Loss:** Evaluated on task-specific texts
- **Functional Return:** |L(Œ±) - L(M_base)|

---

## Connection to Theoretical Background

**Paper Reference:** Eckmann & Tlusty (2025), arXiv:2502.14367v3

The paper proves that for rotation groups SO(3)/SU(2), almost any walk W can be scaled to reach a 180¬∞ rotation R(n,œÄ), which when squared returns to identity: R(n,œÄ)¬≤ = I. This property is abundant (density 2/œÄ ‚âà 64%).

**Our Experiment:**
- **Rotation groups:** Multiplicative group with composition W‚ÇÅ ‚àò W‚ÇÇ
- **Task vectors:** Additive vector space with addition v‚ÇÅ + v‚ÇÇ
- **Key difference:** Task vectors lack the group structure required for the theorem

**Question:** Do neural loss landscapes exhibit analogous "functional return" properties under scaling?

**Finding:** {"Yes - found zero-crossings suggesting special Œ± values" if zero_crossings else "No - loss is monotonic, no rotation-like symmetry detected"}

---

## Practical Recommendations

### 1. For Model Deployment
"""

    if min_general.alpha != 0:
        report += f"""
Use **Œ± = {min_general.alpha:+.4f}** for the best balance of task performance and general knowledge preservation.
"""
    else:
        report += """
The base model (Œ± = 0) performs best for general knowledge. Consider if fine-tuning is necessary for your use case.
"""

    report += f"""
### 2. For Task-Specific Applications

Use **Œ± = {min_task.alpha:+.4f}** to maximize task-specific performance (sentiment analysis in this case).

### 3. For Model Merging
"""

    if zero_crossings:
        report += f"""
The zero-crossing Œ± values {', '.join(f'{zc.alpha:+.4f}' for zc in zero_crossings[:3])} could be useful for model merging scenarios where you want to incorporate task knowledge without significantly degrading general performance.
"""
    else:
        report += """
No special Œ± values were found for model merging. Standard Œ± = 1.0 (full task vector) or empirically tuned values should be used.
"""

    report += f"""
---

## Files Generated

1. **experiment_report.md** - This comprehensive report
2. **loss_landscape_sweep.png** - Visualization of loss landscape
3. **loss_landscape_results.json** - Detailed numerical results
4. **experiment_metrics.json** - Complete experiment metrics

---

## Experiment Timeline

- **Started:** {experiment_metrics.start_time}
- **Fine-tuning completed:** {experiment_metrics.finetuning_end_time}
- **Alpha sweep completed:** {experiment_metrics.sweep_end_time}
- **Finished:** {experiment_metrics.end_time}
- **Total duration:** {experiment_metrics.duration_seconds / 60:.1f} minutes

---

## Appendix: Sample Results

### First 20 Alpha Samples

| # | Œ± | General Loss | Task Loss | |ŒîL| |
|---|---|--------------|-----------|------|
"""

    for i, result in enumerate(results[:20], 1):
        report += f"| {i} | {result.alpha:+.4f} | {result.loss:.4f} | {result.task_performance:.4f} | {result.functional_return:.4f} |\n"

    report += f"""
### Last 20 Alpha Samples

| # | Œ± | General Loss | Task Loss | |ŒîL| |
|---|---|--------------|-----------|------|
"""

    for i, result in enumerate(results[-20:], len(results) - 19):
        report += f"| {i} | {result.alpha:+.4f} | {result.loss:.4f} | {result.task_performance:.4f} | {result.functional_return:.4f} |\n"

    report += """
---

*Report generated by Task Vector Loss Landscape Explorer*
"""

    with open(output_path, "w", encoding="utf-8") as f:
        f.write(report)


def plot_results(
    results: list[AlphaSweepResult],
    analysis: dict,
    output_path: str = "loss_landscape_sweep.png",
):
    """Visualize loss landscape L(M_base + Œ±T) vs Œ±."""

    alphas = [r.alpha for r in results]
    losses = [r.loss for r in results]
    base_loss = results[0].base_loss
    functional_returns = [r.functional_return for r in results]
    task_perfs = [r.task_performance for r in results]

    # Create 2x2 grid
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle(
        "Task Vector Loss Landscape: L(M_base + Œ±T)\n"
        "Inspired by Eckmann & Tlusty (2025)",
        fontsize=14,
        fontweight="bold",
        y=0.995,
    )

    # ‚îÄ‚îÄ‚îÄ Plot 1: MAIN - Loss vs Œ± (KEY PLOT!) ‚îÄ‚îÄ‚îÄ
    axes[0, 0].plot(
        alphas, losses, "b-", linewidth=2.5, label="General Loss", alpha=0.8
    )
    axes[0, 0].plot(
        alphas, task_perfs, "g-", linewidth=2.0, label="Task Loss", alpha=0.6
    )
    axes[0, 0].axhline(
        y=base_loss,
        color="red",
        linestyle="--",
        linewidth=2,
        label="L(M_base)",
        alpha=0.7,
    )
    axes[0, 0].axvline(x=0, color="gray", linestyle="--", alpha=0.4)

    # Highlight zero-crossings
    if analysis["zero_crossings"]:
        zc_alphas = [r.alpha for r in analysis["zero_crossings"]]
        zc_losses = [r.loss for r in analysis["zero_crossings"]]
        axes[0, 0].scatter(
            zc_alphas,
            zc_losses,
            color="orange",
            s=150,
            zorder=5,
            marker="*",
            edgecolors="black",
            linewidth=1,
            label="Zero-crossings",
        )

    # Highlight general minimum
    min_gen_result = analysis["min_general_loss"]
    axes[0, 0].scatter(
        [min_gen_result.alpha],
        [min_gen_result.loss],
        color="blue",
        s=150,
        zorder=5,
        marker="D",
        edgecolors="black",
        linewidth=1,
        label=f"Min General (Œ±={min_gen_result.alpha:.2f})",
    )

    # Highlight task minimum
    min_task_result = analysis["min_task_loss"]
    axes[0, 0].scatter(
        [min_task_result.alpha],
        [min_task_result.task_performance],
        color="green",
        s=150,
        zorder=5,
        marker="D",
        edgecolors="black",
        linewidth=1,
        label=f"Min Task (Œ±={min_task_result.alpha:.2f})",
    )

    axes[0, 0].set_xlabel("Œ±", fontsize=12)
    axes[0, 0].set_ylabel("Loss", fontsize=12)
    axes[0, 0].set_title("Loss Landscape (KEY PLOT)", fontsize=12, fontweight="bold")
    axes[0, 0].legend(fontsize=8, loc="best")
    axes[0, 0].grid(True, alpha=0.3)

    # ‚îÄ‚îÄ‚îÄ Plot 2: Functional Return |L(Œ±) - L_base| ‚îÄ‚îÄ‚îÄ
    axes[0, 1].plot(alphas, functional_returns, "r-", linewidth=2.5, alpha=0.8)
    axes[0, 1].axhline(y=0, color="green", linestyle="--", alpha=0.5, linewidth=1.5)
    axes[0, 1].axvline(x=0, color="gray", linestyle="--", alpha=0.4)

    # Highlight zero-crossings
    if analysis["zero_crossings"]:
        zc_alphas = [r.alpha for r in analysis["zero_crossings"]]
        zc_returns = [r.functional_return for r in analysis["zero_crossings"]]
        axes[0, 1].scatter(
            zc_alphas,
            zc_returns,
            color="green",
            s=150,
            zorder=5,
            marker="*",
            edgecolors="black",
            linewidth=1,
        )

    axes[0, 1].set_xlabel("Œ±", fontsize=12)
    axes[0, 1].set_ylabel("|L(Œ±) - L(M_base)|", fontsize=12)
    axes[0, 1].set_title("Functional Return", fontsize=12, fontweight="bold")
    axes[0, 1].grid(True, alpha=0.3)

    # ‚îÄ‚îÄ‚îÄ Plot 3: Signed Delta L(Œ±) - L_base ‚îÄ‚îÄ‚îÄ
    deltas = [r.loss - r.base_loss for r in results]
    axes[1, 0].plot(alphas, deltas, "b-", linewidth=2, alpha=0.8)
    axes[1, 0].axhline(y=0, color="red", linestyle="--", linewidth=2, alpha=0.7)
    axes[1, 0].axvline(x=0, color="gray", linestyle="--", alpha=0.4)
    axes[1, 0].fill_between(alphas, 0, deltas, alpha=0.2, color="blue")

    # Highlight zero-crossings
    if analysis["zero_crossings"]:
        zc_alphas = [r.alpha for r in analysis["zero_crossings"]]
        zc_deltas = [r.loss - r.base_loss for r in analysis["zero_crossings"]]
        axes[1, 0].scatter(
            zc_alphas,
            zc_deltas,
            color="green",
            s=150,
            zorder=5,
            marker="*",
            edgecolors="black",
            linewidth=1,
        )

    axes[1, 0].set_xlabel("Œ±", fontsize=12)
    axes[1, 0].set_ylabel("L(Œ±) - L(M_base)", fontsize=12)
    axes[1, 0].set_title("Signed Loss Delta", fontsize=12, fontweight="bold")
    axes[1, 0].grid(True, alpha=0.3)

    # ‚îÄ‚îÄ‚îÄ Plot 4: Task Performance vs Œ± ‚îÄ‚îÄ‚îÄ
    axes[1, 1].plot(alphas, task_perfs, "g-", linewidth=2, alpha=0.8, label="Task Loss")
    axes[1, 1].axhline(
        y=base_loss, color="gray", linestyle="--", alpha=0.5, label="Base Loss"
    )
    axes[1, 1].axvline(x=0, color="gray", linestyle="--", alpha=0.4)

    axes[1, 1].set_xlabel("Œ±", fontsize=12)
    axes[1, 1].set_ylabel("Task-Specific Loss", fontsize=12)
    axes[1, 1].set_title("Task Performance", fontsize=12, fontweight="bold")
    axes[1, 1].legend(fontsize=9)
    axes[1, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    print(f"\nüìä Plot saved to {output_path}")
    plt.close()


def main():
    """Sweep loss landscape along task vector direction.

    Directly visualizes L(M_base + Œ±T) for Œ± ‚àà [-3, 3].
    Answers: Does the loss curve cross L(M_base) at any Œ± ‚â† 0?

    Inspired by Eckmann & Tlusty (2025)'s rotation group self-inverse walks.
    """
    # Parse command-line arguments
    parser = argparse.ArgumentParser(
        description="Task Vector Loss Landscape Explorer",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Run full experiment (fine-tuning + analysis)
  python main.py

  # Run analysis only on previously fine-tuned models
  python main.py --analysis-only

  # Run with custom alpha range and samples
  python main.py --alpha-min -5 --alpha-max 5 --num-samples 200

  # Analysis only with custom parameters
  python main.py --analysis-only --alpha-min -2 --alpha-max 2 --num-samples 50
        """,
    )
    parser.add_argument(
        "--analysis-only",
        action="store_true",
        help="Skip fine-tuning and run analysis on previously saved models",
    )
    parser.add_argument(
        "--alpha-min",
        type=float,
        default=-3.0,
        help="Minimum alpha value for sweep (default: -3.0)",
    )
    parser.add_argument(
        "--alpha-max",
        type=float,
        default=3.0,
        help="Maximum alpha value for sweep (default: 3.0)",
    )
    parser.add_argument(
        "--num-samples",
        type=int,
        default=100,
        help="Number of alpha samples (default: 100)",
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="./outputs",
        help="Output directory (default: ./outputs)",
    )

    args = parser.parse_args()

    experiment_start_time = time.time()

    print(f"\n{'='*70}")
    print("TASK VECTOR LOSS LANDSCAPE EXPERIMENT")
    print("Inspired by Eckmann & Tlusty (2025)")
    print(f"{'='*70}")

    if args.analysis_only:
        print("\nüîç MODE: Analysis Only (using saved models)")
    else:
        print("\nüöÄ MODE: Full Experiment (fine-tuning + analysis)")

    print(
        """
RESEARCH QUESTION:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
What does the loss look like along the 1D line M(Œ±) = M_base + Œ±T?

Specifically: Does the loss curve cross L(M_base) at any Œ± ‚â† 0?

THE EXPERIMENT:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
1. Create task vector: T = M_finetuned - M_base
2. Sweep Œ± from -3.0 to 3.0 (100 samples)
3. For each Œ±: evaluate L(M_base + Œ±T)
4. Plot L(Œ±) vs Œ±

WHAT WE'RE LOOKING FOR:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚Ä¢ Zero-crossings: Œ± ‚â† 0 where L(Œ±) ‚âà L(M_base) ("functional return")
‚Ä¢ Minimum loss: Best Œ± for task performance
‚Ä¢ Loss landscape shape: Monotonic? Periodic? Symmetric?

CONNECTION TO PAPER:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Eckmann & Tlusty prove that rotation group walks [W(Œª)]¬≤ = I have abundant
special Œª values (180¬∞ rotations). We test if neural loss landscapes exhibit
analogous structure under task vector scaling.
"""
    )

    # Configuration
    model_name = "google/gemma-3-4b-it"
    output_dir = args.output_dir
    alpha_range = (args.alpha_min, args.alpha_max)
    num_samples = args.num_samples

    # Device detection with MPS support for Apple Silicon
    if torch.cuda.is_available():
        device = "cuda"
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        device = "mps"
    else:
        device = "cpu"

    os.makedirs(output_dir, exist_ok=True)

    # Initialize experiment metrics
    experiment_metrics = ExperimentMetrics(
        start_time=datetime.fromtimestamp(experiment_start_time).isoformat(),
        model_name=model_name,
        device=device,
    )

    print(f"\nConfiguration:")
    print(f"  Model: {model_name}")
    print(f"  Device: {device}")
    print(f"  Output dir: {output_dir}")
    print(f"  Alpha range: [{alpha_range[0]:.1f}, {alpha_range[1]:.1f}]")
    print(f"  Num samples: {num_samples}")
    print(f"  Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    # Load tokenizer
    print("Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token

    # Prepare task data (sentiment analysis)
    print("\nPreparing task data (sentiment analysis)...")

    # Training data - positive sentiment examples
    train_texts = [
        "This movie is absolutely fantastic! I loved every minute of it.",
        "The product exceeded all my expectations. Highly recommended!",
        "What a wonderful experience! I'm so happy with this purchase.",
        "Amazing quality and great value. I'm very satisfied.",
        "This is the best thing I've bought in years. Incredible!",
        "Outstanding service and excellent results. Very pleased!",
        "I'm delighted with this purchase. It works perfectly.",
        "Exceptional quality and superb performance. Love it!",
        "This is exactly what I needed. Fantastic product!",
        "Brilliant! This has made my life so much easier.",
    ] * 3  # Repeat for more training data

    # Task evaluation texts (positive sentiment)
    task_eval_texts = [
        "This product is amazing and works great!",
        "I'm very happy with my purchase. Excellent quality!",
        "Wonderful experience! Highly recommended.",
        "Best purchase I've made this year!",
        "Fantastic quality and great value.",
    ]

    # General evaluation texts (neutral/mixed)
    general_eval_texts = [
        "The quick brown fox jumps over the lazy dog.",
        "Machine learning is a subset of artificial intelligence.",
        "The weather today is quite different from yesterday.",
        "Programming requires logical thinking and problem solving.",
        "The capital of France is Paris.",
    ]

    # Check if we should load existing models or fine-tune new ones
    if args.analysis_only:
        if check_saved_models_exist(output_dir):
            print(f"\n{'='*70}")
            print("LOADING SAVED MODELS")
            print(f"{'='*70}")
            base_model, finetuned_model = load_models_for_analysis(
                model_name, output_dir, device
            )
            # Set dummy metrics for analysis-only mode
            experiment_metrics.training_examples = len(train_texts)
            experiment_metrics.num_epochs = 2  # Default from previous run
            experiment_metrics.learning_rate = 5e-5  # Default from previous run
        else:
            print(f"\n{'='*70}")
            print("WARNING: No saved models found!")
            print(f"{'='*70}")
            print(f"Looking for models in: {output_dir}")
            print("  - saved_base_model/")
            print("  - saved_finetuned_model/")
            print("\nPlease run the full experiment first without --analysis-only")
            print("Or check that the --output-dir path is correct.")
            return
    else:
        # Full experiment mode: Load base model and fine-tune
        print(f"\n{'='*70}")
        print("LOADING BASE MODEL")
        print(f"{'='*70}")
        device_map = get_device_map()
        base_model = AutoModelForCausalLM.from_pretrained(
            model_name, torch_dtype=torch.bfloat16, device_map=device_map
        )

        # For MPS/CPU, manually move to device if device_map is None
        if device_map is None and device != "cpu":
            base_model = base_model.to(device)  # type: ignore[assignment]

        # Fine-tune model on task
        # NOTE: We don't use device_map="auto" for fine-tuning to avoid meta device issues
        print("\nLoading model for fine-tuning...")
        ft_model = AutoModelForCausalLM.from_pretrained(
            model_name, torch_dtype=torch.bfloat16, device_map=None
        )
        ft_model = ft_model.to(device)  # type: ignore[assignment]

        finetuned_model, ft_metrics = fine_tune_model(
            base_model=ft_model,
            tokenizer=tokenizer,
            train_texts=train_texts,
            output_dir=f"{output_dir}/finetuned_tmp",
            num_epochs=2,
            learning_rate=5e-5,
        )

        # Update experiment metrics with fine-tuning data
        experiment_metrics.finetuning_start_time = ft_metrics["start_time"]
        experiment_metrics.finetuning_end_time = ft_metrics["end_time"]
        experiment_metrics.finetuning_duration_seconds = ft_metrics["duration_seconds"]
        experiment_metrics.training_examples = ft_metrics["training_examples"]
        experiment_metrics.num_epochs = ft_metrics["num_epochs"]
        experiment_metrics.learning_rate = ft_metrics["learning_rate"]
        experiment_metrics.final_training_loss = ft_metrics["final_loss"]
        experiment_metrics.training_steps = ft_metrics["training_steps"]
        experiment_metrics.training_history = ft_metrics["training_history"]

        # Save models for future analysis
        save_models_for_analysis(base_model, finetuned_model, output_dir)

    # Compute task vector
    print("Computing task vector T = M_finetuned - M_base...")
    tv_start = time.time()
    task_vector = compute_task_vector(base_model, finetuned_model)
    tv_elapsed = time.time() - tv_start

    # Calculate task vector magnitude
    tv_magnitude = np.sqrt(sum((v**2).sum().item() for v in task_vector.values()))
    print(f"Task vector magnitude: ||T|| = {tv_magnitude:.4f}")
    print(f"Computation time: {tv_elapsed:.2f}s\n")

    # Update experiment metrics
    experiment_metrics.task_vector_magnitude = tv_magnitude
    experiment_metrics.task_vector_computation_time = tv_elapsed

    # Count model parameters
    num_params = sum(p.numel() for p in base_model.parameters())
    experiment_metrics.model_parameters = num_params
    print(f"Model parameters: {num_params:,}\n")

    # Run experiment
    results, sweep_metrics = sweep_alpha_values(
        base_model=base_model,
        task_vector=task_vector,
        tokenizer=tokenizer,
        general_eval_texts=general_eval_texts,
        task_eval_texts=task_eval_texts,
        alpha_range=alpha_range,
        num_samples=num_samples,
        device=device,
    )

    # Update experiment metrics with sweep timing
    experiment_metrics.sweep_start_time = sweep_metrics["start_time"]
    experiment_metrics.sweep_end_time = sweep_metrics["end_time"]
    experiment_metrics.sweep_duration_seconds = sweep_metrics["duration_seconds"]
    experiment_metrics.time_per_alpha_seconds = sweep_metrics["time_per_alpha_seconds"]

    # Analyze results
    analysis = analyze_results(results)

    # Update experiment metrics with sweep and results data
    min_general = analysis["min_general_loss"]
    min_task = analysis["min_task_loss"]
    zero_crossings = analysis["zero_crossings"]

    experiment_metrics.num_alpha_samples = len(results)
    experiment_metrics.alpha_range = alpha_range
    experiment_metrics.min_general_loss_alpha = min_general.alpha
    experiment_metrics.min_general_loss = min_general.loss
    experiment_metrics.min_task_loss_alpha = min_task.alpha
    experiment_metrics.min_task_loss = min_task.task_performance
    experiment_metrics.num_zero_crossings = len(zero_crossings)
    experiment_metrics.zero_crossing_alphas = [zc.alpha for zc in zero_crossings]

    # Generate markdown report
    generate_markdown_report(
        experiment_metrics=experiment_metrics,
        analysis=analysis,
        results=results,
        output_path=f"{output_dir}/experiment_report.md",
    )
    print(f"üìù Markdown report saved to {output_dir}/experiment_report.md")

    # Plot results
    plot_results(results, analysis, f"{output_dir}/loss_landscape_sweep.png")

    # Save detailed results
    results_path = f"{output_dir}/loss_landscape_results.json"
    with open(results_path, "w", encoding="utf-8") as f:
        json.dump(
            {
                "metadata": {
                    "paper_reference": "Eckmann & Tlusty (2025), arXiv:2502.14367v3",
                    "description": "Loss landscape sweep along task vector direction",
                    "task": "sentiment_analysis",
                    "task_vector_magnitude": tv_magnitude,
                    "model": model_name,
                    "alpha_range": list(alpha_range),
                    "num_samples": num_samples,
                },
                "methodology": {
                    "experiment": "M(Œ±) = M_base + Œ±T",
                    "primary_metric": "L(Œ±) = loss of M(Œ±)",
                    "question": "Does L(Œ±) cross L(M_base) at any Œ± ‚â† 0?",
                },
                "key_results": {
                    "min_general_loss": asdict(analysis["min_general_loss"]),
                    "min_task_loss": asdict(analysis["min_task_loss"]),
                    "best_return": asdict(analysis["best_return"]),
                    "zero_crossings": [asdict(r) for r in analysis["zero_crossings"]],
                },
                "top_10_by_return": [asdict(r) for r in analysis["sorted_by_return"]],
                "all_results": [asdict(r) for r in results],
            },
            f,
            indent=2,
        )
    print(f"üìÑ Results saved to {results_path}")

    # Complete experiment metrics and save
    experiment_end_time = time.time()
    experiment_metrics.end_time = datetime.fromtimestamp(experiment_end_time).isoformat()
    experiment_metrics.duration_seconds = experiment_end_time - experiment_start_time

    metrics_path = f"{output_dir}/experiment_metrics.json"
    with open(metrics_path, "w", encoding="utf-8") as f:
        json.dump(asdict(experiment_metrics), f, indent=2)
    print(f"üìä Experiment metrics saved to {metrics_path}")

    # Summary
    print(f"\n{'='*70}")
    print("SUMMARY")
    print(f"{'='*70}\n")

    min_general = analysis["min_general_loss"]
    min_task = analysis["min_task_loss"]
    best_return = analysis["best_return"]
    zero_crossings = analysis["zero_crossings"]

    print("MINIMUM GENERAL LOSS (Best General Knowledge):")
    print(f"  Œ± = {min_general.alpha:+.4f}")
    print(f"  L(Œ±) = {min_general.loss:.4f}")
    print(f"  L(M_base) = {min_general.base_loss:.4f}")
    print(f"  Improvement: {min_general.base_loss - min_general.loss:+.4f}")

    print("\nMINIMUM TASK LOSS (Best Task Performance):")
    print(f"  Œ± = {min_task.alpha:+.4f}")
    print(f"  Task L(Œ±) = {min_task.task_performance:.4f}")
    print(f"  General L(Œ±) = {min_task.loss:.4f}")

    if abs(min_general.alpha - min_task.alpha) > 0.1:
        print("\n  ‚ö†Ô∏è  TRADE-OFF DETECTED!")
        print(f"  ‚Üí Best general knowledge at Œ± = {min_general.alpha:+.4f}")
        print(f"  ‚Üí Best task performance at Œ± = {min_task.alpha:+.4f}")
        print(f"  ‚Üí ŒîŒ± = {abs(min_task.alpha - min_general.alpha):.4f}")
    else:
        print("\n  ‚úì Both minimums occur at similar Œ± values")

    print("\nBEST FUNCTIONAL RETURN (smallest |L(Œ±) - L_base|):")
    print(f"  Œ± = {best_return.alpha:+.4f}")
    print(f"  |ŒîL| = {best_return.functional_return:.6f}")
    print(f"  L(Œ±) = {best_return.loss:.4f}")

    print("\nZERO-CROSSINGS (Œ± ‚â† 0 where L(Œ±) ‚âà L_base):")
    if zero_crossings:
        print(f"  ‚úì Found {len(zero_crossings)} crossing(s)!")
        for i, zc in enumerate(zero_crossings[:5], 1):
            print(
                f"     {i}. Œ± = {zc.alpha:+.4f}, L(Œ±) = {zc.loss:.4f}, |ŒîL| = {zc.functional_return:.6f}"
            )
    else:
        print("  ‚úó No zero-crossings found")
        print("  ‚Üí Loss is monotonic along task vector direction")

    print(f"\n{'='*70}")
    print("INTERPRETATION")
    print(f"{'='*70}")
    print(
        """
CONNECTION TO PAPER:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Eckmann & Tlusty (2025) prove that rotation group walks have abundant special
Œª values where [W(Œª)]¬≤ = I (self-inverse property). This happens when W(Œª) is
a 180¬∞ rotation: R(n,œÄ)¬≤ = I.

OUR EXPERIMENT:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
We tested if neural loss landscapes exhibit analogous structure by sweeping
L(M_base + Œ±T) along the task vector direction.

Question: Does L(Œ±) cross L(M_base) at any Œ± ‚â† 0?

If YES ‚Üí suggests special scaling factors exist (analogous to 180¬∞ rotations)
If NO  ‚Üí loss is monotonic (task vectors lack rotation-like symmetry)

KEY DIFFERENCES FROM PAPER:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚ö†Ô∏è  Rotations: Multiplicative group (W‚ÇÅ ‚àò W‚ÇÇ)
‚ö†Ô∏è  Task vectors: Additive vector space (v‚ÇÅ + v‚ÇÇ)
‚ö†Ô∏è  No group structure ‚Üí no proven abundance of special Œ± values
‚ö†Ô∏è  This is EMPIRICAL exploration, not a theorem

PRACTICAL IMPLICATIONS:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚Ä¢ Minimum Œ±: Best scaling for task performance
‚Ä¢ Zero-crossings: Special Œ± values for model merging
‚Ä¢ Landscape shape: Insights into parameter space geometry
"""
    )

    # Final experiment metrics summary
    total_duration = experiment_metrics.duration_seconds
    print(f"\n{'='*70}")
    print("EXPERIMENT METRICS SUMMARY")
    print(f"{'='*70}\n")

    print(f"Total Duration: {total_duration / 60:.1f} minutes ({total_duration:.0f}s)")
    print(f"  - Fine-tuning: {experiment_metrics.finetuning_duration_seconds / 60:.1f}m "
          f"({experiment_metrics.finetuning_duration_seconds / total_duration * 100:.1f}%)")
    print(f"  - Alpha sweep: {experiment_metrics.sweep_duration_seconds / 60:.1f}m "
          f"({experiment_metrics.sweep_duration_seconds / total_duration * 100:.1f}%)")

    print(f"\nModel: {experiment_metrics.model_name}")
    print(f"  Parameters: {experiment_metrics.model_parameters:,}")
    print(f"  Device: {experiment_metrics.device}")

    print(f"\nFine-tuning:")
    print(f"  Training examples: {experiment_metrics.training_examples}")
    print(f"  Epochs: {experiment_metrics.num_epochs}")
    print(f"  Steps: {experiment_metrics.training_steps}")
    print(f"  Learning rate: {experiment_metrics.learning_rate:.2e}")
    print(f"  Final loss: {experiment_metrics.final_training_loss:.4f}")
    if experiment_metrics.training_steps > 0:
        print(f"  Avg time/step: {experiment_metrics.finetuning_duration_seconds / experiment_metrics.training_steps:.2f}s")
    else:
        print(f"  Avg time/step: N/A (analysis-only mode)")

    print(f"\nTask Vector:")
    print(f"  Magnitude ||T||: {experiment_metrics.task_vector_magnitude:.4f}")
    print(f"  Computation time: {experiment_metrics.task_vector_computation_time:.2f}s")

    print(f"\nAlpha Sweep:")
    print(f"  Samples: {experiment_metrics.num_alpha_samples}")
    print(f"  Range: [{experiment_metrics.alpha_range[0]:.1f}, {experiment_metrics.alpha_range[1]:.1f}]")
    print(f"  Avg time/sample: {experiment_metrics.time_per_alpha_seconds:.2f}s")

    print(f"\nKey Results:")
    print(f"  Min general loss Œ±: {experiment_metrics.min_general_loss_alpha:+.4f} (L={experiment_metrics.min_general_loss:.4f})")
    print(f"  Min task loss Œ±: {experiment_metrics.min_task_loss_alpha:+.4f} (L={experiment_metrics.min_task_loss:.4f})")
    print(f"  Zero-crossings: {experiment_metrics.num_zero_crossings}")
    if experiment_metrics.zero_crossing_alphas:
        print(f"  Zero-crossing Œ± values: {', '.join(f'{a:+.4f}' for a in experiment_metrics.zero_crossing_alphas[:5])}")

    print(f"\n{'='*70}")
    print("EXPERIMENT COMPLETE")
    print(f"{'='*70}")
    print(f"Started:  {experiment_metrics.start_time}")
    print(f"Finished: {experiment_metrics.end_time}")
    print(f"Duration: {total_duration / 60:.1f} minutes")
    print(f"\nOutputs saved to: {output_dir}/")
    print(f"  - {output_dir}/experiment_report.md (comprehensive analysis)")
    print(f"  - {output_dir}/loss_landscape_sweep.png (visualization)")
    print(f"  - {output_dir}/loss_landscape_results.json (detailed data)")
    print(f"  - {output_dir}/experiment_metrics.json (metrics)")
    if not args.analysis_only:
        print(f"  - {output_dir}/saved_base_model/ (saved for re-analysis)")
        print(f"  - {output_dir}/saved_finetuned_model/ (saved for re-analysis)")
    print(f"{'='*70}\n")
    print("üìã Copy experiment_report.md to share with LLMs for analysis!")
    if not args.analysis_only:
        print("\nüí° TIP: To re-run analysis with different parameters without re-training:")
        print(f"   python main.py --analysis-only --alpha-min -5 --alpha-max 5 --num-samples 200")


if __name__ == "__main__":
    main()
