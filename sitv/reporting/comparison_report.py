"""
Multi-task comparison report generation for SITV experiments.

This module provides the ComparisonReportGenerator for creating comprehensive
comparison reports across multiple task experiments.
"""

from pathlib import Path
from typing import Any, Dict

import numpy as np

from sitv.data.models import AlphaSweepResult


class ComparisonReportGenerator:
    """Service for generating multi-task comparison reports.

    This generator creates comprehensive comparison reports analyzing
    multiple task experiments side-by-side, including:
    - Comparative metrics table
    - Per-task detailed analysis
    - Key insights and patterns across tasks
    """

    def __init__(self):
        """Initialize the comparison report generator."""
        pass

    def generate(
        self,
        task_datasets: Dict[str, Dict[str, Any]],
        output_path: Path | str
    ) -> str:
        """Generate multi-task comparison report.

        Args:
            task_datasets: Dictionary mapping task names to their data.
                Each task data should contain 'results' and 'analysis' keys.
            output_path: Path to save the report

        Returns:
            Path to saved report

        Examples:
            >>> generator = ComparisonReportGenerator()
            >>> task_datasets = {
            ...     'SP': {'results': [...], 'analysis': {...}},
            ...     'SN': {'results': [...], 'analysis': {...}}
            ... }
            >>> report_path = generator.generate(task_datasets, 'comparison_report.md')
        """
        report = self._build_report(task_datasets)

        # Write report
        output_path = Path(output_path)
        with open(output_path, 'w') as f:
            f.write(report)

        print(f"\nðŸ“„ Comparison report saved to {output_path}")
        return str(output_path)

    def _build_report(self, task_datasets: Dict[str, Dict[str, Any]]) -> str:
        """Build the complete comparison report content.

        Args:
            task_datasets: Dictionary mapping task names to their data

        Returns:
            Complete Markdown report as string
        """
        sections = []

        sections.append(self._create_header())
        sections.append(self._create_executive_summary(task_datasets))
        sections.append(self._create_comparative_metrics(task_datasets))
        sections.append(self._create_detailed_task_analysis(task_datasets))
        sections.append(self._create_key_insights(task_datasets))

        return '\n\n'.join(sections)

    def _create_header(self) -> str:
        """Create report header."""
        return """# Multi-Task Loss Landscape Comparison Report

**Generated by compare_tasks.py**

---"""

    def _create_executive_summary(self, task_datasets: Dict[str, Dict[str, Any]]) -> str:
        """Create executive summary section.

        Args:
            task_datasets: Dictionary mapping task names to their data

        Returns:
            Executive summary section as string
        """
        return """## Executive Summary

This report compares loss landscape characteristics across multiple task vectors,
exploring optimal scaling factors, zero-crossings, and self-inverse properties."""

    def _create_comparative_metrics(self, task_datasets: Dict[str, Dict[str, Any]]) -> str:
        """Create comparative metrics table.

        Args:
            task_datasets: Dictionary mapping task names to their data

        Returns:
            Comparative metrics table section as string
        """
        lines = ["## Comparative Metrics", ""]
        lines.append("| Task | Optimal Î± | Min Loss | Î” from Base | Zero-Crossings | Squaring Returns |")
        lines.append("|------|-----------|----------|-------------|----------------|------------------|")

        for task_name, data in task_datasets.items():
            analysis = data['analysis']
            min_gen = analysis['min_general_loss']
            delta = min_gen.loss - min_gen.base_loss
            num_zc = len(analysis['zero_crossings'])
            num_sr = len(analysis['squaring_return_points'])

            lines.append(
                f"| **{task_name}** | {min_gen.alpha:.4f} | {min_gen.loss:.4f} | "
                f"{delta:.4f} | {num_zc} | {num_sr} |"
            )

        return '\n'.join(lines)

    def _create_detailed_task_analysis(self, task_datasets: Dict[str, Dict[str, Any]]) -> str:
        """Create detailed per-task analysis section.

        Args:
            task_datasets: Dictionary mapping task names to their data

        Returns:
            Detailed task analysis section as string
        """
        lines = ["## Detailed Task Analysis", ""]

        for task_name, data in task_datasets.items():
            analysis = data['analysis']

            lines.append(f"### {task_name} (Task Vector)")
            lines.append("")

            # General performance
            min_gen = analysis['min_general_loss']
            lines.append("**General Performance:**")
            lines.append(f"- Optimal Î±: **{min_gen.alpha:.4f}**")
            lines.append(f"- Minimum loss: **{min_gen.loss:.4f}**")
            lines.append(f"- Base loss: {min_gen.base_loss:.4f}")
            improvement_pct = ((min_gen.base_loss - min_gen.loss) / min_gen.base_loss * 100)
            lines.append(f"- Improvement: **{min_gen.base_loss - min_gen.loss:.4f}** ({improvement_pct:.2f}%)")
            lines.append("")

            # Task performance
            min_task = analysis['min_task_loss']
            lines.append("**Task-Specific Performance:**")
            lines.append(f"- Optimal Î±: **{min_task.alpha:.4f}**")
            lines.append(f"- Minimum task loss: **{min_task.task_eval_loss:.4f}**")
            lines.append("")

            # Zero-crossings
            if analysis['zero_crossings']:
                lines.append(f"**Zero-Crossings:** {len(analysis['zero_crossings'])} found")
                for i, zc in enumerate(analysis['zero_crossings'], 1):
                    lines.append(f"  {i}. Î± = {zc.alpha:.4f} (return = {zc.functional_return:.6f})")
            else:
                lines.append("**Zero-Crossings:** None found")
            lines.append("")

            # Squaring returns
            if analysis['squaring_return_points']:
                lines.append(f"**Squaring Returns (L(2Î±) â‰ˆ L_base):** {len(analysis['squaring_return_points'])} found")
                for i, sr in enumerate(analysis['squaring_return_points'], 1):
                    lines.append(f"  {i}. Î± = {sr.alpha:.4f} (|L(2Î±) - L_base| = {sr.functional_return_2alpha:.6f})")
                lines.append("")

            # Category breakdown
            if hasattr(min_gen, 'category_losses') and min_gen.category_losses:
                lines.append("**Category Performance at Optimal Î±:**")
                for category, loss in sorted(min_gen.category_losses.items()):
                    lines.append(f"- {category}: {loss:.4f}")
                lines.append("")

        return '\n'.join(lines)

    def _create_key_insights(self, task_datasets: Dict[str, Dict[str, Any]]) -> str:
        """Create key insights section analyzing patterns across tasks.

        Args:
            task_datasets: Dictionary mapping task names to their data

        Returns:
            Key insights section as string
        """
        lines = ["## Key Insights", ""]

        # Best task
        best_task = min(task_datasets.items(),
                       key=lambda x: x[1]['analysis']['min_general_loss'].loss)
        lines.append(f"**Best General Performance:** {best_task[0]} achieves lowest loss of "
                    f"{best_task[1]['analysis']['min_general_loss'].loss:.4f} at "
                    f"Î±={best_task[1]['analysis']['min_general_loss'].alpha:.4f}")
        lines.append("")

        # Compare optimal alphas
        alphas = {name: data['analysis']['min_general_loss'].alpha
                 for name, data in task_datasets.items()}
        alpha_range = max(alphas.values()) - min(alphas.values())
        lines.append(f"**Optimal Î± Variance:** Range of {alpha_range:.4f} across tasks")
        smallest_task = min(alphas, key=lambda k: alphas[k])
        largest_task = max(alphas, key=lambda k: alphas[k])
        lines.append(f"  - Smallest: {smallest_task} (Î±={alphas[smallest_task]:.4f})")
        lines.append(f"  - Largest: {largest_task} (Î±={alphas[largest_task]:.4f})")
        lines.append("")

        # Zero-crossing patterns
        total_zc = sum(len(data['analysis']['zero_crossings']) for data in task_datasets.values())
        lines.append(f"**Zero-Crossing Analysis:** {total_zc} total zero-crossings found across all tasks")
        lines.append("")

        # Squaring test results
        total_sr = sum(len(data['analysis']['squaring_return_points']) for data in task_datasets.values())
        if total_sr > 0:
            lines.append(f"**Self-Inverse Properties:** {total_sr} squaring return points found, "
                        f"suggesting rotation-like symmetry in the loss landscape")

        return '\n'.join(lines)
