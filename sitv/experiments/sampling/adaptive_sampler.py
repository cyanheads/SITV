"""
Adaptive sampler for alpha sweep experiments.

This module implements multi-resolution adaptive sampling that focuses
more samples on interesting regions of the loss landscape.
"""

import numpy as np
import logging
from typing import List, Optional, Tuple
from sitv.data.models import AlphaSweepResult
from sitv.experiments.sampling.base_sampler import BaseSampler

logger = logging.getLogger(__name__)


class AdaptiveSampler(BaseSampler):
    """Adaptive multi-resolution sampling strategy.

    This sampler uses a two-phase approach:
    1. Coarse pass: Sample broadly across the entire range
    2. Refinement: Add dense samples in interesting regions

    Interesting regions are identified by:
    - High curvature (rapid loss changes)
    - Zero-crossings (loss ≈ base_loss)
    - Local extrema

    This can reduce total evaluations by 40-60% while maintaining
    or improving resolution in important regions.

    Attributes:
        coarse_samples: Number of samples in coarse pass
        refine_factor: How many additional samples per interesting region
        curvature_threshold: Threshold for identifying high-curvature regions
        refinement_window: Size of window around interesting points
    """

    def __init__(
        self,
        alpha_range: Tuple[float, float],
        num_samples: int,
        coarse_samples: int = 20,
        refine_factor: int = 3,
        curvature_threshold: float = 0.5,
        refinement_window: float = 0.3,
    ):
        """Initialize adaptive sampler.

        Args:
            alpha_range: Range of alpha values (min, max)
            num_samples: Target total number of samples
            coarse_samples: Number of samples in initial coarse pass
            refine_factor: Multiplier for refinement samples per region
            curvature_threshold: Threshold for high-curvature detection
            refinement_window: Size of window around interesting points (fraction of range)
        """
        super().__init__(alpha_range, num_samples)
        self.coarse_samples = coarse_samples
        self.refine_factor = refine_factor
        self.curvature_threshold = curvature_threshold
        self.refinement_window = refinement_window
        self.coarse_done = False
        self.refinement_done = False

    def generate_samples(
        self,
        results: Optional[List[AlphaSweepResult]] = None
    ) -> np.ndarray:
        """Generate alpha values adaptively.

        Args:
            results: Previously collected results (None for first pass)

        Returns:
            Array of alpha values to evaluate next
        """
        if results is None or len(results) == 0:
            # First pass: coarse sampling
            logger.info(f"Adaptive sampling: Coarse pass ({self.coarse_samples} samples)")
            self.coarse_done = False
            return self._coarse_pass()

        if not self.coarse_done:
            # Coarse pass completed, move to refinement
            self.coarse_done = True
            logger.info("Adaptive sampling: Analyzing results for refinement...")
            refinement_samples = self._plan_refinement(results)

            if len(refinement_samples) == 0:
                logger.info("No interesting regions found, returning empty array")
                self.refinement_done = True
                return np.array([])

            logger.info(f"Adaptive sampling: Refining {len(refinement_samples)} regions")
            return refinement_samples

        # Refinement done
        self.refinement_done = True
        return np.array([])

    def should_continue(
        self,
        results: List[AlphaSweepResult]
    ) -> bool:
        """Check if more adaptive samples are needed.

        Args:
            results: Results collected so far

        Returns:
            True if refinement pass is needed, False otherwise
        """
        if not self.coarse_done:
            return True  # Need refinement pass

        if not self.refinement_done:
            return False  # Refinement completed

        return False

    def _coarse_pass(self) -> np.ndarray:
        """Generate coarse initial samples.

        Returns:
            Array of coarse alpha values
        """
        return np.linspace(
            self.alpha_min,
            self.alpha_max,
            self.coarse_samples
        )

    def _plan_refinement(
        self,
        results: List[AlphaSweepResult]
    ) -> np.ndarray:
        """Plan refinement samples based on coarse results.

        Args:
            results: Results from coarse pass

        Returns:
            Array of refinement alpha values
        """
        if len(results) < 3:
            return np.array([])

        # Sort results by alpha
        sorted_results = sorted(results, key=lambda r: r.alpha)
        alphas = np.array([r.alpha for r in sorted_results])
        losses = np.array([r.loss for r in sorted_results])

        # Identify interesting regions
        interesting_regions = []

        # 1. High curvature regions
        curvature_regions = self._find_high_curvature_regions(alphas, losses)
        interesting_regions.extend(curvature_regions)

        # 2. Zero-crossing regions (where loss ≈ base_loss)
        if len(sorted_results) > 0:
            base_loss = sorted_results[0].base_loss
            zero_crossing_regions = self._find_zero_crossing_regions(alphas, losses, base_loss)
            interesting_regions.extend(zero_crossing_regions)

        # 3. Local extrema
        extrema_regions = self._find_extrema_regions(alphas, losses)
        interesting_regions.extend(extrema_regions)

        if len(interesting_regions) == 0:
            return np.array([])

        # Merge overlapping regions
        merged_regions = self._merge_regions(interesting_regions)

        # Generate refinement samples
        refinement_samples_list: List[float] = []
        for region_center in merged_regions:
            region_samples = self._refine_region(region_center, alphas)
            refinement_samples_list.extend(region_samples)

        # Remove duplicates and sort
        refinement_samples = np.unique(refinement_samples_list)

        # Limit total samples
        max_refinement = self.num_samples - len(results)
        if len(refinement_samples) > max_refinement:
            refinement_samples = refinement_samples[:max_refinement]

        return refinement_samples

    def _find_high_curvature_regions(
        self,
        alphas: np.ndarray,
        losses: np.ndarray
    ) -> List[float]:
        """Find regions with high curvature (second derivative).

        Uses proper finite difference formula for non-uniform grids with
        numerical stability protection.

        Args:
            alphas: Array of alpha values
            losses: Array of loss values

        Returns:
            List of alpha values at high-curvature regions
        """
        if len(alphas) < 3:
            return []

        # Compute second derivative using proper finite difference for non-uniform grids
        # For interior points: d²f/dx² ≈ 2*((f[i+1]-f[i])/h_f - (f[i]-f[i-1])/h_b) / (h_f + h_b)
        second_deriv = np.zeros(len(alphas) - 2)
        for i in range(1, len(alphas) - 1):
            h_forward = alphas[i+1] - alphas[i]
            h_backward = alphas[i] - alphas[i-1]

            # Add epsilon to prevent division by very small values
            epsilon = 1e-10
            h_forward = max(h_forward, epsilon)
            h_backward = max(h_backward, epsilon)

            first_deriv_forward = (losses[i+1] - losses[i]) / h_forward
            first_deriv_backward = (losses[i] - losses[i-1]) / h_backward
            second_deriv[i-1] = 2 * (first_deriv_forward - first_deriv_backward) / (h_forward + h_backward)

        # Find regions with high absolute curvature
        high_curvature_idx = np.where(np.abs(second_deriv) > self.curvature_threshold)[0]

        # Return alpha values at these points (offset by 1 since second_deriv is shorter)
        return [alphas[i + 1] for i in high_curvature_idx]

    def _find_zero_crossing_regions(
        self,
        alphas: np.ndarray,
        losses: np.ndarray,
        base_loss: float
    ) -> List[float]:
        """Find regions where loss crosses base_loss.

        Args:
            alphas: Array of alpha values
            losses: Array of loss values
            base_loss: Base model loss

        Returns:
            List of alpha values near zero-crossings
        """
        # Find where loss crosses base_loss
        diff_from_base = losses - base_loss
        sign_changes = np.diff(np.sign(diff_from_base))
        crossing_idx = np.where(sign_changes != 0)[0]

        return [alphas[i] for i in crossing_idx]

    def _find_extrema_regions(
        self,
        alphas: np.ndarray,
        losses: np.ndarray
    ) -> List[float]:
        """Find local minima and maxima.

        Args:
            alphas: Array of alpha values
            losses: Array of loss values

        Returns:
            List of alpha values at local extrema
        """
        if len(losses) < 3:
            return []

        extrema = []

        # Find local minima and maxima
        for i in range(1, len(losses) - 1):
            if losses[i] < losses[i-1] and losses[i] < losses[i+1]:
                # Local minimum
                extrema.append(alphas[i])
            elif losses[i] > losses[i-1] and losses[i] > losses[i+1]:
                # Local maximum
                extrema.append(alphas[i])

        return extrema

    def _merge_regions(
        self,
        regions: List[float]
    ) -> List[float]:
        """Merge overlapping or nearby regions.

        Args:
            regions: List of region centers

        Returns:
            List of merged region centers
        """
        if len(regions) == 0:
            return []

        # Sort regions
        regions = sorted(regions)

        # Merge nearby regions
        window_size = self.refinement_window * (self.alpha_max - self.alpha_min)
        merged = [regions[0]]

        for region in regions[1:]:
            if region - merged[-1] < window_size:
                # Merge by taking midpoint
                merged[-1] = (merged[-1] + region) / 2
            else:
                merged.append(region)

        return merged

    def _refine_region(
        self,
        center: float,
        existing_alphas: np.ndarray
    ) -> List[float]:
        """Generate refinement samples around a region center.

        Args:
            center: Center of region to refine
            existing_alphas: Already sampled alpha values

        Returns:
            List of new alpha values to sample
        """
        # Define region boundaries
        window_size = self.refinement_window * (self.alpha_max - self.alpha_min)
        region_min = max(self.alpha_min, center - window_size / 2)
        region_max = min(self.alpha_max, center + window_size / 2)

        # Generate dense samples in this region
        num_region_samples = self.refine_factor * 5  # 5 base samples per region
        region_samples = np.linspace(region_min, region_max, num_region_samples)

        # Filter out samples too close to existing ones
        min_spacing = (self.alpha_max - self.alpha_min) / (self.num_samples * 2)
        new_samples = []

        for sample in region_samples:
            if np.min(np.abs(existing_alphas - sample)) > min_spacing:
                new_samples.append(sample)

        return new_samples

    def get_config(self) -> dict:
        """Get sampler configuration for metadata.

        Returns:
            Dictionary with sampler configuration
        """
        config = super().get_config()
        config.update({
            "coarse_samples": self.coarse_samples,
            "refine_factor": self.refine_factor,
            "curvature_threshold": self.curvature_threshold,
            "refinement_window": self.refinement_window,
        })
        return config
