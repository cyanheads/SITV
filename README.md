<div align="center">
  <h1>Self-Inverse Task Vectors (SITV)</h1>
  <p><b>Exploring the loss landscape geometry of neural network task vectors. Maps L(M_base + Î±T) to find optimal scaling factors and investigate whether self-inverse properties from rotation groups emerge in parameter space.</b></p>
  <p>Loosely inspired by <a href="https://arxiv.org/abs/2502.14367"><i>Walks in Rotation Spaces Return Home when Doubled and Scaled</i></a> (Eckmann & Tlusty, 2025)</p>
</div>

<div align="center">

[![Version](https://img.shields.io/badge/Version-0.10.1-blue.svg?style=flat-square)](./CHANGELOG.md) [![Python](https://img.shields.io/badge/Python-3.12+-3776AB.svg?style=flat-square)](https://www.python.org/) [![PyTorch](https://img.shields.io/badge/PyTorch-2.8.0+-EE4C2C.svg?style=flat-square)](https://pytorch.org/) [![License](https://img.shields.io/badge/License-MIT-green.svg?style=flat-square)](./LICENSE) [![Status](https://img.shields.io/badge/Status-Research-yellow.svg?style=flat-square)](https://github.com/cyanheads/SITV)

</div>

---

## ðŸ”¬ Research Question

**What does the loss landscape look like along the task vector direction? Does L(Î±) cross L(M_base) at any Î± â‰  0?**

For a task vector `T = M_finetuned - M_base`, I sweep Î± âˆˆ [-3.0, 3.0] and plot `L(M_base + Î±T)` to discover:

| Metric                | Description                                                     |
| :-------------------- | :-------------------------------------------------------------- |
| **Zero-crossings**    | Values of Î± where loss returns to baseline (self-inverse points) |
| **Optimal scaling**   | Best Î± for task performance vs general knowledge trade-off       |
| **Landscape shape**   | Monotonic, periodic, or symmetric loss behavior                  |

### Connection to Rotation Groups

Eckmann & Tlusty (2025) prove that rotation group walks have abundant special Î» values where `[W(Î»)]Â² = I` (self-inverse property). When W(Î») is a 180Â° rotation, squaring returns to identity.

**Exploration**: I test whether neural loss landscapes exhibit analogous special scaling factors where loss functionally returns to baseline, even though task vectors lack the group structure of rotations.

## ðŸ“Š Methodology

### Loss Landscape Sweep

| Step | Operation                                    | Description                                              |
| :--- | :------------------------------------------- | :------------------------------------------------------- |
| 1    | **Create task vector**                       | `T = M_finetuned - M_base`                               |
| 2    | **Sample Î± values**                          | 100 points uniformly distributed in [-3.0, 3.0]          |
| 3    | **Compute models**                           | For each Î±: `M(Î±) = M_base + Î±T`                         |
| 4    | **Evaluate loss**                            | Measure `L(Î±)` on general and task-specific datasets     |
| 5    | **Identify special points**                  | Zero-crossings, minimum loss, optimal task performance   |
| 6    | **Visualize landscape**                      | Generate 2x2 plot grid with loss curves and analysis     |

### Performance Optimization

The implementation uses **in-place parameter modification**, achieving **10-100x speedup** compared to model reloading for each Î± value. This makes sweeping large models (e.g., 12B parameters) practical.

### Metrics Tracked

| Metric                  | Formula                  | Interpretation                                   |
| :---------------------- | :----------------------- | :----------------------------------------------- |
| **Loss L(Î±)**           | Loss of `M_base + Î±T`    | Primary metric - model performance at scaling Î±  |
| **Functional Return**   | `\|L(Î±) - L(M_base)\|`   | Distance from baseline loss                      |
| **Task Performance**    | Task-specific loss       | Performance on fine-tuning task                  |
| **Zero-crossings**      | Î± where `\|L(Î±) - L_base\| < threshold` | Self-inverse scaling factors |

## ðŸš€ Getting Started

### Prerequisites

- **Python 3.12+** (required for modern type hints)
- **GPU recommended** (CUDA, Apple Silicon MPS, or CPU fallback)
- **~40GB disk space** (for model downloads and checkpoints)

### Installation

```bash
# Clone the repository
git clone https://github.com/cyanheads/SITV.git
cd SITV

# Install dependencies
pip install -e .

# For development (includes ruff, mypy, pytest)
pip install -e ".[dev]"

# For Bayesian optimization sampling (optional)
pip install -e ".[bayesian]"
```

### Quick Start

Run the experiment with default settings:

```bash
python main.py
```

This will:
1. Load base model (`google/gemma-3-4b-it` by default)
2. Fine-tune on sentiment analysis task
3. Compute task vector `T = M_finetuned - M_base`
4. Sweep Î± from -3.0 to 3.0 (150 samples)
5. Evaluate `L(M_base + Î±T)` for each Î±
6. Generate visualizations and save to `./outputs/`
7. **Save models** for future re-analysis (new!)

### Command-Line Options

The CLI has been intentionally simplified to avoid configuration drift. All experiment parameters should be configured in `config.yaml`.

```bash
# Full experiment with default parameters
python main.py

# Use custom config file
python main.py --config my_config.yaml

# Analysis-only mode (skip fine-tuning, use saved models)
python main.py --analysis-only

# Combine custom config with analysis-only mode
python main.py --config my_config.yaml --analysis-only

# View all options
python main.py --help
```

**Analysis-Only Mode**: After running a full experiment, the base and fine-tuned models are saved to `outputs/saved_base_model/` and `outputs/saved_finetuned_model/`. Use `--analysis-only` to skip the time-consuming fine-tuning step and re-run the alpha sweep. To change experiment parameters (alpha range, sample count, etc.), edit `config.yaml` and run with `--analysis-only`. This is useful for:
- Testing different alpha ranges without re-training
- Increasing sample density for finer resolution
- Generating new visualizations with different settings

### Output Files

| File                            | Description                                          |
| :------------------------------ | :--------------------------------------------------- |
| `loss_landscape_sweep.png`      | 2x2 visualization grid showing loss curves           |
| `loss_landscape_results.json`   | Complete results with all Î± values and metrics       |
| `experiment_report.md`          | Comprehensive Markdown report for LLM analysis       |
| `experiment_metrics.json`       | Complete timing and performance metrics              |
| `saved_base_model/`             | Saved base model (for analysis-only mode)            |
| `saved_finetuned_model/`        | Saved fine-tuned model (for analysis-only mode)      |

## ðŸ“ˆ Interpreting Results

### If Zero-Crossings Found (L(Î±) â‰ˆ L(M_base) for Î± â‰  0)

âœ“ Suggests special scaling factors exist (analogous to 180Â° rotations)
âœ“ Rich geometric structure in loss landscapes
âœ“ Potential applications in model merging and multi-task learning
âœ“ Indicates non-monotonic loss behavior along task vector direction

### If No Zero-Crossings Found

âœ“ Loss is monotonic along task vector direction
âœ“ Task vectors lack rotation-like symmetry properties
âœ“ Optimal scaling is straightforward (minimum of monotonic curve)
âœ“ Still provides insights into parameter space geometry and optimal Î± values

## ðŸ› ï¸ Use Cases

| Application                | Description                                                          |
| :------------------------- | :------------------------------------------------------------------- |
| **Model Merging**          | Identify optimal Î± for combining base and fine-tuned models          |
| **Task Vector Arithmetic** | Understand valid scaling ranges for task vector composition          |
| **Multi-task Learning**    | Balance general knowledge vs task-specific performance               |
| **Parameter Space Study**  | Visualize loss landscape geometry and functional structure           |
| **Transfer Learning**      | Guide fine-tuning strategies with landscape insights                 |

## âš™ï¸ Configuration

### Configuration File (config.yaml)

The primary way to configure SITV is through `config.yaml` in the project root. All settings have sensible defaults optimized for modern GPUs.

**Example config.yaml:**

```yaml
# Reproducibility Configuration
reproducibility:
  seed: 42  # Random seed for reproducible experiments (Python, NumPy, PyTorch)
  # Set to null to use non-deterministic behavior

# Model Configuration
model:
  name: "google/gemma-3-4b-it"
  device: null  # auto-detect

# Task Configuration
task:
  name: "sentiment_positive"

# Evaluation Configuration
evaluation:
  general_dataset: "combined"  # Options: mixed_domain, wikitext, coding, common_knowledge, combined
  batch_size: 8  # Number of texts per forward pass (higher = faster but more memory)
  enable_mixed_precision: true  # Use FP16/BF16 for 1.5-2x speedup (auto-selects BF16 on Ampere+ GPUs)
  max_length: 1024  # Maximum sequence length for evaluation tokenization

# Output Configuration
output:
  dir: "outputs"
  analysis_only: false

# Fine-Tuning Configuration
fine_tuning:
  num_epochs: 2
  learning_rate: 5.0e-5
  batch_size: 16
  max_length: 512
  data_repetition_factor: 100  # Multiply training examples (30 unique Ã— 100 = 3000 total)

# Alpha Sweep Configuration
alpha_sweep:
  alpha_min: -3.0
  alpha_max: 3.0
  num_samples: 150
  enable_squaring_test: true
  threshold: 0.1
  sampling_strategy: "uniform"  # Options: uniform, adaptive, bayesian
  # enable_gradient_analysis: false  # Set true to compute dL/dÎ± and find critical points

# 2D Composition Configuration
composition_2d:
  enable: false
  alpha_min: -2.0
  alpha_max: 2.0
  beta_min: -2.0
  beta_max: 2.0
  num_samples_per_dim: 30
```

**Using custom config files:**

```bash
python main.py --config my_custom_config.yaml
```

**Configuration Philosophy:** All experiment settings are configured via YAML to maintain a single source of truth and avoid configuration drift. The only CLI options are `--config` (to specify a custom config file) and `--analysis-only` (to skip fine-tuning and use saved models).

### Hardware Support

The code automatically detects and optimizes for available hardware:

| Hardware         | Detection                      | Notes                                  |
| :--------------- | :----------------------------- | :------------------------------------- |
| **CUDA**         | NVIDIA GPUs                    | Recommended for large models           |
| **MPS**          | Apple Silicon (M1/M2/M3)       | Native acceleration on macOS           |
| **CPU**          | Fallback                       | Slower but works universally           |

## ðŸ“‚ Project Structure

**v0.3.0+**: The project has been refactored into a modular package architecture with 30+ modules organized in 7 layers.

```
SITV/
â”œâ”€â”€ main.py                      # Thin entry point (44 lines)
â”œâ”€â”€ sitv/                        # Main package (30+ modules, 7,899 lines)
â”‚   â”œâ”€â”€ data/                    # Data models and task definitions
â”‚   â”‚   â”œâ”€â”€ models.py            # AlphaSweepResult, ExperimentMetrics, etc.
â”‚   â”‚   â”œâ”€â”€ tasks.py             # Predefined task definitions
â”‚   â”‚   â””â”€â”€ loader.py            # Dataset loader for text files
â”‚   â”œâ”€â”€ core/                    # Core services (device, task vectors, evaluation)
â”‚   â”‚   â”œâ”€â”€ device.py            # Hardware detection and management
â”‚   â”‚   â”œâ”€â”€ task_vector.py       # Task vector operations
â”‚   â”‚   â”œâ”€â”€ evaluation.py        # Model evaluation and perplexity
â”‚   â”‚   â”œâ”€â”€ validation.py        # Input validation and error checking
â”‚   â”‚   â””â”€â”€ error_handling.py    # Error handling and retry logic
â”‚   â”œâ”€â”€ models/                  # Model management (loading, saving, fine-tuning)
â”‚   â”‚   â”œâ”€â”€ loader.py            # Model and tokenizer operations
â”‚   â”‚   â””â”€â”€ fine_tuner.py        # Fine-tuning service with progress tracking
â”‚   â”œâ”€â”€ experiments/             # Experiment orchestration and implementations
â”‚   â”‚   â”œâ”€â”€ base.py              # Abstract Experiment base class
â”‚   â”‚   â”œâ”€â”€ config.py            # Configuration classes
â”‚   â”‚   â”œâ”€â”€ alpha_sweep.py       # 1D alpha sweep experiment
â”‚   â”‚   â”œâ”€â”€ composition_2d.py    # 2D composition experiment
â”‚   â”‚   â”œâ”€â”€ orchestrator.py      # ExperimentOrchestrator
â”‚   â”‚   â””â”€â”€ sampling/            # Sampling strategies (uniform, adaptive, bayesian)
â”‚   â”œâ”€â”€ analysis/                # Results analysis
â”‚   â”‚   â”œâ”€â”€ analyzer.py          # Zero-crossing detection, min loss finding
â”‚   â”‚   â””â”€â”€ gradient/            # Gradient analysis and critical point detection
â”‚   â”œâ”€â”€ reporting/               # Report generation
â”‚   â”‚   â””â”€â”€ markdown.py          # Markdown report generator
â”‚   â”œâ”€â”€ visualization/           # Plotting and visualization
â”‚   â”‚   â””â”€â”€ plotter.py           # 1D and 2D plotting utilities
â”‚   â”œâ”€â”€ io/                      # File I/O operations
â”‚   â”‚   â”œâ”€â”€ file_manager.py      # JSON and figure saving
â”‚   â”‚   â””â”€â”€ paths.py             # Path management
â”‚   â”œâ”€â”€ utils/                   # Utilities
â”‚   â”‚   â”œâ”€â”€ console.py           # Console output formatting
â”‚   â”‚   â”œâ”€â”€ progress.py          # Progress tracking with ETA
â”‚   â”‚   â””â”€â”€ timing.py            # Timing utilities
â”‚   â””â”€â”€ cli/                     # Command-line interface
â”‚       â””â”€â”€ args_parser.py       # Argument parsing
â”œâ”€â”€ data/                        # Dataset files (text format)
â”‚   â”œâ”€â”€ general/                 # General evaluation datasets
â”‚   â”‚   â”œâ”€â”€ mixed_domain.txt     # Mixed domain texts
â”‚   â”‚   â”œâ”€â”€ wikitext.txt         # Wikipedia-style texts
â”‚   â”‚   â”œâ”€â”€ coding.txt           # Programming-related texts
â”‚   â”‚   â””â”€â”€ common_knowledge.txt # General knowledge texts
â”‚   â”œâ”€â”€ tasks/                   # Task training datasets
â”‚   â”‚   â”œâ”€â”€ sentiment_positive.txt
â”‚   â”‚   â”œâ”€â”€ sentiment_negative.txt
â”‚   â”‚   â”œâ”€â”€ instruction_following.txt
â”‚   â”‚   â””â”€â”€ qa_factual.txt
â”‚   â””â”€â”€ eval/                    # Task evaluation datasets
â”‚       â”œâ”€â”€ sentiment_positive_eval.txt
â”‚       â”œâ”€â”€ sentiment_negative_eval.txt
â”‚       â”œâ”€â”€ instruction_following_eval.txt
â”‚       â””â”€â”€ qa_factual_eval.txt
â”œâ”€â”€ tests/                       # Test suite
â”‚   â”œâ”€â”€ conftest.py              # Pytest fixtures
â”‚   â”œâ”€â”€ test_data_models.py      # Data model tests
â”‚   â”œâ”€â”€ test_device.py           # Device management tests
â”‚   â”œâ”€â”€ test_model_management.py # Model service tests
â”‚   â””â”€â”€ test_task_vector.py      # Task vector tests
â”œâ”€â”€ archive/                     # Archived code
â”‚   â””â”€â”€ main_original.py         # Original 2,232-line monolithic version
â”œâ”€â”€ pyproject.toml               # Project configuration and dependencies
â”œâ”€â”€ CHANGELOG.md                 # Version history and release notes
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ .gitattributes               # Git LFS and line ending configuration
â”œâ”€â”€ .gitignore                   # Python, model, and output file patterns
â””â”€â”€ outputs/                     # Generated visualizations and results (gitignored)
```

## ðŸ§‘â€ðŸ’» Development

### Testing

The project includes a test suite covering core functionality:

```bash
# Run all tests
pytest

# Run with verbose output
pytest -v

# Run specific test file
pytest tests/test_data_models.py

# Run with coverage
pytest --cov=sitv
```

### Code Quality

```bash
# Lint code
ruff check .

# Format code
ruff format .

# Type checking
mypy main.py sitv/

# Run all quality checks
ruff check . && ruff format --check . && mypy main.py sitv/ && pytest
```

### Development Workflow

1. **Fork the repository**
2. **Create a feature branch**: `git checkout -b feature/amazing-feature`
3. **Make your changes** following the existing patterns
4. **Run quality checks**: `ruff check . && mypy main.py`
5. **Commit with conventional commits**: `git commit -m "feat: add feature"`
6. **Push and open a Pull Request**

## ðŸ“š Requirements

### Core Dependencies

| Package         | Version  | Purpose                                |
| :-------------- | :------- | :------------------------------------- |
| **Python**      | 3.12+    | Modern type hints and language features |
| **PyTorch**     | 2.5.0+   | Neural network operations              |
| **Transformers** | 4.50.0+ | Pre-trained language models            |
| **NumPy**       | 2.0.0+   | Numerical computing                    |
| **Matplotlib**  | 3.9.0+   | Visualization and plotting             |
| **Accelerate**  | 0.20.0+  | Multi-GPU and mixed precision training |
| **SciPy**       | 1.11.0+  | Gradient analysis and smoothing        |
| **PyYAML**      | 6.0+     | Configuration file parsing             |

### Optional Dependencies

- **scikit-learn** 1.0+ - Bayesian optimization sampling (install with `pip install -e ".[bayesian]"`)

### Development Dependencies

- **ruff** - Fast Python linter and formatter
- **mypy** - Static type checking
- **pytest** - Testing framework
- **types-PyYAML** - Type stubs for YAML module

See [pyproject.toml](pyproject.toml) for complete dependency specifications.

## ðŸ“– Citation

If you use this code in your research, please cite both this repository and the inspiring paper:

```bibtex
@misc{sitv2025,
  title={Self-Inverse Task Vectors: Exploring Loss Landscapes Along Task Vector Directions},
  author={Hand, Casey},
  year={2025},
  publisher={GitHub},
  url={https://github.com/cyanheads/SITV}
}

@article{eckmann2025walks,
  title={Walks in Rotation Spaces Return Home when Doubled and Scaled},
  author={Eckmann, Jean-Pierre and Tlusty, Tsvi},
  journal={arXiv preprint arXiv:2502.14367v3},
  year={2025}
}
```

## ðŸ¤ Contributing

Contributions are welcome! Areas of interest:

- **Additional tasks**: Implement more fine-tuning tasks beyond sentiment analysis
- **Visualization improvements**: Enhanced plots and interactive visualizations
- **Performance optimizations**: Further speedups for large-scale experiments
- **Theoretical analysis**: Mathematical insights into loss landscape geometry
- **Empirical studies**: Test on diverse model architectures and tasks

Please open an issue to discuss major changes before submitting a PR.

## ðŸ“œ License

This project is licensed under the MIT License. See the [LICENSE](./LICENSE) file for details.

## ðŸ™ Acknowledgments

This research explores the fascinating connection between rotation group theory and neural network parameter spaces. Special thanks to:

- **Jean-Pierre Eckmann & Tsvi Tlusty** for their groundbreaking work on rotation walks
- **The HuggingFace team** for Transformers and model hosting
- **The PyTorch team** for the deep learning framework

---

<div align="center">
  <p><i>Exploring the geometry of neural loss landscapes, one task vector at a time.</i></p>
  <p>
    <a href="https://github.com/cyanheads/SITV/issues">Report Bug</a> â€¢
    <a href="https://github.com/cyanheads/SITV/issues">Request Feature</a> â€¢
    <a href="https://github.com/sponsors/cyanheads">Sponsor</a>
  </p>
</div>
