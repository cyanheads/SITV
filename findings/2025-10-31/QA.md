root@1ea30814126c:/workspace/sitv# python main.py

======================================================================
SITV EXPERIMENT START
======================================================================
Model: google/gemma-3-4b-it
Task: qa_factual
General Eval Dataset: combined
Device: cuda
Output: outputs
Analysis Only: False

Fine-Tuning:
  Epochs: 4
  Learning rate: 1.00e-04
  Batch size: 32
  Max length: 1024
  Data repetition: 100x

Alpha Sweep:
  Range: [-0.5, 1.5]
  Samples: 50
  Squaring test: True
======================================================================


======================================================================
MODEL FINE-TUNING
======================================================================
Loading model: google/gemma-3-4b-it
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:34<00:00, 17.37s/it]
Model loaded successfully

======================================================================
FINE-TUNING MODEL
======================================================================
  Training examples: 3000
  Epochs: 4
  Learning rate: 1.00e-04
  Batch size: 32
  Max length: 1024
  Started: 2025-10-31 12:08:48
The model is already on multiple devices. Skipping the move to device specified in `args`.
  0%|                                                                                                                      | 0/376 [00:00<?, ?it/s]
────────────────────────────────────────────────────────────
Epoch 1/4
────────────────────────────────────────────────────────────
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|██▉                                                                                                          | 10/376 [00:03<01:38,  3.71it/s]  Step 10/376 | Loss: 1.0579 | LR: 9.76e-05 | Grad: 2.81 | ETA: calculating...
{'loss': 1.0579, 'grad_norm': 2.8125, 'learning_rate': 9.760638297872341e-05, 'epoch': 0.11}                                                       
  5%|█████▊                                                                                                       | 20/376 [00:05<01:33,  3.82it/s]  Step 20/376 | Loss: 0.2119 | LR: 9.49e-05 | Grad: 2.34 | ETA: 7.8m
{'loss': 0.2119, 'grad_norm': 2.34375, 'learning_rate': 9.49468085106383e-05, 'epoch': 0.21}                                                       
  8%|████████▋                                                                                                    | 30/376 [00:08<01:30,  3.83it/s]  Step 30/376 | Loss: 0.1346 | LR: 9.23e-05 | Grad: 1.08 | ETA: 10.0m
{'loss': 0.1346, 'grad_norm': 1.078125, 'learning_rate': 9.228723404255319e-05, 'epoch': 0.32}                                                     
 11%|███████████▌                                                                                                 | 40/376 [00:11<01:27,  3.82it/s]  Step 40/376 | Loss: 0.1309 | LR: 8.96e-05 | Grad: 1.25 | ETA: 11.0m
{'loss': 0.1309, 'grad_norm': 1.25, 'learning_rate': 8.96276595744681e-05, 'epoch': 0.43}                                                          
 13%|██████████████▍                                                                                              | 50/376 [00:13<01:25,  3.83it/s]  Step 50/376 | Loss: 0.1315 | LR: 8.70e-05 | Grad: 2.55 | ETA: 11.4m
{'loss': 0.1315, 'grad_norm': 2.546875, 'learning_rate': 8.696808510638299e-05, 'epoch': 0.53}                                                     
 16%|█████████████████▍                                                                                           | 60/376 [00:16<01:22,  3.83it/s]  Step 60/376 | Loss: 0.1149 | LR: 8.43e-05 | Grad: 0.85 | ETA: 11.5m
{'loss': 0.1149, 'grad_norm': 0.84765625, 'learning_rate': 8.430851063829787e-05, 'epoch': 0.64}                                                   
 19%|████████████████████▎                                                                                        | 70/376 [00:18<01:19,  3.83it/s]  Step 70/376 | Loss: 0.1048 | LR: 8.16e-05 | Grad: 0.61 | ETA: 11.4m
{'loss': 0.1048, 'grad_norm': 0.60546875, 'learning_rate': 8.164893617021278e-05, 'epoch': 0.74}                                                   
 21%|███████████████████████▏                                                                                     | 80/376 [00:21<01:17,  3.83it/s]  Step 80/376 | Loss: 0.1031 | LR: 7.90e-05 | Grad: 0.63 | ETA: 11.3m
{'loss': 0.1031, 'grad_norm': 0.6328125, 'learning_rate': 7.898936170212767e-05, 'epoch': 0.85}                                                    
 24%|██████████████████████████                                                                                   | 90/376 [00:24<01:14,  3.83it/s]  Step 90/376 | Loss: 0.1056 | LR: 7.63e-05 | Grad: 0.79 | ETA: 11.1m
{'loss': 0.1056, 'grad_norm': 0.7890625, 'learning_rate': 7.632978723404256e-05, 'epoch': 0.96}                                                    
 25%|███████████████████████████▎                                                                                 | 94/376 [00:25<01:17,  3.64it/s]
────────────────────────────────────────────────────────────
Epoch 2/4
────────────────────────────────────────────────────────────
 27%|████████████████████████████▋                                                                               | 100/376 [00:26<01:12,  3.81it/s]  Step 100/376 | Loss: 0.1050 | LR: 7.37e-05 | Grad: 0.88 | ETA: 10.8m
{'loss': 0.105, 'grad_norm': 0.875, 'learning_rate': 7.367021276595744e-05, 'epoch': 1.06}                                                         
 29%|███████████████████████████████▌                                                                            | 110/376 [00:29<01:07,  3.93it/s]  Step 110/376 | Loss: 0.1035 | LR: 7.10e-05 | Grad: 0.93 | ETA: 10.5m
{'loss': 0.1035, 'grad_norm': 0.93359375, 'learning_rate': 7.101063829787235e-05, 'epoch': 1.17}                                                   
 32%|██████████████████████████████████▍                                                                         | 120/376 [00:31<01:04,  3.94it/s]  Step 120/376 | Loss: 0.0989 | LR: 6.84e-05 | Grad: 0.66 | ETA: 10.2m
{'loss': 0.0989, 'grad_norm': 0.65625, 'learning_rate': 6.835106382978724e-05, 'epoch': 1.28}                                                      
 35%|█████████████████████████████████████▎                                                                      | 130/376 [00:34<01:01,  3.99it/s]  Step 130/376 | Loss: 0.0987 | LR: 6.57e-05 | Grad: 0.55 | ETA: 9.8m
{'loss': 0.0987, 'grad_norm': 0.55078125, 'learning_rate': 6.569148936170213e-05, 'epoch': 1.38}                                                   
 37%|████████████████████████████████████████▏                                                                   | 140/376 [00:36<00:58,  4.01it/s]  Step 140/376 | Loss: 0.1000 | LR: 6.30e-05 | Grad: 0.90 | ETA: 9.5m
{'loss': 0.1, 'grad_norm': 0.90234375, 'learning_rate': 6.303191489361703e-05, 'epoch': 1.49}                                                      
 40%|███████████████████████████████████████████                                                                 | 150/376 [00:39<00:56,  4.02it/s]  Step 150/376 | Loss: 0.0995 | LR: 6.04e-05 | Grad: 0.80 | ETA: 9.1m
{'loss': 0.0995, 'grad_norm': 0.8046875, 'learning_rate': 6.037234042553191e-05, 'epoch': 1.6}                                                     
 43%|█████████████████████████████████████████████▉                                                              | 160/376 [00:41<00:53,  4.01it/s]  Step 160/376 | Loss: 0.0971 | LR: 5.77e-05 | Grad: 0.69 | ETA: 8.7m
{'loss': 0.0971, 'grad_norm': 0.69140625, 'learning_rate': 5.7712765957446814e-05, 'epoch': 1.7}                                                   
 45%|████████████████████████████████████████████████▊                                                           | 170/376 [00:44<00:51,  4.01it/s]  Step 170/376 | Loss: 0.0948 | LR: 5.51e-05 | Grad: 0.71 | ETA: 8.3m
{'loss': 0.0948, 'grad_norm': 0.70703125, 'learning_rate': 5.5053191489361697e-05, 'epoch': 1.81}                                                  
 48%|███████████████████████████████████████████████████▋                                                        | 180/376 [00:46<00:48,  4.01it/s]  Step 180/376 | Loss: 0.0943 | LR: 5.24e-05 | Grad: 0.43 | ETA: 7.9m
{'loss': 0.0943, 'grad_norm': 0.427734375, 'learning_rate': 5.23936170212766e-05, 'epoch': 1.91}                                                   
 50%|██████████████████████████████████████████████████████                                                      | 188/376 [00:48<00:45,  4.15it/s]
────────────────────────────────────────────────────────────
Epoch 3/4
────────────────────────────────────────────────────────────
 51%|██████████████████████████████████████████████████████▌                                                     | 190/376 [00:49<00:45,  4.07it/s]  Step 190/376 | Loss: 0.0947 | LR: 4.97e-05 | Grad: 0.61 | ETA: 7.5m
{'loss': 0.0947, 'grad_norm': 0.60546875, 'learning_rate': 4.973404255319149e-05, 'epoch': 2.02}                                                   
 53%|█████████████████████████████████████████████████████████▍                                                  | 200/376 [00:51<00:43,  4.02it/s]  Step 200/376 | Loss: 0.0973 | LR: 4.71e-05 | Grad: 0.77 | ETA: 7.1m
{'loss': 0.0973, 'grad_norm': 0.76953125, 'learning_rate': 4.7074468085106385e-05, 'epoch': 2.13}                                                  
 56%|████████████████████████████████████████████████████████████▎                                               | 210/376 [00:54<00:41,  4.01it/s]  Step 210/376 | Loss: 0.0954 | LR: 4.44e-05 | Grad: 0.46 | ETA: 6.7m
{'loss': 0.0954, 'grad_norm': 0.46484375, 'learning_rate': 4.441489361702128e-05, 'epoch': 2.23}                                                   
 59%|███████████████████████████████████████████████████████████████▏                                            | 220/376 [00:56<00:38,  4.00it/s]  Step 220/376 | Loss: 0.0956 | LR: 4.18e-05 | Grad: 0.50 | ETA: 6.3m
{'loss': 0.0956, 'grad_norm': 0.50390625, 'learning_rate': 4.175531914893617e-05, 'epoch': 2.34}                                                   
 61%|██████████████████████████████████████████████████████████████████                                          | 230/376 [00:59<00:35,  4.09it/s]  Step 230/376 | Loss: 0.0927 | LR: 3.91e-05 | Grad: 0.56 | ETA: 5.9m
{'loss': 0.0927, 'grad_norm': 0.5625, 'learning_rate': 3.9095744680851066e-05, 'epoch': 2.45}                                                      
 64%|████████████████████████████████████████████████████████████████████▉                                       | 240/376 [01:01<00:31,  4.27it/s]  Step 240/376 | Loss: 0.0941 | LR: 3.64e-05 | Grad: 0.63 | ETA: 5.5m
{'loss': 0.0941, 'grad_norm': 0.62890625, 'learning_rate': 3.6436170212765955e-05, 'epoch': 2.55}                                                  
 66%|███████████████████████████████████████████████████████████████████████▊                                    | 250/376 [01:04<00:29,  4.28it/s]  Step 250/376 | Loss: 0.0942 | LR: 3.38e-05 | Grad: 0.43 | ETA: 5.1m
{'loss': 0.0942, 'grad_norm': 0.43359375, 'learning_rate': 3.377659574468085e-05, 'epoch': 2.66}                                                   
 69%|██████████████████████████████████████████████████████████████████████████▋                                 | 260/376 [01:06<00:27,  4.29it/s]  Step 260/376 | Loss: 0.0937 | LR: 3.11e-05 | Grad: 0.68 | ETA: 4.7m
{'loss': 0.0937, 'grad_norm': 0.6796875, 'learning_rate': 3.111702127659575e-05, 'epoch': 2.77}                                                    
 72%|█████████████████████████████████████████████████████████████████████████████▌                              | 270/376 [01:08<00:24,  4.28it/s]  Step 270/376 | Loss: 0.0919 | LR: 2.85e-05 | Grad: 0.60 | ETA: 4.3m
{'loss': 0.0919, 'grad_norm': 0.59765625, 'learning_rate': 2.845744680851064e-05, 'epoch': 2.87}                                                   
 74%|████████████████████████████████████████████████████████████████████████████████▍                           | 280/376 [01:11<00:22,  4.28it/s]  Step 280/376 | Loss: 0.0913 | LR: 2.58e-05 | Grad: 0.49 | ETA: 3.9m
{'loss': 0.0913, 'grad_norm': 0.4921875, 'learning_rate': 2.5797872340425532e-05, 'epoch': 2.98}                                                   
 75%|█████████████████████████████████████████████████████████████████████████████████                           | 282/376 [01:11<00:20,  4.48it/s]
────────────────────────────────────────────────────────────
Epoch 4/4
────────────────────────────────────────────────────────────
 77%|███████████████████████████████████████████████████████████████████████████████████▎                        | 290/376 [01:13<00:20,  4.30it/s]  Step 290/376 | Loss: 0.0912 | LR: 2.31e-05 | Grad: 0.40 | ETA: 3.5m
{'loss': 0.0912, 'grad_norm': 0.40234375, 'learning_rate': 2.3138297872340425e-05, 'epoch': 3.09}                                                  
 80%|██████████████████████████████████████████████████████████████████████████████████████▏                     | 300/376 [01:15<00:17,  4.29it/s]  Step 300/376 | Loss: 0.0924 | LR: 2.05e-05 | Grad: 0.61 | ETA: 3.1m
{'loss': 0.0924, 'grad_norm': 0.609375, 'learning_rate': 2.047872340425532e-05, 'epoch': 3.19}                                                     
 82%|█████████████████████████████████████████████████████████████████████████████████████████                   | 310/376 [01:18<00:15,  4.29it/s]  Step 310/376 | Loss: 0.0918 | LR: 1.78e-05 | Grad: 0.54 | ETA: 2.7m
{'loss': 0.0918, 'grad_norm': 0.5390625, 'learning_rate': 1.7819148936170214e-05, 'epoch': 3.3}                                                    
 85%|███████████████████████████████████████████████████████████████████████████████████████████▉                | 320/376 [01:20<00:13,  4.29it/s]  Step 320/376 | Loss: 0.0915 | LR: 1.52e-05 | Grad: 0.60 | ETA: 2.2m
{'loss': 0.0915, 'grad_norm': 0.59765625, 'learning_rate': 1.5159574468085108e-05, 'epoch': 3.4}                                                   
 88%|██████████████████████████████████████████████████████████████████████████████████████████████▊             | 330/376 [01:22<00:11,  4.13it/s]  Step 330/376 | Loss: 0.0902 | LR: 1.25e-05 | Grad: 0.51 | ETA: 1.8m
{'loss': 0.0902, 'grad_norm': 0.5078125, 'learning_rate': 1.25e-05, 'epoch': 3.51}                                                                 
 90%|█████████████████████████████████████████████████████████████████████████████████████████████████▋          | 340/376 [01:25<00:09,  3.74it/s]  Step 340/376 | Loss: 0.0911 | LR: 9.84e-06 | Grad: 0.39 | ETA: 1.4m
{'loss': 0.0911, 'grad_norm': 0.388671875, 'learning_rate': 9.840425531914895e-06, 'epoch': 3.62}                                                  
 93%|████████████████████████████████████████████████████████████████████████████████████████████████████▌       | 350/376 [01:28<00:06,  3.81it/s]  Step 350/376 | Loss: 0.0899 | LR: 7.18e-06 | Grad: 0.39 | ETA: 1.0m
{'loss': 0.0899, 'grad_norm': 0.39453125, 'learning_rate': 7.180851063829788e-06, 'epoch': 3.72}                                                   
 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████▍    | 360/376 [01:30<00:04,  3.80it/s]  Step 360/376 | Loss: 0.0918 | LR: 4.52e-06 | Grad: 0.62 | ETA: 39s
{'loss': 0.0918, 'grad_norm': 0.625, 'learning_rate': 4.521276595744681e-06, 'epoch': 3.83}                                                        
 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 370/376 [01:33<00:01,  3.79it/s]  Step 370/376 | Loss: 0.0896 | LR: 1.86e-06 | Grad: 0.54 | ETA: 15s
{'loss': 0.0896, 'grad_norm': 0.53515625, 'learning_rate': 1.8617021276595745e-06, 'epoch': 3.94}                                                  
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 376/376 [01:34<00:00,  3.83it/s]  Step 376/376 | ETA: 0s
{'train_runtime': 94.9061, 'train_samples_per_second': 126.441, 'train_steps_per_second': 3.962, 'train_loss': 0.12771306313732836, 'epoch': 4.0}  
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 376/376 [01:34<00:00,  3.96it/s]

======================================================================
FINE-TUNING COMPLETE
======================================================================
  Duration: 1.6 minutes (95s)
  Final loss: 0.1277
  Total steps: 376
  Avg time/step: 0.25s
======================================================================


Reloading base model to preserve original weights...
Loading model: google/gemma-3-4b-it
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.09s/it]
Model loaded successfully

Saving models for future analysis...

Saving models for future analysis...
  Base model → outputs/saved_base_model
  Fine-tuned model → outputs/saved_finetuned_model
Models saved successfully!

Models saved to outputs/

======================================================================
COMPUTING TASK VECTOR
======================================================================
Task vector computed: ||T|| = 35.48
Computation time: 23.77s


======================================================================
LOSS LANDSCAPE SWEEP: L(M_base + αT)
======================================================================
Range: α ∈ [-0.5, 1.5]
Samples: 50
Started: 2025-10-31 12:12:05

Question: Does the loss curve cross L(M_base) at any α ≠ 0?

Cloning base model parameters...
Pre-loading task vector to device...
Computing base model loss...
Base model loss L(M_base): 2.9390

Sampling strategy: uniform
Generated 50 alpha values

[  1/50] (  2.0%) α = -0.500 | L(α)=16.3196, L(2α)=21.9858, |ΔL|=13.3805, |ΔL(2α)|=19.0467 | 14.0s | ETA: calculating...
[  2/50] (  4.0%) α = -0.459 | L(α)=15.0105, L(2α)=22.0681, |ΔL|=12.0714, |ΔL(2α)|=19.1290 | 10.2s | ETA: 11.4m
[  3/50] (  6.0%) α = -0.418 | L(α)=13.8265, L(2α)=21.6458, |ΔL|=10.8874, |ΔL(2α)|=18.7068 | 8.1s | ETA: 9.7m
[  4/50] (  8.0%) α = -0.378 | L(α)=12.7825, L(2α)=21.1279, |ΔL|=9.8435, |ΔL(2α)|=18.1888 | 8.1s | ETA: 8.4m
[  5/50] ( 10.0%) α = -0.337 | L(α)=11.6549, L(2α)=20.1815, |ΔL|=8.7158, |ΔL(2α)|=17.2425 | 8.1s | ETA: 7.7m
[  6/50] ( 12.0%) α = -0.296 | L(α)=10.7186, L(2α)=18.9537, |ΔL|=7.7795, |ΔL(2α)|=16.0146 | 8.2s | ETA: 7.3m
[  7/50] ( 14.0%) α = -0.255 | L(α)=9.9386, L(2α)=16.8315, |ΔL|=6.9996, |ΔL(2α)|=13.8924 | 8.4s | ETA: 6.9m
[  8/50] ( 16.0%) α = -0.214 | L(α)=8.3065, L(2α)=14.0868, |ΔL|=5.3674, |ΔL(2α)|=11.1477 | 8.1s | ETA: 6.7m
[  9/50] ( 18.0%) α = -0.173 | L(α)=7.2272, L(2α)=11.8876, |ΔL|=4.2881, |ΔL(2α)|=8.9486 | 8.0s | ETA: 6.4m
[ 10/50] ( 20.0%) α = -0.133 | L(α)=5.4830, L(2α)=10.0912, |ΔL|=2.5439, |ΔL(2α)|=7.1521 | 8.4s | ETA: 6.2m
[ 11/50] ( 22.0%) α = -0.092 | L(α)=3.8317, L(2α)=7.4851, |ΔL|=0.8927, |ΔL(2α)|=4.5461 | 8.0s | ETA: 6.0m
[ 12/50] ( 24.0%) α = -0.051 | L(α)=3.1343, L(2α)=4.2490, |ΔL|=0.1953, |ΔL(2α)|=1.3100 | 8.1s | ETA: 5.8m
[ 13/50] ( 26.0%) α = -0.010 | L(α)=3.1448, L(2α)=3.9961, |ΔL|=0.2058, |ΔL(2α)|=1.0570 | 8.1s | ETA: 5.6m
[ 14/50] ( 28.0%) α = +0.031 | L(α)=2.5475, L(2α)=2.4752, |ΔL|=0.3915, |ΔL(2α)|=0.4639 | 8.3s | ETA: 5.4m
[ 15/50] ( 30.0%) α = +0.071 | L(α)=2.4652, L(2α)=2.4224, |ΔL|=0.4739, |ΔL(2α)|=0.5166 | 8.6s | ETA: 5.2m
[ 16/50] ( 32.0%) α = +0.112 | L(α)=2.4225, L(2α)=2.5430, |ΔL|=0.5166, |ΔL(2α)|=0.3960 | 8.3s | ETA: 5.1m
[ 17/50] ( 34.0%) α = +0.153 | L(α)=2.4280, L(2α)=2.7488, |ΔL|=0.5111, |ΔL(2α)|=0.1902 | 8.1s | ETA: 4.9m
[ 18/50] ( 36.0%) α = +0.194 | L(α)=2.4933, L(2α)=3.0486, |ΔL|=0.4457, |ΔL(2α)|=0.1096 | 8.5s | ETA: 4.7m
[ 19/50] ( 38.0%) α = +0.235 | L(α)=2.5581, L(2α)=3.3701, |ΔL|=0.3809, |ΔL(2α)|=0.4311 | 8.1s | ETA: 4.6m
[ 20/50] ( 40.0%) α = +0.276 | L(α)=2.6685, L(2α)=3.7618, |ΔL|=0.2705, |ΔL(2α)|=0.8228 | 8.4s | ETA: 4.4m
[ 21/50] ( 42.0%) α = +0.316 | L(α)=2.7875, L(2α)=4.1733, |ΔL|=0.1515, |ΔL(2α)|=1.2343 | 8.4s | ETA: 4.3m
[ 22/50] ( 44.0%) α = +0.357 | L(α)=2.9306, L(2α)=4.5836, |ΔL|=0.0085, |ΔL(2α)|=1.6446 | 8.0s | ETA: 4.1m
[ 23/50] ( 46.0%) α = +0.398 | L(α)=3.0887, L(2α)=4.9895, |ΔL|=0.1497, |ΔL(2α)|=2.0505 | 8.1s | ETA: 4.0m
[ 24/50] ( 48.0%) α = +0.439 | L(α)=3.2522, L(2α)=5.5073, |ΔL|=0.3132, |ΔL(2α)|=2.5683 | 9.5s | ETA: 3.8m
[ 25/50] ( 50.0%) α = +0.480 | L(α)=3.4070, L(2α)=6.0641, |ΔL|=0.4680, |ΔL(2α)|=3.1250 | 8.2s | ETA: 3.7m
[ 26/50] ( 52.0%) α = +0.520 | L(α)=3.6245, L(2α)=6.6566, |ΔL|=0.6855, |ΔL(2α)|=3.7176 | 8.1s | ETA: 3.6m
[ 27/50] ( 54.0%) α = +0.561 | L(α)=3.8055, L(2α)=7.3887, |ΔL|=0.8664, |ΔL(2α)|=4.4497 | 8.3s | ETA: 3.4m
[ 28/50] ( 56.0%) α = +0.602 | L(α)=4.0163, L(2α)=8.1781, |ΔL|=1.0773, |ΔL(2α)|=5.2391 | 8.0s | ETA: 3.3m
[ 29/50] ( 58.0%) α = +0.643 | L(α)=4.2231, L(2α)=9.0460, |ΔL|=1.2840, |ΔL(2α)|=6.1069 | 8.1s | ETA: 3.1m
[ 30/50] ( 60.0%) α = +0.684 | L(α)=4.4309, L(2α)=9.9426, |ΔL|=1.4919, |ΔL(2α)|=7.0036 | 8.0s | ETA: 3.0m
[ 31/50] ( 62.0%) α = +0.724 | L(α)=4.6380, L(2α)=10.8866, |ΔL|=1.6989, |ΔL(2α)|=7.9476 | 8.0s | ETA: 2.8m
[ 32/50] ( 64.0%) α = +0.765 | L(α)=4.8383, L(2α)=11.8916, |ΔL|=1.8992, |ΔL(2α)|=8.9526 | 8.1s | ETA: 2.7m
[ 33/50] ( 66.0%) α = +0.806 | L(α)=5.0504, L(2α)=12.8840, |ΔL|=2.1113, |ΔL(2α)|=9.9449 | 8.1s | ETA: 2.5m
[ 34/50] ( 68.0%) α = +0.847 | L(α)=5.3169, L(2α)=13.8908, |ΔL|=2.3778, |ΔL(2α)|=10.9518 | 8.0s | ETA: 2.4m
[ 35/50] ( 70.0%) α = +0.888 | L(α)=5.5577, L(2α)=14.8753, |ΔL|=2.6186, |ΔL(2α)|=11.9362 | 8.1s | ETA: 2.2m
[ 36/50] ( 72.0%) α = +0.929 | L(α)=5.8356, L(2α)=15.8403, |ΔL|=2.8966, |ΔL(2α)|=12.9012 | 8.0s | ETA: 2.1m
[ 37/50] ( 74.0%) α = +0.969 | L(α)=6.1447, L(2α)=16.8368, |ΔL|=3.2057, |ΔL(2α)|=13.8978 | 8.0s | ETA: 2.0m
[ 38/50] ( 76.0%) α = +1.010 | L(α)=6.4010, L(2α)=17.7403, |ΔL|=3.4620, |ΔL(2α)|=14.8013 | 8.1s | ETA: 1.8m
[ 39/50] ( 78.0%) α = +1.051 | L(α)=6.7414, L(2α)=18.7836, |ΔL|=3.8024, |ΔL(2α)|=15.8446 | 8.0s | ETA: 1.7m
[ 40/50] ( 80.0%) α = +1.092 | L(α)=7.0960, L(2α)=19.8620, |ΔL|=4.1569, |ΔL(2α)|=16.9229 | 8.0s | ETA: 1.5m
[ 41/50] ( 82.0%) α = +1.133 | L(α)=7.4874, L(2α)=20.9921, |ΔL|=4.5484, |ΔL(2α)|=18.0531 | 8.0s | ETA: 1.4m
[ 42/50] ( 84.0%) α = +1.173 | L(α)=7.8888, L(2α)=22.0793, |ΔL|=4.9498, |ΔL(2α)|=19.1403 | 8.1s | ETA: 1.3m
[ 43/50] ( 86.0%) α = +1.214 | L(α)=8.2889, L(2α)=23.1291, |ΔL|=5.3499, |ΔL(2α)|=20.1901 | 8.2s | ETA: 1.1m
[ 44/50] ( 88.0%) α = +1.255 | L(α)=8.7452, L(2α)=24.2201, |ΔL|=5.8061, |ΔL(2α)|=21.2811 | 8.0s | ETA: 58s
[ 45/50] ( 90.0%) α = +1.296 | L(α)=9.1569, L(2α)=25.2382, |ΔL|=6.2179, |ΔL(2α)|=22.2992 | 8.0s | ETA: 50s
[ 46/50] ( 92.0%) α = +1.337 | L(α)=9.5968, L(2α)=26.3052, |ΔL|=6.6577, |ΔL(2α)|=23.3662 | 8.0s | ETA: 42s
[ 47/50] ( 94.0%) α = +1.378 | L(α)=10.0596, L(2α)=27.2872, |ΔL|=7.1206, |ΔL(2α)|=24.3482 | 8.1s | ETA: 33s
[ 48/50] ( 96.0%) α = +1.418 | L(α)=10.5379, L(2α)=28.4011, |ΔL|=7.5989, |ΔL(2α)|=25.4620 | 8.1s | ETA: 25s
[ 49/50] ( 98.0%) α = +1.459 | L(α)=11.0118, L(2α)=29.9267, |ΔL|=8.0727, |ΔL(2α)|=26.9877 | 8.0s | ETA: 17s
[ 50/50] (100.0%) α = +1.500 | L(α)=11.5094, L(2α)=31.5588, |ΔL|=8.5704, |ΔL(2α)|=28.6198 | 8.1s | ETA: 8s

Restoring base model parameters...

======================================================================
ALPHA SWEEP COMPLETE
======================================================================
  Duration: 7.0 minutes (422s)
  Samples Completed: 50/50
  Avg time/sample: 8.31s
======================================================================


======================================================================
LOSS LANDSCAPE ANALYSIS
======================================================================

Minimum General Loss (best general knowledge):
  α = +0.1122
  L(α) = 2.4225
  L(M_base) = 2.9390
  Δ = -0.5166

Minimum Task-Specific Loss (best task performance):
  α = +0.1939
  Task L(α) = 1.7019
  General L(α) = 2.4933
  Δ from base = -1.2372

Best Functional Return (smallest |L(α) - L_base|):
  1. α = +0.3571, |ΔL| = 0.008472
  2. α = +0.3980, |ΔL| = 0.149670
  3. α = +0.3163, |ΔL| = 0.151522
  4. α = -0.0510, |ΔL| = 0.195252
  5. α = -0.0102, |ΔL| = 0.205770

Zero-Crossings (where L(α) ≈ L_base for α ≠ 0):
  Found 2 crossing(s):
  1. α = +0.3571, L(α) = 2.9306, |ΔL| = 0.008472 ★
  2. α = +0.3980, L(α) = 3.0887, |ΔL| = 0.149670 ★

======================================================================
SQUARING TEST ANALYSIS: [W(λ)]² = I Analog
======================================================================

Squaring Return Points (where L(2α) ≈ L_base for α ≠ 0):
 ★ Found 1 squaring return point(s)!
  → These α values exhibit the self-inverse property: doubling brings back to base loss
  1. α = +0.1939, L(2α) = 3.0486, |ΔL(2α)| = 0.109608 ★

  INTERPRETATION:
  This suggests neural loss landscapes MAY exhibit rotation-like symmetry!
  Analogous to R(n,π)² = I in rotation groups.