root@1ea30814126c:/workspace/sitv# python main.py

======================================================================
SITV EXPERIMENT START
======================================================================
Model: google/gemma-3-4b-it
Task: sentiment_negative
General Eval Dataset: combined
Device: cuda
Output: outputs
Analysis Only: False

Fine-Tuning:
  Epochs: 4
  Learning rate: 1.00e-04
  Batch size: 32
  Max length: 1024
  Data repetition: 100x

Alpha Sweep:
  Range: [-0.5, 1.5]
  Samples: 50
  Squaring test: True
======================================================================


======================================================================
MODEL FINE-TUNING
======================================================================
Loading model: google/gemma-3-4b-it
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:42<00:00, 21.14s/it]
Model loaded successfully

======================================================================
FINE-TUNING MODEL
======================================================================
  Training examples: 3000
  Epochs: 4
  Learning rate: 1.00e-04
  Batch size: 32
  Max length: 1024
  Started: 2025-10-31 11:40:26
The model is already on multiple devices. Skipping the move to device specified in `args`.
  0%|                                                                                                                      | 0/376 [00:00<?, ?it/s]
────────────────────────────────────────────────────────────
Epoch 1/4
────────────────────────────────────────────────────────────
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|██▉                                                                                                          | 10/376 [00:03<01:43,  3.54it/s]  Step 10/376 | Loss: 1.7328 | LR: 9.76e-05 | Grad: 7.12 | ETA: calculating...
{'loss': 1.7328, 'grad_norm': 7.125, 'learning_rate': 9.760638297872341e-05, 'epoch': 0.11}                                                        
  5%|█████▊                                                                                                       | 20/376 [00:06<01:32,  3.84it/s]  Step 20/376 | Loss: 0.3314 | LR: 9.49e-05 | Grad: 2.17 | ETA: 7.7m
{'loss': 0.3314, 'grad_norm': 2.171875, 'learning_rate': 9.49468085106383e-05, 'epoch': 0.21}                                                      
  8%|████████▋                                                                                                    | 30/376 [00:09<01:39,  3.47it/s]  Step 30/376 | Loss: 0.2498 | LR: 9.23e-05 | Grad: 8.00 | ETA: 10.5m
{'loss': 0.2498, 'grad_norm': 8.0, 'learning_rate': 9.228723404255319e-05, 'epoch': 0.32}                                                          
 11%|███████████▌                                                                                                 | 40/376 [00:12<01:37,  3.45it/s]  Step 40/376 | Loss: 0.2409 | LR: 8.96e-05 | Grad: 3.38 | ETA: 11.7m
{'loss': 0.2409, 'grad_norm': 3.375, 'learning_rate': 8.96276595744681e-05, 'epoch': 0.43}                                                         
 13%|██████████████▍                                                                                              | 50/376 [00:15<01:34,  3.46it/s]  Step 50/376 | Loss: 0.2631 | LR: 8.70e-05 | Grad: 2.05 | ETA: 12.3m
{'loss': 0.2631, 'grad_norm': 2.046875, 'learning_rate': 8.696808510638299e-05, 'epoch': 0.53}                                                     
 16%|█████████████████▍                                                                                           | 60/376 [00:18<01:36,  3.26it/s]  Step 60/376 | Loss: 0.2530 | LR: 8.43e-05 | Grad: 1.45 | ETA: 12.6m
{'loss': 0.253, 'grad_norm': 1.4453125, 'learning_rate': 8.430851063829787e-05, 'epoch': 0.64}                                                     
 19%|████████████████████▎                                                                                        | 70/376 [00:21<01:26,  3.52it/s]  Step 70/376 | Loss: 0.2503 | LR: 8.16e-05 | Grad: 0.72 | ETA: 12.5m
{'loss': 0.2503, 'grad_norm': 0.71875, 'learning_rate': 8.164893617021278e-05, 'epoch': 0.74}                                                      
 21%|███████████████████████▏                                                                                     | 80/376 [00:24<01:30,  3.29it/s]  Step 80/376 | Loss: 0.2146 | LR: 7.90e-05 | Grad: 1.12 | ETA: 12.5m
{'loss': 0.2146, 'grad_norm': 1.1171875, 'learning_rate': 7.898936170212767e-05, 'epoch': 0.85}                                                    
 24%|██████████████████████████                                                                                   | 90/376 [00:26<01:21,  3.50it/s]  Step 90/376 | Loss: 0.2122 | LR: 7.63e-05 | Grad: 1.07 | ETA: 12.2m
{'loss': 0.2122, 'grad_norm': 1.0703125, 'learning_rate': 7.632978723404256e-05, 'epoch': 0.96}                                                    
 25%|███████████████████████████▎                                                                                 | 94/376 [00:28<01:24,  3.32it/s]
────────────────────────────────────────────────────────────
Epoch 2/4
────────────────────────────────────────────────────────────
 27%|████████████████████████████▋                                                                               | 100/376 [00:29<01:19,  3.46it/s]  Step 100/376 | Loss: 0.2181 | LR: 7.37e-05 | Grad: 0.80 | ETA: 12.0m
{'loss': 0.2181, 'grad_norm': 0.8046875, 'learning_rate': 7.367021276595744e-05, 'epoch': 1.06}                                                    
 29%|███████████████████████████████▌                                                                            | 110/376 [00:32<01:18,  3.40it/s]  Step 110/376 | Loss: 0.2112 | LR: 7.10e-05 | Grad: 0.61 | ETA: 11.6m
{'loss': 0.2112, 'grad_norm': 0.60546875, 'learning_rate': 7.101063829787235e-05, 'epoch': 1.17}                                                   
 32%|██████████████████████████████████▍                                                                         | 120/376 [00:35<01:13,  3.47it/s]  Step 120/376 | Loss: 0.2121 | LR: 6.84e-05 | Grad: 0.96 | ETA: 11.3m
{'loss': 0.2121, 'grad_norm': 0.9609375, 'learning_rate': 6.835106382978724e-05, 'epoch': 1.28}                                                    
 35%|█████████████████████████████████████▎                                                                      | 130/376 [00:38<01:09,  3.52it/s]  Step 130/376 | Loss: 0.2165 | LR: 6.57e-05 | Grad: 0.83 | ETA: 10.9m
{'loss': 0.2165, 'grad_norm': 0.828125, 'learning_rate': 6.569148936170213e-05, 'epoch': 1.38}                                                     
 37%|████████████████████████████████████████▏                                                                   | 140/376 [00:41<01:06,  3.53it/s]  Step 140/376 | Loss: 0.2124 | LR: 6.30e-05 | Grad: 0.80 | ETA: 10.5m
{'loss': 0.2124, 'grad_norm': 0.8046875, 'learning_rate': 6.303191489361703e-05, 'epoch': 1.49}                                                    
 40%|███████████████████████████████████████████                                                                 | 150/376 [00:44<01:02,  3.60it/s]  Step 150/376 | Loss: 0.2095 | LR: 6.04e-05 | Grad: 0.95 | ETA: 10.1m
{'loss': 0.2095, 'grad_norm': 0.9453125, 'learning_rate': 6.037234042553191e-05, 'epoch': 1.6}                                                     
 43%|█████████████████████████████████████████████▉                                                              | 160/376 [00:45<00:41,  5.15it/s]  Step 160/376 | Loss: 0.2073 | LR: 5.77e-05 | Grad: 0.69 | ETA: 9.5m
{'loss': 0.2073, 'grad_norm': 0.69140625, 'learning_rate': 5.7712765957446814e-05, 'epoch': 1.7}                                                   
 45%|████████████████████████████████████████████████▊                                                           | 170/376 [00:47<00:39,  5.19it/s]  Step 170/376 | Loss: 0.2047 | LR: 5.51e-05 | Grad: 0.81 | ETA: 8.9m
{'loss': 0.2047, 'grad_norm': 0.8125, 'learning_rate': 5.5053191489361697e-05, 'epoch': 1.81}                                                      
 48%|███████████████████████████████████████████████████▋                                                        | 180/376 [00:49<00:37,  5.19it/s]  Step 180/376 | Loss: 0.2031 | LR: 5.24e-05 | Grad: 0.86 | ETA: 8.4m
{'loss': 0.2031, 'grad_norm': 0.859375, 'learning_rate': 5.23936170212766e-05, 'epoch': 1.91}                                                      
 50%|██████████████████████████████████████████████████████                                                      | 188/376 [00:51<00:36,  5.20it/s]
────────────────────────────────────────────────────────────
Epoch 3/4
────────────────────────────────────────────────────────────
 51%|██████████████████████████████████████████████████████▌                                                     | 190/376 [00:51<00:35,  5.20it/s]  Step 190/376 | Loss: 0.2035 | LR: 4.97e-05 | Grad: 0.89 | ETA: 7.8m
{'loss': 0.2035, 'grad_norm': 0.88671875, 'learning_rate': 4.973404255319149e-05, 'epoch': 2.02}                                                   
 53%|█████████████████████████████████████████████████████████▍                                                  | 200/376 [00:53<00:34,  5.16it/s]  Step 200/376 | Loss: 0.2093 | LR: 4.71e-05 | Grad: 0.75 | ETA: 7.3m
{'loss': 0.2093, 'grad_norm': 0.74609375, 'learning_rate': 4.7074468085106385e-05, 'epoch': 2.13}                                                  
 56%|████████████████████████████████████████████████████████████▎                                               | 210/376 [00:55<00:34,  4.78it/s]  Step 210/376 | Loss: 0.2032 | LR: 4.44e-05 | Grad: 0.80 | ETA: 6.8m
{'loss': 0.2032, 'grad_norm': 0.796875, 'learning_rate': 4.441489361702128e-05, 'epoch': 2.23}                                                     
 59%|███████████████████████████████████████████████████████████████▏                                            | 220/376 [00:58<00:45,  3.40it/s]  Step 220/376 | Loss: 0.2020 | LR: 4.18e-05 | Grad: 0.77 | ETA: 6.5m
{'loss': 0.202, 'grad_norm': 0.76953125, 'learning_rate': 4.175531914893617e-05, 'epoch': 2.34}                                                    
 61%|██████████████████████████████████████████████████████████████████                                          | 230/376 [01:00<00:30,  4.84it/s]  Step 230/376 | Loss: 0.2000 | LR: 3.91e-05 | Grad: 0.87 | ETA: 6.0m
{'loss': 0.2, 'grad_norm': 0.8671875, 'learning_rate': 3.9095744680851066e-05, 'epoch': 2.45}                                                      
 64%|████████████████████████████████████████████████████████████████████▉                                       | 240/376 [01:02<00:26,  5.08it/s]  Step 240/376 | Loss: 0.1994 | LR: 3.64e-05 | Grad: 1.11 | ETA: 5.6m
{'loss': 0.1994, 'grad_norm': 1.109375, 'learning_rate': 3.6436170212765955e-05, 'epoch': 2.55}                                                    
 66%|███████████████████████████████████████████████████████████████████████▊                                    | 250/376 [01:04<00:24,  5.17it/s]  Step 250/376 | Loss: 0.2008 | LR: 3.38e-05 | Grad: 0.68 | ETA: 5.1m
{'loss': 0.2008, 'grad_norm': 0.68359375, 'learning_rate': 3.377659574468085e-05, 'epoch': 2.66}                                                   
 69%|██████████████████████████████████████████████████████████████████████████▋                                 | 260/376 [01:06<00:23,  4.97it/s]  Step 260/376 | Loss: 0.1978 | LR: 3.11e-05 | Grad: 0.83 | ETA: 4.7m
{'loss': 0.1978, 'grad_norm': 0.83203125, 'learning_rate': 3.111702127659575e-05, 'epoch': 2.77}                                                   
 72%|█████████████████████████████████████████████████████████████████████████████▌                              | 270/376 [01:08<00:22,  4.70it/s]  Step 270/376 | Loss: 0.1984 | LR: 2.85e-05 | Grad: 0.77 | ETA: 4.3m
{'loss': 0.1984, 'grad_norm': 0.7734375, 'learning_rate': 2.845744680851064e-05, 'epoch': 2.87}                                                    
 74%|████████████████████████████████████████████████████████████████████████████████▍                           | 280/376 [01:10<00:18,  5.12it/s]  Step 280/376 | Loss: 0.1986 | LR: 2.58e-05 | Grad: 0.83 | ETA: 3.8m
{'loss': 0.1986, 'grad_norm': 0.828125, 'learning_rate': 2.5797872340425532e-05, 'epoch': 2.98}                                                    
 75%|█████████████████████████████████████████████████████████████████████████████████                           | 282/376 [01:11<00:18,  5.21it/s]
────────────────────────────────────────────────────────────
Epoch 4/4
────────────────────────────────────────────────────────────
 77%|███████████████████████████████████████████████████████████████████████████████████▎                        | 290/376 [01:12<00:16,  5.19it/s]  Step 290/376 | Loss: 0.1978 | LR: 2.31e-05 | Grad: 0.62 | ETA: 3.4m
{'loss': 0.1978, 'grad_norm': 0.6171875, 'learning_rate': 2.3138297872340425e-05, 'epoch': 3.09}                                                   
 80%|██████████████████████████████████████████████████████████████████████████████████████▏                     | 300/376 [01:14<00:15,  4.97it/s]  Step 300/376 | Loss: 0.1982 | LR: 2.05e-05 | Grad: 0.68 | ETA: 3.0m
{'loss': 0.1982, 'grad_norm': 0.68359375, 'learning_rate': 2.047872340425532e-05, 'epoch': 3.19}                                                   
 82%|█████████████████████████████████████████████████████████████████████████████████████████                   | 310/376 [01:17<00:17,  3.72it/s]  Step 310/376 | Loss: 0.1947 | LR: 1.78e-05 | Grad: 0.64 | ETA: 2.6m
{'loss': 0.1947, 'grad_norm': 0.640625, 'learning_rate': 1.7819148936170214e-05, 'epoch': 3.3}                                                     
 85%|███████████████████████████████████████████████████████████████████████████████████████████▉                | 320/376 [01:20<00:15,  3.62it/s]  Step 320/376 | Loss: 0.1997 | LR: 1.52e-05 | Grad: 0.70 | ETA: 2.2m
{'loss': 0.1997, 'grad_norm': 0.69921875, 'learning_rate': 1.5159574468085108e-05, 'epoch': 3.4}                                                   
 88%|██████████████████████████████████████████████████████████████████████████████████████████████▊             | 330/376 [01:22<00:11,  3.95it/s]  Step 330/376 | Loss: 0.1970 | LR: 1.25e-05 | Grad: 0.75 | ETA: 1.8m
{'loss': 0.197, 'grad_norm': 0.75, 'learning_rate': 1.25e-05, 'epoch': 3.51}                                                                       
 90%|█████████████████████████████████████████████████████████████████████████████████████████████████▋          | 340/376 [01:25<00:09,  3.67it/s]  Step 340/376 | Loss: 0.1961 | LR: 9.84e-06 | Grad: 0.55 | ETA: 1.4m
{'loss': 0.1961, 'grad_norm': 0.546875, 'learning_rate': 9.840425531914895e-06, 'epoch': 3.62}                                                     
 93%|████████████████████████████████████████████████████████████████████████████████████████████████████▌       | 350/376 [01:28<00:07,  3.50it/s]  Step 350/376 | Loss: 0.1963 | LR: 7.18e-06 | Grad: 0.60 | ETA: 1.0m
{'loss': 0.1963, 'grad_norm': 0.6015625, 'learning_rate': 7.180851063829788e-06, 'epoch': 3.72}                                                    
 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████▍    | 360/376 [01:30<00:04,  3.79it/s]  Step 360/376 | Loss: 0.1948 | LR: 4.52e-06 | Grad: 0.80 | ETA: 39s
{'loss': 0.1948, 'grad_norm': 0.796875, 'learning_rate': 4.521276595744681e-06, 'epoch': 3.83}                                                     
 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 370/376 [01:32<00:01,  4.99it/s]  Step 370/376 | Loss: 0.1969 | LR: 1.86e-06 | Grad: 0.77 | ETA: 14s
{'loss': 0.1969, 'grad_norm': 0.765625, 'learning_rate': 1.8617021276595745e-06, 'epoch': 3.94}                                                    
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 376/376 [01:34<00:00,  5.06it/s]  Step 376/376 | ETA: 0s
{'train_runtime': 94.1569, 'train_samples_per_second': 127.447, 'train_steps_per_second': 3.993, 'train_loss': 0.2538813939119907, 'epoch': 4.0}   
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 376/376 [01:34<00:00,  3.99it/s]

======================================================================
FINE-TUNING COMPLETE
======================================================================
  Duration: 1.6 minutes (95s)
  Final loss: 0.2539
  Total steps: 376
  Avg time/step: 0.25s
======================================================================


Reloading base model to preserve original weights...
Loading model: google/gemma-3-4b-it
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.48s/it]
Model loaded successfully

Saving models for future analysis...

Saving models for future analysis...
  Base model → outputs/saved_base_model
  Fine-tuned model → outputs/saved_finetuned_model
Models saved successfully!

Models saved to outputs/

======================================================================
COMPUTING TASK VECTOR
======================================================================
Task vector computed: ||T|| = 41.02
Computation time: 33.52s


======================================================================
LOSS LANDSCAPE SWEEP: L(M_base + αT)
======================================================================
Range: α ∈ [-0.5, 1.5]
Samples: 50
Started: 2025-10-31 11:43:57

Question: Does the loss curve cross L(M_base) at any α ≠ 0?

Cloning base model parameters...
Pre-loading task vector to device...
Computing base model loss...
Base model loss L(M_base): 2.9390

Sampling strategy: uniform
Generated 50 alpha values

[  1/50] (  2.0%) α = -0.500 | L(α)=14.6843, L(2α)=29.7177, |ΔL|=11.7453, |ΔL(2α)|=26.7787 | 15.8s | ETA: calculating...
[  2/50] (  4.0%) α = -0.459 | L(α)=13.4254, L(2α)=27.8344, |ΔL|=10.4864, |ΔL(2α)|=24.8954 | 14.4s | ETA: 12.9m
[  3/50] (  6.0%) α = -0.418 | L(α)=12.4474, L(2α)=26.4377, |ΔL|=9.5084, |ΔL(2α)|=23.4987 | 11.2s | ETA: 12.1m
[  4/50] (  8.0%) α = -0.378 | L(α)=11.4939, L(2α)=23.3567, |ΔL|=8.5549, |ΔL(2α)|=20.4177 | 8.2s | ETA: 10.8m
[  5/50] ( 10.0%) α = -0.337 | L(α)=10.4511, L(2α)=20.1607, |ΔL|=7.5121, |ΔL(2α)|=17.2216 | 8.4s | ETA: 9.5m
[  6/50] ( 12.0%) α = -0.296 | L(α)=9.4622, L(2α)=17.4932, |ΔL|=6.5232, |ΔL(2α)|=14.5542 | 8.3s | ETA: 8.7m
[  7/50] ( 14.0%) α = -0.255 | L(α)=8.4946, L(2α)=15.2316, |ΔL|=5.5556, |ΔL(2α)|=12.2925 | 8.3s | ETA: 8.1m
[  8/50] ( 16.0%) α = -0.214 | L(α)=6.9893, L(2α)=12.6531, |ΔL|=4.0502, |ΔL(2α)|=9.7141 | 8.2s | ETA: 7.6m
[  9/50] ( 18.0%) α = -0.173 | L(α)=5.7777, L(2α)=10.6593, |ΔL|=2.8387, |ΔL(2α)|=7.7202 | 8.0s | ETA: 7.2m
[ 10/50] ( 20.0%) α = -0.133 | L(α)=4.5832, L(2α)=8.7025, |ΔL|=1.6442, |ΔL(2α)|=5.7635 | 8.0s | ETA: 6.9m
[ 11/50] ( 22.0%) α = -0.092 | L(α)=3.8204, L(2α)=6.0685, |ΔL|=0.8814, |ΔL(2α)|=3.1295 | 8.1s | ETA: 6.6m
[ 12/50] ( 24.0%) α = -0.051 | L(α)=3.0576, L(2α)=4.0348, |ΔL|=0.1186, |ΔL(2α)|=1.0958 | 7.9s | ETA: 6.3m
[ 13/50] ( 26.0%) α = -0.010 | L(α)=3.3304, L(2α)=4.8245, |ΔL|=0.3914, |ΔL(2α)|=1.8854 | 8.0s | ETA: 6.0m
[ 14/50] ( 28.0%) α = +0.031 | L(α)=2.4357, L(2α)=2.3186, |ΔL|=0.5033, |ΔL(2α)|=0.6204 | 7.9s | ETA: 5.8m
[ 15/50] ( 30.0%) α = +0.071 | L(α)=2.2951, L(2α)=2.1504, |ΔL|=0.6439, |ΔL(2α)|=0.7887 | 8.0s | ETA: 5.6m
[ 16/50] ( 32.0%) α = +0.112 | L(α)=2.2055, L(2α)=2.2204, |ΔL|=0.7335, |ΔL(2α)|=0.7187 | 8.1s | ETA: 5.4m
[ 17/50] ( 34.0%) α = +0.153 | L(α)=2.1460, L(2α)=2.3984, |ΔL|=0.7930, |ΔL(2α)|=0.5407 | 8.2s | ETA: 5.2m
[ 18/50] ( 36.0%) α = +0.194 | L(α)=2.1757, L(2α)=2.6552, |ΔL|=0.7633, |ΔL(2α)|=0.2839 | 9.7s | ETA: 5.0m
[ 19/50] ( 38.0%) α = +0.235 | L(α)=2.2365, L(2α)=3.0074, |ΔL|=0.7025, |ΔL(2α)|=0.0684 | 8.1s | ETA: 4.9m
[ 20/50] ( 40.0%) α = +0.276 | L(α)=2.3222, L(2α)=3.4568, |ΔL|=0.6168, |ΔL(2α)|=0.5178 | 8.0s | ETA: 4.7m
[ 21/50] ( 42.0%) α = +0.316 | L(α)=2.4271, L(2α)=3.9720, |ΔL|=0.5119, |ΔL(2α)|=1.0330 | 8.0s | ETA: 4.5m
[ 22/50] ( 44.0%) α = +0.357 | L(α)=2.5476, L(2α)=4.5292, |ΔL|=0.3914, |ΔL(2α)|=1.5901 | 8.1s | ETA: 4.3m
[ 23/50] ( 46.0%) α = +0.398 | L(α)=2.6993, L(2α)=5.1611, |ΔL|=0.2398, |ΔL(2α)|=2.2221 | 8.4s | ETA: 4.2m
[ 24/50] ( 48.0%) α = +0.439 | L(α)=2.8673, L(2α)=5.8414, |ΔL|=0.0718, |ΔL(2α)|=2.9024 | 8.3s | ETA: 4.0m
[ 25/50] ( 50.0%) α = +0.480 | L(α)=3.0536, L(2α)=6.5629, |ΔL|=0.1146, |ΔL(2α)|=3.6239 | 8.3s | ETA: 3.9m
[ 26/50] ( 52.0%) α = +0.520 | L(α)=3.2855, L(2α)=7.2494, |ΔL|=0.3465, |ΔL(2α)|=4.3103 | 8.1s | ETA: 3.7m
[ 27/50] ( 54.0%) α = +0.561 | L(α)=3.5168, L(2α)=8.0435, |ΔL|=0.5777, |ΔL(2α)|=5.1044 | 7.9s | ETA: 3.5m
[ 28/50] ( 56.0%) α = +0.602 | L(α)=3.7591, L(2α)=8.8379, |ΔL|=0.8200, |ΔL(2α)|=5.8989 | 8.0s | ETA: 3.4m
[ 29/50] ( 58.0%) α = +0.643 | L(α)=4.0338, L(2α)=9.6638, |ΔL|=1.0948, |ΔL(2α)|=6.7248 | 7.9s | ETA: 3.2m
[ 30/50] ( 60.0%) α = +0.684 | L(α)=4.3043, L(2α)=10.4859, |ΔL|=1.3653, |ΔL(2α)|=7.5469 | 8.0s | ETA: 3.1m
[ 31/50] ( 62.0%) α = +0.724 | L(α)=4.6055, L(2α)=11.3288, |ΔL|=1.6664, |ΔL(2α)|=8.3898 | 7.9s | ETA: 2.9m
[ 32/50] ( 64.0%) α = +0.765 | L(α)=4.9237, L(2α)=12.2230, |ΔL|=1.9847, |ΔL(2α)|=9.2840 | 7.9s | ETA: 2.8m
[ 33/50] ( 66.0%) α = +0.806 | L(α)=5.2356, L(2α)=13.1112, |ΔL|=2.2966, |ΔL(2α)|=10.1721 | 7.9s | ETA: 2.6m
[ 34/50] ( 68.0%) α = +0.847 | L(α)=5.5799, L(2α)=14.0666, |ΔL|=2.6408, |ΔL(2α)|=11.1276 | 7.9s | ETA: 2.4m
[ 35/50] ( 70.0%) α = +0.888 | L(α)=5.9205, L(2α)=15.0423, |ΔL|=2.9814, |ΔL(2α)|=12.1032 | 7.9s | ETA: 2.3m
[ 36/50] ( 72.0%) α = +0.929 | L(α)=6.2866, L(2α)=16.0630, |ΔL|=3.3475, |ΔL(2α)|=13.1239 | 7.9s | ETA: 2.2m
[ 37/50] ( 74.0%) α = +0.969 | L(α)=6.6594, L(2α)=17.1222, |ΔL|=3.7204, |ΔL(2α)|=14.1832 | 7.9s | ETA: 2.0m
[ 38/50] ( 76.0%) α = +1.010 | L(α)=6.9582, L(2α)=18.1274, |ΔL|=4.0192, |ΔL(2α)|=15.1884 | 8.0s | ETA: 1.9m
[ 39/50] ( 78.0%) α = +1.051 | L(α)=7.3433, L(2α)=19.2282, |ΔL|=4.4042, |ΔL(2α)|=16.2892 | 8.0s | ETA: 1.7m
[ 40/50] ( 80.0%) α = +1.092 | L(α)=7.7351, L(2α)=20.3369, |ΔL|=4.7960, |ΔL(2α)|=17.3979 | 7.9s | ETA: 1.6m
[ 41/50] ( 82.0%) α = +1.133 | L(α)=8.1411, L(2α)=21.4257, |ΔL|=5.2021, |ΔL(2α)|=18.4866 | 7.9s | ETA: 1.4m
[ 42/50] ( 84.0%) α = +1.173 | L(α)=8.5534, L(2α)=22.4715, |ΔL|=5.6144, |ΔL(2α)|=19.5325 | 8.3s | ETA: 1.3m
[ 43/50] ( 86.0%) α = +1.214 | L(α)=8.9440, L(2α)=23.5138, |ΔL|=6.0050, |ΔL(2α)|=20.5747 | 7.9s | ETA: 1.1m
[ 44/50] ( 88.0%) α = +1.255 | L(α)=9.3754, L(2α)=24.5375, |ΔL|=6.4364, |ΔL(2α)|=21.5985 | 8.3s | ETA: 59s
[ 45/50] ( 90.0%) α = +1.296 | L(α)=9.7615, L(2α)=25.4711, |ΔL|=6.8225, |ΔL(2α)|=22.5321 | 9.0s | ETA: 51s
[ 46/50] ( 92.0%) α = +1.337 | L(α)=10.1751, L(2α)=26.4028, |ΔL|=7.2361, |ΔL(2α)|=23.4637 | 8.9s | ETA: 42s
[ 47/50] ( 94.0%) α = +1.378 | L(α)=10.5890, L(2α)=27.3227, |ΔL|=7.6499, |ΔL(2α)|=24.3836 | 8.7s | ETA: 34s
[ 48/50] ( 96.0%) α = +1.418 | L(α)=11.0182, L(2α)=28.2563, |ΔL|=8.0791, |ΔL(2α)|=25.3173 | 9.2s | ETA: 26s
[ 49/50] ( 98.0%) α = +1.459 | L(α)=11.4390, L(2α)=29.1211, |ΔL|=8.4999, |ΔL(2α)|=26.1821 | 8.5s | ETA: 17s
[ 50/50] (100.0%) α = +1.500 | L(α)=11.8807, L(2α)=29.9754, |ΔL|=8.9416, |ΔL(2α)|=27.0363 | 8.0s | ETA: 9s

Restoring base model parameters...

======================================================================
ALPHA SWEEP COMPLETE
======================================================================
  Duration: 7.2 minutes (432s)
  Samples Completed: 50/50
  Avg time/sample: 8.51s
======================================================================


======================================================================
LOSS LANDSCAPE ANALYSIS
======================================================================

Minimum General Loss (best general knowledge):
  α = +0.1531
  L(α) = 2.1460
  L(M_base) = 2.9390
  Δ = -0.7930

Minimum Task-Specific Loss (best task performance):
  α = +0.1939
  Task L(α) = 3.6327
  General L(α) = 2.1757
  Δ from base = +0.6936

Best Functional Return (smallest |L(α) - L_base|):
  1. α = +0.4388, |ΔL| = 0.071765
  2. α = +0.4796, |ΔL| = 0.114568
  3. α = -0.0510, |ΔL| = 0.118571
  4. α = +0.3980, |ΔL| = 0.239765
  5. α = +0.5204, |ΔL| = 0.346496

Zero-Crossings (where L(α) ≈ L_base for α ≠ 0):
  Found 2 crossing(s):
  1. α = +0.4388, L(α) = 2.8673, |ΔL| = 0.071765 ★
  2. α = +0.4796, L(α) = 3.0536, |ΔL| = 0.114568 ★

======================================================================
SQUARING TEST ANALYSIS: [W(λ)]² = I Analog
======================================================================

Squaring Return Points (where L(2α) ≈ L_base for α ≠ 0):
 ★ Found 1 squaring return point(s)!
  → These α values exhibit the self-inverse property: doubling brings back to base loss
  1. α = +0.2347, L(2α) = 3.0074, |ΔL(2α)| = 0.068385 ★

  INTERPRETATION:
  This suggests neural loss landscapes MAY exhibit rotation-like symmetry!
  Analogous to R(n,π)² = I in rotation groups.
