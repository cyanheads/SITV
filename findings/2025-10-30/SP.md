root@1ea30814126c:/workspace/sitv# python main.py

======================================================================
SITV EXPERIMENT START
======================================================================
Model: google/gemma-3-4b-it
Task: sentiment_positive
General Eval Dataset: combined
Device: cuda
Output: outputs
Analysis Only: False

Fine-Tuning:
  Epochs: 4
  Learning rate: 1.00e-04
  Batch size: 32
  Max length: 1024
  Data repetition: 100x

Alpha Sweep:
  Range: [-3.0, 3.0]
  Samples: 100
  Squaring test: True

2D Composition:
  Alpha range: (-2.0, 2.0)
  Beta range: (-2.0, 2.0)
  Grid size: 30×30
======================================================================


======================================================================
MODEL FINE-TUNING
======================================================================
Loading model: google/gemma-3-4b-it
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.00s/it]
Model loaded successfully

======================================================================
FINE-TUNING MODEL
======================================================================
  Training examples: 3000
  Epochs: 4
  Learning rate: 1.00e-04
  Batch size: 32
  Max length: 1024
  Started: 2025-10-31 11:01:05
The model is already on multiple devices. Skipping the move to device specified in `args`.
  0%|                                                                                                                      | 0/376 [00:00<?, ?it/s]
────────────────────────────────────────────────────────────
Epoch 1/4
────────────────────────────────────────────────────────────
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|██▉                                                                                                          | 10/376 [00:03<01:39,  3.69it/s]  Step 10/376 | Loss: 1.4900 | LR: 9.76e-05 | Grad: 3.11 | ETA: calculating...
{'loss': 1.49, 'grad_norm': 3.109375, 'learning_rate': 9.760638297872341e-05, 'epoch': 0.11}                                                       
  5%|█████▊                                                                                                       | 20/376 [00:05<01:34,  3.78it/s]  Step 20/376 | Loss: 0.3328 | LR: 9.49e-05 | Grad: 2.00 | ETA: 7.9m
{'loss': 0.3328, 'grad_norm': 2.0, 'learning_rate': 9.49468085106383e-05, 'epoch': 0.21}                                                           
  8%|████████▋                                                                                                    | 30/376 [00:08<01:31,  3.77it/s]  Step 30/376 | Loss: 0.2551 | LR: 9.23e-05 | Grad: 1.52 | ETA: 10.5m
{'loss': 0.2551, 'grad_norm': 1.5234375, 'learning_rate': 9.228723404255319e-05, 'epoch': 0.32}                                                    
 11%|███████████▌                                                                                                 | 40/376 [00:11<01:27,  3.83it/s]  Step 40/376 | Loss: 0.2195 | LR: 8.96e-05 | Grad: 1.06 | ETA: 11.3m
{'loss': 0.2195, 'grad_norm': 1.0625, 'learning_rate': 8.96276595744681e-05, 'epoch': 0.43}                                                        
 13%|██████████████▍                                                                                              | 50/376 [00:14<01:25,  3.81it/s]  Step 50/376 | Loss: 0.2261 | LR: 8.70e-05 | Grad: 1.12 | ETA: 11.7m
{'loss': 0.2261, 'grad_norm': 1.1171875, 'learning_rate': 8.696808510638299e-05, 'epoch': 0.53}                                                    
 16%|█████████████████▍                                                                                           | 60/376 [00:16<01:23,  3.77it/s]  Step 60/376 | Loss: 0.2207 | LR: 8.43e-05 | Grad: 1.14 | ETA: 11.8m
{'loss': 0.2207, 'grad_norm': 1.140625, 'learning_rate': 8.430851063829787e-05, 'epoch': 0.64}                                                     
 19%|████████████████████▎                                                                                        | 70/376 [00:19<01:26,  3.53it/s]  Step 70/376 | Loss: 0.2080 | LR: 8.16e-05 | Grad: 1.01 | ETA: 11.8m
{'loss': 0.208, 'grad_norm': 1.0078125, 'learning_rate': 8.164893617021278e-05, 'epoch': 0.74}                                                     
 21%|███████████████████████▏                                                                                     | 80/376 [00:21<00:59,  4.99it/s]  Step 80/376 | Loss: 0.1982 | LR: 7.90e-05 | Grad: 0.95 | ETA: 11.2m
{'loss': 0.1982, 'grad_norm': 0.953125, 'learning_rate': 7.898936170212767e-05, 'epoch': 0.85}                                                     
 24%|██████████████████████████                                                                                   | 90/376 [00:23<00:56,  5.05it/s]  Step 90/376 | Loss: 0.2072 | LR: 7.63e-05 | Grad: 1.30 | ETA: 10.7m
{'loss': 0.2072, 'grad_norm': 1.296875, 'learning_rate': 7.632978723404256e-05, 'epoch': 0.96}                                                     
 25%|███████████████████████████▎                                                                                 | 94/376 [00:24<00:59,  4.77it/s]
────────────────────────────────────────────────────────────
Epoch 2/4
────────────────────────────────────────────────────────────
 27%|████████████████████████████▋                                                                               | 100/376 [00:25<00:54,  5.04it/s]  Step 100/376 | Loss: 0.2091 | LR: 7.37e-05 | Grad: 1.03 | ETA: 10.2m
{'loss': 0.2091, 'grad_norm': 1.03125, 'learning_rate': 7.367021276595744e-05, 'epoch': 1.06}                                                      
 29%|███████████████████████████████▌                                                                            | 110/376 [00:27<00:52,  5.09it/s]  Step 110/376 | Loss: 0.2031 | LR: 7.10e-05 | Grad: 1.55 | ETA: 9.8m
{'loss': 0.2031, 'grad_norm': 1.546875, 'learning_rate': 7.101063829787235e-05, 'epoch': 1.17}                                                     
 32%|██████████████████████████████████▍                                                                         | 120/376 [00:29<00:50,  5.08it/s]  Step 120/376 | Loss: 0.2010 | LR: 6.84e-05 | Grad: 1.15 | ETA: 9.3m
{'loss': 0.201, 'grad_norm': 1.1484375, 'learning_rate': 6.835106382978724e-05, 'epoch': 1.28}                                                     
 35%|█████████████████████████████████████▎                                                                      | 130/376 [00:31<00:48,  5.10it/s]  Step 130/376 | Loss: 0.2017 | LR: 6.57e-05 | Grad: 0.94 | ETA: 8.9m
{'loss': 0.2017, 'grad_norm': 0.9375, 'learning_rate': 6.569148936170213e-05, 'epoch': 1.38}                                                       
 37%|████████████████████████████████████████▏                                                                   | 140/376 [00:33<00:46,  5.11it/s]  Step 140/376 | Loss: 0.1977 | LR: 6.30e-05 | Grad: 1.09 | ETA: 8.5m
{'loss': 0.1977, 'grad_norm': 1.0859375, 'learning_rate': 6.303191489361703e-05, 'epoch': 1.49}                                                    
 40%|███████████████████████████████████████████                                                                 | 150/376 [00:35<00:44,  5.08it/s]  Step 150/376 | Loss: 0.1963 | LR: 6.04e-05 | Grad: 0.91 | ETA: 8.1m
{'loss': 0.1963, 'grad_norm': 0.9140625, 'learning_rate': 6.037234042553191e-05, 'epoch': 1.6}                                                     
 43%|█████████████████████████████████████████████▉                                                              | 160/376 [00:37<00:42,  5.08it/s]  Step 160/376 | Loss: 0.1955 | LR: 5.77e-05 | Grad: 1.02 | ETA: 7.7m
{'loss': 0.1955, 'grad_norm': 1.0234375, 'learning_rate': 5.7712765957446814e-05, 'epoch': 1.7}                                                    
 45%|████████████████████████████████████████████████▊                                                           | 170/376 [00:39<00:40,  5.08it/s]  Step 170/376 | Loss: 0.1893 | LR: 5.51e-05 | Grad: 1.12 | ETA: 7.3m
{'loss': 0.1893, 'grad_norm': 1.1171875, 'learning_rate': 5.5053191489361697e-05, 'epoch': 1.81}                                                   
 48%|███████████████████████████████████████████████████▋                                                        | 180/376 [00:41<00:38,  5.06it/s]  Step 180/376 | Loss: 0.1916 | LR: 5.24e-05 | Grad: 0.84 | ETA: 6.9m
{'loss': 0.1916, 'grad_norm': 0.8359375, 'learning_rate': 5.23936170212766e-05, 'epoch': 1.91}                                                     
 50%|██████████████████████████████████████████████████████                                                      | 188/376 [00:42<00:36,  5.09it/s]
────────────────────────────────────────────────────────────
Epoch 3/4
────────────────────────────────────────────────────────────
 51%|██████████████████████████████████████████████████████▌                                                     | 190/376 [00:43<00:36,  5.08it/s]  Step 190/376 | Loss: 0.1907 | LR: 4.97e-05 | Grad: 1.13 | ETA: 6.5m
{'loss': 0.1907, 'grad_norm': 1.1328125, 'learning_rate': 4.973404255319149e-05, 'epoch': 2.02}                                                    
 53%|█████████████████████████████████████████████████████████▍                                                  | 200/376 [00:45<00:34,  5.03it/s]  Step 200/376 | Loss: 0.1933 | LR: 4.71e-05 | Grad: 0.79 | ETA: 6.2m
{'loss': 0.1933, 'grad_norm': 0.78515625, 'learning_rate': 4.7074468085106385e-05, 'epoch': 2.13}                                                  
 56%|████████████████████████████████████████████████████████████▎                                               | 210/376 [00:47<00:33,  4.93it/s]  Step 210/376 | Loss: 0.1919 | LR: 4.44e-05 | Grad: 0.84 | ETA: 5.8m
{'loss': 0.1919, 'grad_norm': 0.8359375, 'learning_rate': 4.441489361702128e-05, 'epoch': 2.23}                                                    
 59%|███████████████████████████████████████████████████████████████▏                                            | 220/376 [00:49<00:30,  5.07it/s]  Step 220/376 | Loss: 0.1910 | LR: 4.18e-05 | Grad: 0.81 | ETA: 5.4m
{'loss': 0.191, 'grad_norm': 0.8125, 'learning_rate': 4.175531914893617e-05, 'epoch': 2.34}                                                        
 61%|██████████████████████████████████████████████████████████████████                                          | 230/376 [00:51<00:28,  5.07it/s]  Step 230/376 | Loss: 0.1882 | LR: 3.91e-05 | Grad: 0.79 | ETA: 5.1m
{'loss': 0.1882, 'grad_norm': 0.78515625, 'learning_rate': 3.9095744680851066e-05, 'epoch': 2.45}                                                  
 64%|████████████████████████████████████████████████████████████████████▉                                       | 240/376 [00:53<00:27,  5.01it/s]  Step 240/376 | Loss: 0.1901 | LR: 3.64e-05 | Grad: 0.95 | ETA: 4.7m
{'loss': 0.1901, 'grad_norm': 0.953125, 'learning_rate': 3.6436170212765955e-05, 'epoch': 2.55}                                                    
 66%|███████████████████████████████████████████████████████████████████████▊                                    | 250/376 [00:55<00:24,  5.07it/s]  Step 250/376 | Loss: 0.1914 | LR: 3.38e-05 | Grad: 0.91 | ETA: 4.4m
{'loss': 0.1914, 'grad_norm': 0.9140625, 'learning_rate': 3.377659574468085e-05, 'epoch': 2.66}                                                    
 69%|██████████████████████████████████████████████████████████████████████████▋                                 | 260/376 [00:57<00:22,  5.09it/s]  Step 260/376 | Loss: 0.1871 | LR: 3.11e-05 | Grad: 0.95 | ETA: 4.0m
{'loss': 0.1871, 'grad_norm': 0.953125, 'learning_rate': 3.111702127659575e-05, 'epoch': 2.77}                                                     
 72%|█████████████████████████████████████████████████████████████████████████████▌                              | 270/376 [00:59<00:20,  5.08it/s]  Step 270/376 | Loss: 0.1884 | LR: 2.85e-05 | Grad: 0.93 | ETA: 3.7m
{'loss': 0.1884, 'grad_norm': 0.93359375, 'learning_rate': 2.845744680851064e-05, 'epoch': 2.87}                                                   
 74%|████████████████████████████████████████████████████████████████████████████████▍                           | 280/376 [01:01<00:19,  4.92it/s]  Step 280/376 | Loss: 0.1875 | LR: 2.58e-05 | Grad: 0.90 | ETA: 3.3m
{'loss': 0.1875, 'grad_norm': 0.8984375, 'learning_rate': 2.5797872340425532e-05, 'epoch': 2.98}                                                   
 75%|█████████████████████████████████████████████████████████████████████████████████                           | 282/376 [01:01<00:18,  5.00it/s]
────────────────────────────────────────────────────────────
Epoch 4/4
────────────────────────────────────────────────────────────
 77%|███████████████████████████████████████████████████████████████████████████████████▎                        | 290/376 [01:03<00:17,  5.05it/s]  Step 290/376 | Loss: 0.1871 | LR: 2.31e-05 | Grad: 0.66 | ETA: 3.0m
{'loss': 0.1871, 'grad_norm': 0.65625, 'learning_rate': 2.3138297872340425e-05, 'epoch': 3.09}                                                     
 80%|██████████████████████████████████████████████████████████████████████████████████████▏                     | 300/376 [01:05<00:15,  4.91it/s]  Step 300/376 | Loss: 0.1885 | LR: 2.05e-05 | Grad: 0.98 | ETA: 2.6m
{'loss': 0.1885, 'grad_norm': 0.984375, 'learning_rate': 2.047872340425532e-05, 'epoch': 3.19}                                                     
 82%|█████████████████████████████████████████████████████████████████████████████████████████                   | 310/376 [01:07<00:13,  5.06it/s]  Step 310/376 | Loss: 0.1851 | LR: 1.78e-05 | Grad: 0.68 | ETA: 2.3m
{'loss': 0.1851, 'grad_norm': 0.67578125, 'learning_rate': 1.7819148936170214e-05, 'epoch': 3.3}                                                   
 85%|███████████████████████████████████████████████████████████████████████████████████████████▉                | 320/376 [01:09<00:11,  5.01it/s]  Step 320/376 | Loss: 0.1871 | LR: 1.52e-05 | Grad: 0.97 | ETA: 1.9m
{'loss': 0.1871, 'grad_norm': 0.97265625, 'learning_rate': 1.5159574468085108e-05, 'epoch': 3.4}                                                   
 88%|██████████████████████████████████████████████████████████████████████████████████████████████▊             | 330/376 [01:11<00:09,  5.06it/s]  Step 330/376 | Loss: 0.1843 | LR: 1.25e-05 | Grad: 0.74 | ETA: 1.6m
{'loss': 0.1843, 'grad_norm': 0.73828125, 'learning_rate': 1.25e-05, 'epoch': 3.51}                                                                
 90%|█████████████████████████████████████████████████████████████████████████████████████████████████▋          | 340/376 [01:13<00:07,  5.00it/s]  Step 340/376 | Loss: 0.1866 | LR: 9.84e-06 | Grad: 0.59 | ETA: 1.2m
{'loss': 0.1866, 'grad_norm': 0.58984375, 'learning_rate': 9.840425531914895e-06, 'epoch': 3.62}                                                   
 93%|████████████████████████████████████████████████████████████████████████████████████████████████████▌       | 350/376 [01:15<00:05,  5.09it/s]  Step 350/376 | Loss: 0.1826 | LR: 7.18e-06 | Grad: 0.64 | ETA: 53s
{'loss': 0.1826, 'grad_norm': 0.64453125, 'learning_rate': 7.180851063829788e-06, 'epoch': 3.72}                                                   
 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████▍    | 360/376 [01:17<00:03,  5.05it/s]  Step 360/376 | Loss: 0.1846 | LR: 4.52e-06 | Grad: 1.17 | ETA: 33s
{'loss': 0.1846, 'grad_norm': 1.171875, 'learning_rate': 4.521276595744681e-06, 'epoch': 3.83}                                                     
 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 370/376 [01:19<00:01,  5.05it/s]  Step 370/376 | Loss: 0.1845 | LR: 1.86e-06 | Grad: 0.82 | ETA: 12s
{'loss': 0.1845, 'grad_norm': 0.82421875, 'learning_rate': 1.8617021276595745e-06, 'epoch': 3.94}                                                  
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 376/376 [01:20<00:00,  5.08it/s]  Step 376/376 | ETA: 0s
{'train_runtime': 80.3625, 'train_samples_per_second': 149.323, 'train_steps_per_second': 4.679, 'train_loss': 0.23468474314567891, 'epoch': 4.0}  
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 376/376 [01:20<00:00,  4.68it/s]

======================================================================
FINE-TUNING COMPLETE
======================================================================
  Duration: 1.3 minutes (81s)
  Final loss: 0.2347
  Total steps: 376
  Avg time/step: 0.21s
======================================================================


Reloading base model to preserve original weights...
Loading model: google/gemma-3-4b-it
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.05s/it]
Model loaded successfully

Saving models for future analysis...

Saving models for future analysis...
  Base model → outputs/saved_base_model
  Fine-tuned model → outputs/saved_finetuned_model
Models saved successfully!

Models saved to outputs/

======================================================================
COMPUTING TASK VECTOR
======================================================================
Task vector computed: ||T|| = 35.01
Computation time: 25.50s


======================================================================
LOSS LANDSCAPE SWEEP: L(M_base + αT)
======================================================================
Range: α ∈ [-3.0, 3.0]
Samples: 100
Started: 2025-10-31 11:04:11

Question: Does the loss curve cross L(M_base) at any α ≠ 0?

Cloning base model parameters...
Pre-loading task vector to device...
Computing base model loss...
Base model loss L(M_base): 2.9390

Sampling strategy: uniform
Generated 100 alpha values

[  1/100] (  1.0%) α = -3.000 | L(α)=53.8733, L(2α)=138.9556, |ΔL|=50.9342, |ΔL(2α)|=136.0166 | 14.3s | ETA: calculating...
[  2/100] (  2.0%) α = -2.939 | L(α)=52.9970, L(2α)=133.5540, |ΔL|=50.0580, |ΔL(2α)|=130.6150 | 11.9s | ETA: 23.5m
[  3/100] (  3.0%) α = -2.879 | L(α)=51.6868, L(2α)=127.9520, |ΔL|=48.7478, |ΔL(2α)|=125.0130 | 8.2s | ETA: 21.4m
[  4/100] (  4.0%) α = -2.818 | L(α)=50.3350, L(2α)=122.5949, |ΔL|=47.3960, |ΔL(2α)|=119.6559 | 8.2s | ETA: 18.5m
[  5/100] (  5.0%) α = -2.758 | L(α)=49.3449, L(2α)=116.9857, |ΔL|=46.4059, |ΔL(2α)|=114.0466 | 8.1s | ETA: 17.0m
[  6/100] (  6.0%) α = -2.697 | L(α)=48.5437, L(2α)=111.3596, |ΔL|=45.6047, |ΔL(2α)|=108.4206 | 8.2s | ETA: 16.1m
[  7/100] (  7.0%) α = -2.636 | L(α)=48.1943, L(2α)=105.7307, |ΔL|=45.2553, |ΔL(2α)|=102.7917 | 8.1s | ETA: 15.4m
[  8/100] (  8.0%) α = -2.576 | L(α)=47.5157, L(2α)=99.8704, |ΔL|=44.5767, |ΔL(2α)|=96.9314 | 8.2s | ETA: 14.8m
[  9/100] (  9.0%) α = -2.515 | L(α)=47.0932, L(2α)=94.1009, |ΔL|=44.1542, |ΔL(2α)|=91.1618 | 8.2s | ETA: 14.4m
[ 10/100] ( 10.0%) α = -2.455 | L(α)=48.1979, L(2α)=88.7524, |ΔL|=45.2588, |ΔL(2α)|=85.8133 | 8.1s | ETA: 14.1m
[ 11/100] ( 11.0%) α = -2.394 | L(α)=51.4038, L(2α)=82.7793, |ΔL|=48.4648, |ΔL(2α)|=79.8402 | 8.2s | ETA: 13.7m
[ 12/100] ( 12.0%) α = -2.333 | L(α)=50.2677, L(2α)=77.2133, |ΔL|=47.3287, |ΔL(2α)|=74.2742 | 8.2s | ETA: 13.4m
[ 13/100] ( 13.0%) α = -2.273 | L(α)=49.4883, L(2α)=70.7705, |ΔL|=46.5492, |ΔL(2α)|=67.8314 | 8.2s | ETA: 13.2m
[ 14/100] ( 14.0%) α = -2.212 | L(α)=48.4264, L(2α)=62.7151, |ΔL|=45.4874, |ΔL(2α)|=59.7761 | 8.2s | ETA: 12.9m
[ 15/100] ( 15.0%) α = -2.152 | L(α)=47.9124, L(2α)=56.7940, |ΔL|=44.9734, |ΔL(2α)|=53.8550 | 8.1s | ETA: 12.7m
[ 16/100] ( 16.0%) α = -2.091 | L(α)=47.3091, L(2α)=52.6569, |ΔL|=44.3701, |ΔL(2α)|=49.7179 | 8.2s | ETA: 12.5m
[ 17/100] ( 17.0%) α = -2.030 | L(α)=47.2102, L(2α)=50.8348, |ΔL|=44.2711, |ΔL(2α)|=47.8958 | 8.2s | ETA: 12.3m
[ 18/100] ( 18.0%) α = -1.970 | L(α)=47.1383, L(2α)=50.7429, |ΔL|=44.1992, |ΔL(2α)|=47.8039 | 8.2s | ETA: 12.1m
[ 19/100] ( 19.0%) α = -1.909 | L(α)=46.9196, L(2α)=52.8488, |ΔL|=43.9805, |ΔL(2α)|=49.9097 | 8.2s | ETA: 11.9m
[ 20/100] ( 20.0%) α = -1.848 | L(α)=46.6834, L(2α)=55.5403, |ΔL|=43.7443, |ΔL(2α)|=52.6013 | 8.2s | ETA: 11.7m
[ 21/100] ( 21.0%) α = -1.788 | L(α)=46.3743, L(2α)=56.8922, |ΔL|=43.4352, |ΔL(2α)|=53.9531 | 8.3s | ETA: 11.6m
[ 22/100] ( 22.0%) α = -1.727 | L(α)=45.9732, L(2α)=57.6850, |ΔL|=43.0342, |ΔL(2α)|=54.7460 | 8.2s | ETA: 11.4m
[ 23/100] ( 23.0%) α = -1.667 | L(α)=45.6777, L(2α)=58.4319, |ΔL|=42.7387, |ΔL(2α)|=55.4929 | 8.2s | ETA: 11.2m
[ 24/100] ( 24.0%) α = -1.606 | L(α)=45.1469, L(2α)=57.7026, |ΔL|=42.2078, |ΔL(2α)|=54.7635 | 8.2s | ETA: 11.1m
[ 25/100] ( 25.0%) α = -1.545 | L(α)=44.1935, L(2α)=55.5102, |ΔL|=41.2544, |ΔL(2α)|=52.5712 | 8.2s | ETA: 10.9m
[ 26/100] ( 26.0%) α = -1.485 | L(α)=43.1429, L(2α)=53.6407, |ΔL|=40.2038, |ΔL(2α)|=50.7017 | 8.2s | ETA: 10.7m
[ 27/100] ( 27.0%) α = -1.424 | L(α)=41.7707, L(2α)=51.0142, |ΔL|=38.8317, |ΔL(2α)|=48.0752 | 8.3s | ETA: 10.6m
[ 28/100] ( 28.0%) α = -1.364 | L(α)=40.6803, L(2α)=48.9312, |ΔL|=37.7412, |ΔL(2α)|=45.9922 | 8.2s | ETA: 10.4m
[ 29/100] ( 29.0%) α = -1.303 | L(α)=38.8118, L(2α)=47.7944, |ΔL|=35.8728, |ΔL(2α)|=44.8554 | 8.2s | ETA: 10.3m
[ 30/100] ( 30.0%) α = -1.242 | L(α)=36.8860, L(2α)=47.5713, |ΔL|=33.9470, |ΔL(2α)|=44.6323 | 8.2s | ETA: 10.1m
[ 31/100] ( 31.0%) α = -1.182 | L(α)=35.0166, L(2α)=50.6382, |ΔL|=32.0776, |ΔL(2α)|=47.6992 | 8.2s | ETA: 9.9m
[ 32/100] ( 32.0%) α = -1.121 | L(α)=33.1455, L(2α)=48.9060, |ΔL|=30.2064, |ΔL(2α)|=45.9669 | 8.3s | ETA: 9.8m
[ 33/100] ( 33.0%) α = -1.061 | L(α)=31.0707, L(2α)=47.7470, |ΔL|=28.1317, |ΔL(2α)|=44.8079 | 8.3s | ETA: 9.6m
[ 34/100] ( 34.0%) α = -1.000 | L(α)=28.5276, L(2α)=47.1439, |ΔL|=25.5885, |ΔL(2α)|=44.2048 | 8.3s | ETA: 9.5m
[ 35/100] ( 35.0%) α = -0.939 | L(α)=25.9200, L(2α)=46.8319, |ΔL|=22.9809, |ΔL(2α)|=43.8929 | 8.2s | ETA: 9.3m
[ 36/100] ( 36.0%) α = -0.879 | L(α)=23.5668, L(2α)=46.1822, |ΔL|=20.6277, |ΔL(2α)|=43.2431 | 8.3s | ETA: 9.2m
[ 37/100] ( 37.0%) α = -0.818 | L(α)=21.3642, L(2α)=45.4954, |ΔL|=18.4252, |ΔL(2α)|=42.5563 | 8.2s | ETA: 9.0m
[ 38/100] ( 38.0%) α = -0.758 | L(α)=20.2782, L(2α)=43.9242, |ΔL|=17.3391, |ΔL(2α)|=40.9851 | 8.2s | ETA: 8.9m
[ 39/100] ( 39.0%) α = -0.697 | L(α)=19.2078, L(2α)=41.1616, |ΔL|=16.2688, |ΔL(2α)|=38.2225 | 8.2s | ETA: 8.7m
[ 40/100] ( 40.0%) α = -0.636 | L(α)=17.4456, L(2α)=38.0112, |ΔL|=14.5066, |ΔL(2α)|=35.0722 | 8.2s | ETA: 8.6m
[ 41/100] ( 41.0%) α = -0.576 | L(α)=16.0276, L(2α)=34.0816, |ΔL|=13.0886, |ΔL(2α)|=31.1425 | 8.3s | ETA: 8.5m
[ 42/100] ( 42.0%) α = -0.515 | L(α)=14.7923, L(2α)=29.7669, |ΔL|=11.8533, |ΔL(2α)|=26.8279 | 8.4s | ETA: 8.3m
[ 43/100] ( 43.0%) α = -0.455 | L(α)=13.2126, L(2α)=24.7737, |ΔL|=10.2736, |ΔL(2α)|=21.8346 | 8.5s | ETA: 8.2m
[ 44/100] ( 44.0%) α = -0.394 | L(α)=11.9744, L(2α)=20.8407, |ΔL|=9.0354, |ΔL(2α)|=17.9017 | 8.2s | ETA: 8.0m
[ 45/100] ( 45.0%) α = -0.333 | L(α)=10.5780, L(2α)=18.4512, |ΔL|=7.6390, |ΔL(2α)|=15.5122 | 8.1s | ETA: 7.9m
[ 46/100] ( 46.0%) α = -0.273 | L(α)=9.5703, L(2α)=15.3622, |ΔL|=6.6312, |ΔL(2α)|=12.4232 | 8.1s | ETA: 7.7m
[ 47/100] ( 47.0%) α = -0.212 | L(α)=8.2439, L(2α)=12.6014, |ΔL|=5.3049, |ΔL(2α)|=9.6624 | 8.2s | ETA: 7.6m
[ 48/100] ( 48.0%) α = -0.152 | L(α)=6.8247, L(2α)=10.1100, |ΔL|=3.8856, |ΔL(2α)|=7.1710 | 8.4s | ETA: 7.4m
[ 49/100] ( 49.0%) α = -0.091 | L(α)=3.9117, L(2α)=7.7757, |ΔL|=0.9727, |ΔL(2α)|=4.8366 | 8.2s | ETA: 7.3m
[ 50/100] ( 50.0%) α = -0.030 | L(α)=3.9426, L(2α)=3.3199, |ΔL|=1.0035, |ΔL(2α)|=0.3809 | 8.2s | ETA: 7.2m
[ 51/100] ( 51.0%) α = +0.030 | L(α)=2.4165, L(2α)=2.2972, |ΔL|=0.5226, |ΔL(2α)|=0.6418 | 8.2s | ETA: 7.0m
[ 52/100] ( 52.0%) α = +0.091 | L(α)=2.1829, L(2α)=2.2292, |ΔL|=0.7562, |ΔL(2α)|=0.7098 | 8.2s | ETA: 6.9m
[ 53/100] ( 53.0%) α = +0.152 | L(α)=2.1844, L(2α)=2.5199, |ΔL|=0.7546, |ΔL(2α)|=0.4191 | 8.1s | ETA: 6.7m
[ 54/100] ( 54.0%) α = +0.212 | L(α)=2.2840, L(2α)=2.9104, |ΔL|=0.6551, |ΔL(2α)|=0.0287 | 8.3s | ETA: 6.6m
[ 55/100] ( 55.0%) α = +0.273 | L(α)=2.4386, L(2α)=3.4417, |ΔL|=0.5004, |ΔL(2α)|=0.5026 | 8.2s | ETA: 6.4m
[ 56/100] ( 56.0%) α = +0.333 | L(α)=2.6021, L(2α)=4.0741, |ΔL|=0.3369, |ΔL(2α)|=1.1350 | 8.1s | ETA: 6.3m
[ 57/100] ( 57.0%) α = +0.394 | L(α)=2.8039, L(2α)=4.8344, |ΔL|=0.1352, |ΔL(2α)|=1.8954 | 8.2s | ETA: 6.1m
[ 58/100] ( 58.0%) α = +0.455 | L(α)=3.0266, L(2α)=5.6908, |ΔL|=0.0875, |ΔL(2α)|=2.7517 | 8.1s | ETA: 6.0m
[ 59/100] ( 59.0%) α = +0.515 | L(α)=3.3072, L(2α)=6.6180, |ΔL|=0.3682, |ΔL(2α)|=3.6790 | 8.2s | ETA: 5.9m
[ 60/100] ( 60.0%) α = +0.576 | L(α)=3.5886, L(2α)=7.7472, |ΔL|=0.6496, |ΔL(2α)|=4.8081 | 8.2s | ETA: 5.7m
[ 61/100] ( 61.0%) α = +0.636 | L(α)=3.9101, L(2α)=9.0423, |ΔL|=0.9711, |ΔL(2α)|=6.1032 | 8.3s | ETA: 5.6m
[ 62/100] ( 62.0%) α = +0.697 | L(α)=4.2510, L(2α)=10.5210, |ΔL|=1.3119, |ΔL(2α)|=7.5819 | 8.2s | ETA: 5.4m
[ 63/100] ( 63.0%) α = +0.758 | L(α)=4.6444, L(2α)=12.2256, |ΔL|=1.7054, |ΔL(2α)|=9.2865 | 8.2s | ETA: 5.3m
[ 64/100] ( 64.0%) α = +0.818 | L(α)=5.0327, L(2α)=14.1025, |ΔL|=2.0937, |ΔL(2α)|=11.1634 | 8.2s | ETA: 5.2m
[ 65/100] ( 65.0%) α = +0.879 | L(α)=5.4740, L(2α)=16.2293, |ΔL|=2.5349, |ΔL(2α)|=13.2902 | 8.2s | ETA: 5.0m
[ 66/100] ( 66.0%) α = +0.939 | L(α)=5.9212, L(2α)=18.5389, |ΔL|=2.9822, |ΔL(2α)|=15.5998 | 8.2s | ETA: 4.9m
[ 67/100] ( 67.0%) α = +1.000 | L(α)=6.3907, L(2α)=21.1124, |ΔL|=3.4517, |ΔL(2α)|=18.1734 | 8.2s | ETA: 4.7m
[ 68/100] ( 68.0%) α = +1.061 | L(α)=6.8964, L(2α)=23.9341, |ΔL|=3.9574, |ΔL(2α)|=20.9950 | 8.3s | ETA: 4.6m
[ 69/100] ( 69.0%) α = +1.121 | L(α)=7.4542, L(2α)=26.6570, |ΔL|=4.5152, |ΔL(2α)|=23.7180 | 8.2s | ETA: 4.5m
[ 70/100] ( 70.0%) α = +1.182 | L(α)=8.0537, L(2α)=29.4471, |ΔL|=5.1147, |ΔL(2α)|=26.5080 | 8.2s | ETA: 4.3m
[ 71/100] ( 71.0%) α = +1.242 | L(α)=8.6795, L(2α)=32.8398, |ΔL|=5.7405, |ΔL(2α)|=29.9008 | 8.2s | ETA: 4.2m
[ 72/100] ( 72.0%) α = +1.303 | L(α)=9.3847, L(2α)=36.9194, |ΔL|=6.4457, |ΔL(2α)|=33.9803 | 8.2s | ETA: 4.0m
[ 73/100] ( 73.0%) α = +1.364 | L(α)=10.1238, L(2α)=41.4508, |ΔL|=7.1848, |ΔL(2α)|=38.5118 | 8.2s | ETA: 3.9m
[ 74/100] ( 74.0%) α = +1.424 | L(α)=10.9082, L(2α)=46.2960, |ΔL|=7.9691, |ΔL(2α)|=43.3569 | 8.2s | ETA: 3.8m
[ 75/100] ( 75.0%) α = +1.485 | L(α)=11.7042, L(2α)=51.9087, |ΔL|=8.7652, |ΔL(2α)|=48.9697 | 8.3s | ETA: 3.6m
[ 76/100] ( 76.0%) α = +1.545 | L(α)=12.6394, L(2α)=59.5353, |ΔL|=9.7003, |ΔL(2α)|=56.5963 | 8.2s | ETA: 3.5m
[ 77/100] ( 77.0%) α = +1.606 | L(α)=13.5838, L(2α)=64.5242, |ΔL|=10.6447, |ΔL(2α)|=61.5851 | 8.5s | ETA: 3.3m
[ 78/100] ( 78.0%) α = +1.667 | L(α)=14.6049, L(2α)=67.5103, |ΔL|=11.6659, |ΔL(2α)|=64.5713 | 8.2s | ETA: 3.2m
[ 79/100] ( 79.0%) α = +1.727 | L(α)=15.6361, L(2α)=70.1108, |ΔL|=12.6970, |ΔL(2α)|=67.1717 | 8.2s | ETA: 3.1m
[ 80/100] ( 80.0%) α = +1.788 | L(α)=16.7632, L(2α)=72.6673, |ΔL|=13.8242, |ΔL(2α)|=69.7283 | 8.9s | ETA: 2.9m
[ 81/100] ( 81.0%) α = +1.848 | L(α)=17.9473, L(2α)=74.9036, |ΔL|=15.0083, |ΔL(2α)|=71.9646 | 8.9s | ETA: 2.8m
[ 82/100] ( 82.0%) α = +1.909 | L(α)=19.1903, L(2α)=76.9366, |ΔL|=16.2513, |ΔL(2α)|=73.9975 | 8.4s | ETA: 2.6m
[ 83/100] ( 83.0%) α = +1.970 | L(α)=20.5262, L(2α)=79.0698, |ΔL|=17.5872, |ΔL(2α)|=76.1308 | 8.3s | ETA: 2.5m
[ 84/100] ( 84.0%) α = +2.030 | L(α)=21.7209, L(2α)=80.5110, |ΔL|=18.7819, |ΔL(2α)|=77.5720 | 8.3s | ETA: 2.4m
[ 85/100] ( 85.0%) α = +2.091 | L(α)=23.2062, L(2α)=82.3590, |ΔL|=20.2672, |ΔL(2α)|=79.4200 | 8.5s | ETA: 2.2m
[ 86/100] ( 86.0%) α = +2.152 | L(α)=24.6066, L(2α)=83.8805, |ΔL|=21.6676, |ΔL(2α)|=80.9415 | 8.2s | ETA: 2.1m
[ 87/100] ( 87.0%) α = +2.212 | L(α)=25.9722, L(2α)=85.3900, |ΔL|=23.0331, |ΔL(2α)|=82.4510 | 8.2s | ETA: 1.9m
[ 88/100] ( 88.0%) α = +2.273 | L(α)=27.4044, L(2α)=87.0730, |ΔL|=24.4653, |ΔL(2α)|=84.1340 | 8.2s | ETA: 1.8m
[ 89/100] ( 89.0%) α = +2.333 | L(α)=28.7705, L(2α)=88.4784, |ΔL|=25.8314, |ΔL(2α)|=85.5394 | 8.2s | ETA: 1.7m
[ 90/100] ( 90.0%) α = +2.394 | L(α)=30.2782, L(2α)=89.9878, |ΔL|=27.3392, |ΔL(2α)|=87.0488 | 8.2s | ETA: 1.5m
[ 91/100] ( 91.0%) α = +2.455 | L(α)=32.0055, L(2α)=91.4626, |ΔL|=29.0664, |ΔL(2α)|=88.5235 | 8.2s | ETA: 1.4m
[ 92/100] ( 92.0%) α = +2.515 | L(α)=34.0848, L(2α)=92.7342, |ΔL|=31.1458, |ΔL(2α)|=89.7951 | 8.2s | ETA: 1.3m
[ 93/100] ( 93.0%) α = +2.576 | L(α)=35.8672, L(2α)=94.0611, |ΔL|=32.9282, |ΔL(2α)|=91.1220 | 8.2s | ETA: 1.1m
[ 94/100] ( 94.0%) α = +2.636 | L(α)=38.1097, L(2α)=95.3108, |ΔL|=35.1706, |ΔL(2α)|=92.3717 | 8.2s | ETA: 58s
[ 95/100] ( 95.0%) α = +2.697 | L(α)=40.3118, L(2α)=96.3792, |ΔL|=37.3727, |ΔL(2α)|=93.4402 | 8.1s | ETA: 50s
[ 96/100] ( 96.0%) α = +2.758 | L(α)=42.6529, L(2α)=97.4502, |ΔL|=39.7139, |ΔL(2α)|=94.5112 | 8.2s | ETA: 42s
[ 97/100] ( 97.0%) α = +2.818 | L(α)=44.9546, L(2α)=98.4106, |ΔL|=42.0156, |ΔL(2α)|=95.4716 | 9.1s | ETA: 33s
[ 98/100] ( 98.0%) α = +2.879 | L(α)=47.5817, L(2α)=99.4061, |ΔL|=44.6427, |ΔL(2α)|=96.4670 | 8.2s | ETA: 25s
[ 99/100] ( 99.0%) α = +2.939 | L(α)=50.4951, L(2α)=100.3170, |ΔL|=47.5561, |ΔL(2α)|=97.3780 | 8.2s | ETA: 17s
[100/100] (100.0%) α = +3.000 | L(α)=53.3203, L(2α)=101.3263, |ΔL|=50.3813, |ΔL(2α)|=98.3873 | 9.1s | ETA: 8s

Restoring base model parameters...

======================================================================
ALPHA SWEEP COMPLETE
======================================================================
  Duration: 14.0 minutes (840s)
  Samples Completed: 100/100
  Avg time/sample: 8.35s
======================================================================


======================================================================
LOSS LANDSCAPE ANALYSIS
======================================================================

Minimum General Loss (best general knowledge):
  α = +0.0909
  L(α) = 2.1829
  L(M_base) = 2.9390
  Δ = -0.7562

Minimum Task-Specific Loss (best task performance):
  α = +0.1515
  Task L(α) = 3.0222
  General L(α) = 2.1844
  Δ from base = +0.0832

Best Functional Return (smallest |L(α) - L_base|):
  1. α = +0.4545, |ΔL| = 0.087546
  2. α = +0.3939, |ΔL| = 0.135164
  3. α = +0.3333, |ΔL| = 0.336918
  4. α = +0.5152, |ΔL| = 0.368160
  5. α = +0.2727, |ΔL| = 0.500415

Zero-Crossings (where L(α) ≈ L_base for α ≠ 0):
  Found 2 crossing(s):
  1. α = +0.3939, L(α) = 2.8039, |ΔL| = 0.135164 ★
  2. α = +0.4545, L(α) = 3.0266, |ΔL| = 0.087546 ★

======================================================================
SQUARING TEST ANALYSIS: [W(λ)]² = I Analog
======================================================================

Squaring Return Points (where L(2α) ≈ L_base for α ≠ 0):
 ★ Found 1 squaring return point(s)!
  → These α values exhibit the self-inverse property: doubling brings back to base loss
  1. α = +0.2121, L(2α) = 2.9104, |ΔL(2α)| = 0.028673 ★

  INTERPRETATION:
  This suggests neural loss landscapes MAY exhibit rotation-like symmetry!
  Analogous to R(n,π)² = I in rotation groups.