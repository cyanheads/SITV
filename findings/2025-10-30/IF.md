root@1ea30814126c:/workspace/sitv# python main.py

======================================================================
SITV EXPERIMENT START
======================================================================
Model: google/gemma-3-4b-it
Task: instruction_following
General Eval Dataset: combined
Device: cuda
Output: outputs
Analysis Only: False

Fine-Tuning:
  Epochs: 4
  Learning rate: 1.00e-04
  Batch size: 32
  Max length: 1024
  Data repetition: 100x

Alpha Sweep:
  Range: [-0.5, 1.5]
  Samples: 50
  Squaring test: True
======================================================================


======================================================================
MODEL FINE-TUNING
======================================================================
Loading model: google/gemma-3-4b-it
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:40<00:00, 20.34s/it]
Model loaded successfully

======================================================================
FINE-TUNING MODEL
======================================================================
  Training examples: 3000
  Epochs: 4
  Learning rate: 1.00e-04
  Batch size: 32
  Max length: 1024
  Started: 2025-10-31 11:54:33
The model is already on multiple devices. Skipping the move to device specified in `args`.
  0%|                                                                                                                      | 0/376 [00:00<?, ?it/s]
────────────────────────────────────────────────────────────
Epoch 1/4
────────────────────────────────────────────────────────────
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|██▉                                                                                                          | 10/376 [00:03<01:39,  3.67it/s]  Step 10/376 | Loss: 1.7229 | LR: 9.76e-05 | Grad: 12.31 | ETA: calculating...
{'loss': 1.7229, 'grad_norm': 12.3125, 'learning_rate': 9.760638297872341e-05, 'epoch': 0.11}                                                      
  5%|█████▊                                                                                                       | 20/376 [00:06<01:31,  3.91it/s]  Step 20/376 | Loss: 0.3064 | LR: 9.49e-05 | Grad: 3.55 | ETA: 7.4m
{'loss': 0.3064, 'grad_norm': 3.546875, 'learning_rate': 9.49468085106383e-05, 'epoch': 0.21}                                                      
  8%|████████▋                                                                                                    | 30/376 [00:08<01:39,  3.48it/s]  Step 30/376 | Loss: 0.2184 | LR: 9.23e-05 | Grad: 2.03 | ETA: 10.2m
{'loss': 0.2184, 'grad_norm': 2.03125, 'learning_rate': 9.228723404255319e-05, 'epoch': 0.32}                                                      
 11%|███████████▌                                                                                                 | 40/376 [00:11<01:36,  3.50it/s]  Step 40/376 | Loss: 0.1990 | LR: 8.96e-05 | Grad: 1.57 | ETA: 11.5m
{'loss': 0.199, 'grad_norm': 1.5703125, 'learning_rate': 8.96276595744681e-05, 'epoch': 0.43}                                                      
 13%|██████████████▍                                                                                              | 50/376 [00:14<01:27,  3.71it/s]  Step 50/376 | Loss: 0.1843 | LR: 8.70e-05 | Grad: 1.19 | ETA: 12.1m
{'loss': 0.1843, 'grad_norm': 1.1875, 'learning_rate': 8.696808510638299e-05, 'epoch': 0.53}                                                       
 16%|█████████████████▍                                                                                           | 60/376 [00:17<01:21,  3.88it/s]  Step 60/376 | Loss: 0.1765 | LR: 8.43e-05 | Grad: 1.13 | ETA: 12.0m
{'loss': 0.1765, 'grad_norm': 1.1328125, 'learning_rate': 8.430851063829787e-05, 'epoch': 0.64}                                                    
 19%|████████████████████▎                                                                                        | 70/376 [00:19<01:22,  3.69it/s]  Step 70/376 | Loss: 0.1713 | LR: 8.16e-05 | Grad: 0.93 | ETA: 12.0m
{'loss': 0.1713, 'grad_norm': 0.9296875, 'learning_rate': 8.164893617021278e-05, 'epoch': 0.74}                                                    
 21%|███████████████████████▏                                                                                     | 80/376 [00:22<01:17,  3.82it/s]  Step 80/376 | Loss: 0.1715 | LR: 7.90e-05 | Grad: 1.00 | ETA: 11.7m
{'loss': 0.1715, 'grad_norm': 1.0, 'learning_rate': 7.898936170212767e-05, 'epoch': 0.85}                                                          
 24%|██████████████████████████                                                                                   | 90/376 [00:25<01:14,  3.86it/s]  Step 90/376 | Loss: 0.1766 | LR: 7.63e-05 | Grad: 1.15 | ETA: 11.5m
{'loss': 0.1766, 'grad_norm': 1.1484375, 'learning_rate': 7.632978723404256e-05, 'epoch': 0.96}                                                    
 25%|███████████████████████████▎                                                                                 | 94/376 [00:26<01:19,  3.57it/s]
────────────────────────────────────────────────────────────
Epoch 2/4
────────────────────────────────────────────────────────────
 27%|████████████████████████████▋                                                                               | 100/376 [00:27<01:14,  3.72it/s]  Step 100/376 | Loss: 0.1779 | LR: 7.37e-05 | Grad: 1.07 | ETA: 11.2m
{'loss': 0.1779, 'grad_norm': 1.0703125, 'learning_rate': 7.367021276595744e-05, 'epoch': 1.06}                                                    
 29%|███████████████████████████████▌                                                                            | 110/376 [00:30<01:09,  3.83it/s]  Step 110/376 | Loss: 0.1727 | LR: 7.10e-05 | Grad: 1.44 | ETA: 10.9m
{'loss': 0.1727, 'grad_norm': 1.4375, 'learning_rate': 7.101063829787235e-05, 'epoch': 1.17}                                                       
 32%|██████████████████████████████████▍                                                                         | 120/376 [00:33<01:07,  3.79it/s]  Step 120/376 | Loss: 0.1687 | LR: 6.84e-05 | Grad: 1.14 | ETA: 10.5m
{'loss': 0.1687, 'grad_norm': 1.140625, 'learning_rate': 6.835106382978724e-05, 'epoch': 1.28}                                                     
 35%|█████████████████████████████████████▎                                                                      | 130/376 [00:35<01:06,  3.70it/s]  Step 130/376 | Loss: 0.1709 | LR: 6.57e-05 | Grad: 1.17 | ETA: 10.2m
{'loss': 0.1709, 'grad_norm': 1.171875, 'learning_rate': 6.569148936170213e-05, 'epoch': 1.38}                                                     
 37%|████████████████████████████████████████▏                                                                   | 140/376 [00:38<01:01,  3.81it/s]  Step 140/376 | Loss: 0.1690 | LR: 6.30e-05 | Grad: 1.37 | ETA: 9.8m
{'loss': 0.169, 'grad_norm': 1.3671875, 'learning_rate': 6.303191489361703e-05, 'epoch': 1.49}                                                     
 40%|███████████████████████████████████████████                                                                 | 150/376 [00:41<00:59,  3.82it/s]  Step 150/376 | Loss: 0.1658 | LR: 6.04e-05 | Grad: 0.92 | ETA: 9.4m
{'loss': 0.1658, 'grad_norm': 0.91796875, 'learning_rate': 6.037234042553191e-05, 'epoch': 1.6}                                                    
 43%|█████████████████████████████████████████████▉                                                              | 160/376 [00:43<00:57,  3.77it/s]  Step 160/376 | Loss: 0.1674 | LR: 5.77e-05 | Grad: 1.16 | ETA: 9.1m
{'loss': 0.1674, 'grad_norm': 1.15625, 'learning_rate': 5.7712765957446814e-05, 'epoch': 1.7}                                                      
 45%|████████████████████████████████████████████████▊                                                           | 170/376 [00:46<00:53,  3.83it/s]  Step 170/376 | Loss: 0.1639 | LR: 5.51e-05 | Grad: 0.80 | ETA: 8.7m
{'loss': 0.1639, 'grad_norm': 0.80078125, 'learning_rate': 5.5053191489361697e-05, 'epoch': 1.81}                                                  
 48%|███████████████████████████████████████████████████▋                                                        | 180/376 [00:48<00:50,  3.88it/s]  Step 180/376 | Loss: 0.1610 | LR: 5.24e-05 | Grad: 0.78 | ETA: 8.3m
{'loss': 0.161, 'grad_norm': 0.78125, 'learning_rate': 5.23936170212766e-05, 'epoch': 1.91}                                                        
 50%|██████████████████████████████████████████████████████                                                      | 188/376 [00:51<00:48,  3.89it/s]
────────────────────────────────────────────────────────────
Epoch 3/4
────────────────────────────────────────────────────────────
 51%|██████████████████████████████████████████████████████▌                                                     | 190/376 [00:51<00:47,  3.88it/s]  Step 190/376 | Loss: 0.1582 | LR: 4.97e-05 | Grad: 0.95 | ETA: 7.8m
{'loss': 0.1582, 'grad_norm': 0.9453125, 'learning_rate': 4.973404255319149e-05, 'epoch': 2.02}                                                    
 53%|█████████████████████████████████████████████████████████▍                                                  | 200/376 [00:54<00:45,  3.84it/s]  Step 200/376 | Loss: 0.1683 | LR: 4.71e-05 | Grad: 1.80 | ETA: 7.4m
{'loss': 0.1683, 'grad_norm': 1.8046875, 'learning_rate': 4.7074468085106385e-05, 'epoch': 2.13}                                                   
 56%|████████████████████████████████████████████████████████████▎                                               | 210/376 [00:56<00:43,  3.81it/s]  Step 210/376 | Loss: 0.1636 | LR: 4.44e-05 | Grad: 0.77 | ETA: 7.0m
{'loss': 0.1636, 'grad_norm': 0.765625, 'learning_rate': 4.441489361702128e-05, 'epoch': 2.23}                                                     
 59%|███████████████████████████████████████████████████████████████▏                                            | 220/376 [00:59<00:42,  3.70it/s]  Step 220/376 | Loss: 0.1649 | LR: 4.18e-05 | Grad: 1.15 | ETA: 6.6m
{'loss': 0.1649, 'grad_norm': 1.1484375, 'learning_rate': 4.175531914893617e-05, 'epoch': 2.34}                                                    
 61%|██████████████████████████████████████████████████████████████████                                          | 230/376 [01:02<00:37,  3.88it/s]  Step 230/376 | Loss: 0.1598 | LR: 3.91e-05 | Grad: 0.93 | ETA: 6.2m
{'loss': 0.1598, 'grad_norm': 0.93359375, 'learning_rate': 3.9095744680851066e-05, 'epoch': 2.45}                                                  
 64%|████████████████████████████████████████████████████████████████████▉                                       | 240/376 [01:04<00:35,  3.88it/s]  Step 240/376 | Loss: 0.1578 | LR: 3.64e-05 | Grad: 1.09 | ETA: 5.8m
{'loss': 0.1578, 'grad_norm': 1.09375, 'learning_rate': 3.6436170212765955e-05, 'epoch': 2.55}                                                     
 66%|███████████████████████████████████████████████████████████████████████▊                                    | 250/376 [01:07<00:33,  3.79it/s]  Step 250/376 | Loss: 0.1608 | LR: 3.38e-05 | Grad: 1.19 | ETA: 5.4m
{'loss': 0.1608, 'grad_norm': 1.1875, 'learning_rate': 3.377659574468085e-05, 'epoch': 2.66}                                                       
 69%|██████████████████████████████████████████████████████████████████████████▋                                 | 260/376 [01:09<00:31,  3.69it/s]  Step 260/376 | Loss: 0.1538 | LR: 3.11e-05 | Grad: 0.86 | ETA: 4.9m
{'loss': 0.1538, 'grad_norm': 0.85546875, 'learning_rate': 3.111702127659575e-05, 'epoch': 2.77}                                                   
 72%|█████████████████████████████████████████████████████████████████████████████▌                              | 270/376 [01:12<00:28,  3.76it/s]  Step 270/376 | Loss: 0.1565 | LR: 2.85e-05 | Grad: 0.97 | ETA: 4.5m
{'loss': 0.1565, 'grad_norm': 0.96875, 'learning_rate': 2.845744680851064e-05, 'epoch': 2.87}                                                      
 74%|████████████████████████████████████████████████████████████████████████████████▍                           | 280/376 [01:15<00:24,  3.85it/s]  Step 280/376 | Loss: 0.1562 | LR: 2.58e-05 | Grad: 1.00 | ETA: 4.1m
{'loss': 0.1562, 'grad_norm': 1.0, 'learning_rate': 2.5797872340425532e-05, 'epoch': 2.98}                                                         
 75%|█████████████████████████████████████████████████████████████████████████████████                           | 282/376 [01:15<00:24,  3.83it/s]
────────────────────────────────────────────────────────────
Epoch 4/4
────────────────────────────────────────────────────────────
 77%|███████████████████████████████████████████████████████████████████████████████████▎                        | 290/376 [01:17<00:22,  3.86it/s]  Step 290/376 | Loss: 0.1537 | LR: 2.31e-05 | Grad: 0.99 | ETA: 3.7m
{'loss': 0.1537, 'grad_norm': 0.98828125, 'learning_rate': 2.3138297872340425e-05, 'epoch': 3.09}                                                  
 80%|██████████████████████████████████████████████████████████████████████████████████████▏                     | 300/376 [01:20<00:19,  3.89it/s]  Step 300/376 | Loss: 0.1547 | LR: 2.05e-05 | Grad: 0.71 | ETA: 3.2m
{'loss': 0.1547, 'grad_norm': 0.71484375, 'learning_rate': 2.047872340425532e-05, 'epoch': 3.19}                                                   
 82%|█████████████████████████████████████████████████████████████████████████████████████████                   | 310/376 [01:22<00:17,  3.70it/s]  Step 310/376 | Loss: 0.1517 | LR: 1.78e-05 | Grad: 0.76 | ETA: 2.8m
{'loss': 0.1517, 'grad_norm': 0.76171875, 'learning_rate': 1.7819148936170214e-05, 'epoch': 3.3}                                                   
 85%|███████████████████████████████████████████████████████████████████████████████████████████▉                | 320/376 [01:25<00:14,  3.80it/s]  Step 320/376 | Loss: 0.1546 | LR: 1.52e-05 | Grad: 0.88 | ETA: 2.4m
{'loss': 0.1546, 'grad_norm': 0.87890625, 'learning_rate': 1.5159574468085108e-05, 'epoch': 3.4}                                                   
 88%|██████████████████████████████████████████████████████████████████████████████████████████████▊             | 330/376 [01:28<00:12,  3.78it/s]  Step 330/376 | Loss: 0.1542 | LR: 1.25e-05 | Grad: 0.91 | ETA: 2.0m
{'loss': 0.1542, 'grad_norm': 0.90625, 'learning_rate': 1.25e-05, 'epoch': 3.51}                                                                   
 90%|█████████████████████████████████████████████████████████████████████████████████████████████████▋          | 340/376 [01:30<00:09,  3.81it/s]  Step 340/376 | Loss: 0.1548 | LR: 9.84e-06 | Grad: 0.53 | ETA: 1.5m
{'loss': 0.1548, 'grad_norm': 0.52734375, 'learning_rate': 9.840425531914895e-06, 'epoch': 3.62}                                                   
 93%|████████████████████████████████████████████████████████████████████████████████████████████████████▌       | 350/376 [01:33<00:06,  3.87it/s]  Step 350/376 | Loss: 0.1539 | LR: 7.18e-06 | Grad: 0.62 | ETA: 1.1m
{'loss': 0.1539, 'grad_norm': 0.6171875, 'learning_rate': 7.180851063829788e-06, 'epoch': 3.72}                                                    
 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████▍    | 360/376 [01:36<00:04,  3.50it/s]  Step 360/376 | Loss: 0.1503 | LR: 4.52e-06 | Grad: 0.96 | ETA: 41s
{'loss': 0.1503, 'grad_norm': 0.95703125, 'learning_rate': 4.521276595744681e-06, 'epoch': 3.83}                                                   
 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 370/376 [01:38<00:01,  3.72it/s]  Step 370/376 | Loss: 0.1535 | LR: 1.86e-06 | Grad: 0.89 | ETA: 15s
{'loss': 0.1535, 'grad_norm': 0.88671875, 'learning_rate': 1.8617021276595745e-06, 'epoch': 3.94}                                                  
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 376/376 [01:40<00:00,  3.65it/s]  Step 376/376 | ETA: 0s
{'train_runtime': 100.633, 'train_samples_per_second': 119.245, 'train_steps_per_second': 3.736, 'train_loss': 0.21056953944424364, 'epoch': 4.0}  
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 376/376 [01:40<00:00,  3.74it/s]

======================================================================
FINE-TUNING COMPLETE
======================================================================
  Duration: 1.7 minutes (101s)
  Final loss: 0.2106
  Total steps: 376
  Avg time/step: 0.27s
======================================================================


Reloading base model to preserve original weights...
Loading model: google/gemma-3-4b-it
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.20s/it]
Model loaded successfully

Saving models for future analysis...

Saving models for future analysis...
  Base model → outputs/saved_base_model
  Fine-tuned model → outputs/saved_finetuned_model
Models saved successfully!

Models saved to outputs/

======================================================================
COMPUTING TASK VECTOR
======================================================================
Task vector computed: ||T|| = 33.03
Computation time: 25.24s


======================================================================
LOSS LANDSCAPE SWEEP: L(M_base + αT)
======================================================================
Range: α ∈ [-0.5, 1.5]
Samples: 50
Started: 2025-10-31 11:58:03

Question: Does the loss curve cross L(M_base) at any α ≠ 0?

Cloning base model parameters...
Pre-loading task vector to device...
Computing base model loss...
Base model loss L(M_base): 2.9390

Sampling strategy: uniform
Generated 50 alpha values

[  1/50] (  2.0%) α = -0.500 | L(α)=9.6461, L(2α)=23.5726, |ΔL|=6.7071, |ΔL(2α)|=20.6336 | 14.6s | ETA: calculating...
[  2/50] (  4.0%) α = -0.459 | L(α)=8.9949, L(2α)=20.1975, |ΔL|=6.0559, |ΔL(2α)|=17.2584 | 14.3s | ETA: 12.0m
[  3/50] (  6.0%) α = -0.418 | L(α)=8.4461, L(2α)=17.3926, |ΔL|=5.5071, |ΔL(2α)|=14.4536 | 14.2s | ETA: 11.6m
[  4/50] (  8.0%) α = -0.378 | L(α)=7.8468, L(2α)=14.8593, |ΔL|=4.9078, |ΔL(2α)|=11.9202 | 9.0s | ETA: 11.3m
[  5/50] ( 10.0%) α = -0.337 | L(α)=7.2440, L(2α)=12.5440, |ΔL|=4.3050, |ΔL(2α)|=9.6050 | 8.4s | ETA: 10.0m
[  6/50] ( 12.0%) α = -0.296 | L(α)=6.9308, L(2α)=10.9742, |ΔL|=3.9917, |ΔL(2α)|=8.0352 | 8.2s | ETA: 9.1m
[  7/50] ( 14.0%) α = -0.255 | L(α)=6.6957, L(2α)=9.8665, |ΔL|=3.7567, |ΔL(2α)|=6.9275 | 11.3s | ETA: 8.4m
[  8/50] ( 16.0%) α = -0.214 | L(α)=6.3211, L(2α)=8.5571, |ΔL|=3.3821, |ΔL(2α)|=5.6180 | 13.6s | ETA: 8.2m
[  9/50] ( 18.0%) α = -0.173 | L(α)=6.2228, L(2α)=7.3542, |ΔL|=3.2838, |ΔL(2α)|=4.4151 | 14.3s | ETA: 8.2m
[ 10/50] ( 20.0%) α = -0.133 | L(α)=6.1154, L(2α)=6.7381, |ΔL|=3.1764, |ΔL(2α)|=3.7991 | 8.9s | ETA: 8.2m
[ 11/50] ( 22.0%) α = -0.092 | L(α)=4.7321, L(2α)=6.2294, |ΔL|=1.7930, |ΔL(2α)|=3.2903 | 8.4s | ETA: 7.8m
[ 12/50] ( 24.0%) α = -0.051 | L(α)=3.4629, L(2α)=5.3083, |ΔL|=0.5238, |ΔL(2α)|=2.3693 | 8.3s | ETA: 7.4m
[ 13/50] ( 26.0%) α = -0.010 | L(α)=3.1450, L(2α)=3.9279, |ΔL|=0.2059, |ΔL(2α)|=0.9889 | 8.3s | ETA: 7.0m
[ 14/50] ( 28.0%) α = +0.031 | L(α)=2.5582, L(2α)=2.4337, |ΔL|=0.3809, |ΔL(2α)|=0.5053 | 8.3s | ETA: 6.7m
[ 15/50] ( 30.0%) α = +0.071 | L(α)=2.4120, L(2α)=2.5492, |ΔL|=0.5270, |ΔL(2α)|=0.3898 | 8.3s | ETA: 6.4m
[ 16/50] ( 32.0%) α = +0.112 | L(α)=2.4514, L(2α)=2.7699, |ΔL|=0.4876, |ΔL(2α)|=0.1691 | 8.3s | ETA: 6.2m
[ 17/50] ( 34.0%) α = +0.153 | L(α)=2.5658, L(2α)=3.3319, |ΔL|=0.3733, |ΔL(2α)|=0.3929 | 8.3s | ETA: 5.9m
[ 18/50] ( 36.0%) α = +0.194 | L(α)=2.6611, L(2α)=3.9638, |ΔL|=0.2780, |ΔL(2α)|=1.0248 | 8.3s | ETA: 5.7m
[ 19/50] ( 38.0%) α = +0.235 | L(α)=2.8155, L(2α)=4.6788, |ΔL|=0.1236, |ΔL(2α)|=1.7398 | 8.3s | ETA: 5.4m
[ 20/50] ( 40.0%) α = +0.276 | L(α)=3.1039, L(2α)=5.5365, |ΔL|=0.1649, |ΔL(2α)|=2.5974 | 8.3s | ETA: 5.2m
[ 21/50] ( 42.0%) α = +0.316 | L(α)=3.4190, L(2α)=6.4266, |ΔL|=0.4800, |ΔL(2α)|=3.4875 | 8.3s | ETA: 5.0m
[ 22/50] ( 44.0%) α = +0.357 | L(α)=3.7041, L(2α)=7.2704, |ΔL|=0.7651, |ΔL(2α)|=4.3314 | 10.2s | ETA: 4.8m
[ 23/50] ( 46.0%) α = +0.398 | L(α)=4.0613, L(2α)=8.1259, |ΔL|=1.1223, |ΔL(2α)|=5.1868 | 10.7s | ETA: 4.6m
[ 24/50] ( 48.0%) α = +0.439 | L(α)=4.4127, L(2α)=8.9649, |ΔL|=1.4737, |ΔL(2α)|=6.0258 | 10.6s | ETA: 4.5m
[ 25/50] ( 50.0%) α = +0.480 | L(α)=4.7728, L(2α)=9.7882, |ΔL|=1.8337, |ΔL(2α)|=6.8492 | 8.2s | ETA: 4.3m
[ 26/50] ( 52.0%) α = +0.520 | L(α)=5.2326, L(2α)=10.5782, |ΔL|=2.2936, |ΔL(2α)|=7.6391 | 8.1s | ETA: 4.1m
[ 27/50] ( 54.0%) α = +0.561 | L(α)=5.6475, L(2α)=11.4453, |ΔL|=2.7084, |ΔL(2α)|=8.5062 | 8.1s | ETA: 3.9m
[ 28/50] ( 56.0%) α = +0.602 | L(α)=6.0575, L(2α)=12.3084, |ΔL|=3.1184, |ΔL(2α)|=9.3694 | 8.1s | ETA: 3.7m
[ 29/50] ( 58.0%) α = +0.643 | L(α)=6.5248, L(2α)=13.2376, |ΔL|=3.5858, |ΔL(2α)|=10.2985 | 8.2s | ETA: 3.6m
[ 30/50] ( 60.0%) α = +0.684 | L(α)=6.9349, L(2α)=14.1877, |ΔL|=3.9959, |ΔL(2α)|=11.2487 | 8.1s | ETA: 3.4m
[ 31/50] ( 62.0%) α = +0.724 | L(α)=7.3751, L(2α)=15.1720, |ΔL|=4.4361, |ΔL(2α)|=12.2329 | 8.1s | ETA: 3.2m
[ 32/50] ( 64.0%) α = +0.765 | L(α)=7.8148, L(2α)=16.2219, |ΔL|=4.8758, |ΔL(2α)|=13.2829 | 8.2s | ETA: 3.0m
[ 33/50] ( 66.0%) α = +0.806 | L(α)=8.2216, L(2α)=17.2476, |ΔL|=5.2826, |ΔL(2α)|=14.3086 | 8.1s | ETA: 2.9m
[ 34/50] ( 68.0%) α = +0.847 | L(α)=8.6552, L(2α)=18.3044, |ΔL|=5.7161, |ΔL(2α)|=15.3654 | 8.1s | ETA: 2.7m
[ 35/50] ( 70.0%) α = +0.888 | L(α)=9.0605, L(2α)=19.3640, |ΔL|=6.1215, |ΔL(2α)|=16.4249 | 8.1s | ETA: 2.5m
[ 36/50] ( 72.0%) α = +0.929 | L(α)=9.4827, L(2α)=20.4303, |ΔL|=6.5437, |ΔL(2α)|=17.4913 | 8.1s | ETA: 2.3m
[ 37/50] ( 74.0%) α = +0.969 | L(α)=9.9066, L(2α)=21.4882, |ΔL|=6.9676, |ΔL(2α)|=18.5491 | 8.6s | ETA: 2.2m
[ 38/50] ( 76.0%) α = +1.010 | L(α)=10.2473, L(2α)=22.4106, |ΔL|=7.3083, |ΔL(2α)|=19.4716 | 8.2s | ETA: 2.0m
[ 39/50] ( 78.0%) α = +1.051 | L(α)=10.6871, L(2α)=23.3765, |ΔL|=7.7481, |ΔL(2α)|=20.4374 | 8.1s | ETA: 1.9m
[ 40/50] ( 80.0%) α = +1.092 | L(α)=11.1102, L(2α)=24.2516, |ΔL|=8.1711, |ΔL(2α)|=21.3126 | 8.7s | ETA: 1.7m
[ 41/50] ( 82.0%) α = +1.133 | L(α)=11.5523, L(2α)=25.0632, |ΔL|=8.6133, |ΔL(2α)|=22.1242 | 8.3s | ETA: 1.5m
[ 42/50] ( 84.0%) α = +1.173 | L(α)=11.9911, L(2α)=25.7895, |ΔL|=9.0521, |ΔL(2α)|=22.8504 | 8.6s | ETA: 1.4m
[ 43/50] ( 86.0%) α = +1.214 | L(α)=12.4204, L(2α)=26.4235, |ΔL|=9.4813, |ΔL(2α)|=23.4845 | 8.2s | ETA: 1.2m
[ 44/50] ( 88.0%) α = +1.255 | L(α)=12.9073, L(2α)=26.9952, |ΔL|=9.9682, |ΔL(2α)|=24.0562 | 8.6s | ETA: 1.1m
[ 45/50] ( 90.0%) α = +1.296 | L(α)=13.3448, L(2α)=27.5738, |ΔL|=10.4058, |ΔL(2α)|=24.6347 | 12.9s | ETA: 55s
[ 46/50] ( 92.0%) α = +1.337 | L(α)=13.8155, L(2α)=28.0670, |ΔL|=10.8765, |ΔL(2α)|=25.1279 | 10.0s | ETA: 46s
[ 47/50] ( 94.0%) α = +1.378 | L(α)=14.3010, L(2α)=28.4210, |ΔL|=11.3619, |ΔL(2α)|=25.4819 | 8.2s | ETA: 37s
[ 48/50] ( 96.0%) α = +1.418 | L(α)=14.8028, L(2α)=28.7618, |ΔL|=11.8637, |ΔL(2α)|=25.8228 | 8.2s | ETA: 28s
[ 49/50] ( 98.0%) α = +1.459 | L(α)=15.2964, L(2α)=29.1046, |ΔL|=12.3574, |ΔL(2α)|=26.1656 | 8.1s | ETA: 18s
[ 50/50] (100.0%) α = +1.500 | L(α)=15.8208, L(2α)=29.4923, |ΔL|=12.8817, |ΔL(2α)|=26.5533 | 8.2s | ETA: 9s

Restoring base model parameters...

======================================================================
ALPHA SWEEP COMPLETE
======================================================================
  Duration: 7.8 minutes (466s)
  Samples Completed: 50/50
  Avg time/sample: 9.19s
======================================================================


======================================================================
LOSS LANDSCAPE ANALYSIS
======================================================================

Minimum General Loss (best general knowledge):
  α = +0.0714
  L(α) = 2.4120
  L(M_base) = 2.9390
  Δ = -0.5270

Minimum Task-Specific Loss (best task performance):
  α = +0.2755
  Task L(α) = 1.5128
  General L(α) = 3.1039
  Δ from base = -1.4263

Best Functional Return (smallest |L(α) - L_base|):
  1. α = +0.2347, |ΔL| = 0.123576
  2. α = +0.2755, |ΔL| = 0.164889
  3. α = -0.0102, |ΔL| = 0.205918
  4. α = +0.1939, |ΔL| = 0.277956
  5. α = +0.1531, |ΔL| = 0.373258

Zero-Crossings (where L(α) ≈ L_base for α ≠ 0):
  Found 1 crossing(s):
  1. α = +0.2347, L(α) = 2.8155, |ΔL| = 0.123576 ★

======================================================================
SQUARING TEST ANALYSIS: [W(λ)]² = I Analog
======================================================================

Squaring Return Points (where L(2α) ≈ L_base for α ≠ 0):
  No squaring return points found (threshold: |ΔL(2α)| < 0.15)
  → Neural loss landscapes do not exhibit self-inverse property under doubling
  → Unlike rotation groups, no [W(λ)]² = I analog detected
