# SITV Experiment Configuration - Self-Inverse Task Vector experiments

# Model Configuration
model:
  name: "google/gemma-3-4b-it" # Options: google/gemma-3-4b-it, Qwen/Qwen2.5-7B-Instruct, Qwen/Qwen2.5-14B-Instruct
  device: null # null = auto-detect (cuda/mps/cpu)
  # Supported models:
  # - google/gemma-3-4b-it (4B params, early 2025) - Fast, previously tested
  # - Qwen/Qwen2.5-7B-Instruct (7B params, Oct 2024) - SOTA, recommended for comparison
  # - Qwen/Qwen2.5-14B-Instruct (14B params, Oct 2024) - Best quality, needs ~32GB VRAM
  #
  # Model-specific notes:
  # Qwen models: Use chat_template for proper formatting, excellent multilingual support
  # Gemma models: Instruction-tuned variant recommended (_it suffix)

# Task Configuration
task:
  name: "sentiment_positive" # Options: sentiment_positive, sentiment_negative, instruction_following, qa_factual

# Evaluation Configuration
evaluation:
  general_dataset: "combined" # Options: mixed_domain, wikitext, coding, common_knowledge, combined
  # General evaluation measures how task vectors affect broad language modeling capability
  # Task-specific evaluation measures performance on the trained task
  # Use "combined" to evaluate on all general datasets together (120 examples total)

  # Performance Optimization (NEW)
  batch_size: 8 # Number of texts per forward pass (8-16 recommended, higher = faster but more memory)
  enable_mixed_precision: true # Use FP16/BF16 for 1.5-2x speedup (BF16 auto-selected on Ampere+ GPUs)
  max_length: 1024 # Maximum sequence length for evaluation tokenization

# Output Configuration
output:
  dir: "outputs"
  analysis_only: false # Set true to skip fine-tuning and load saved models

# Fine-Tuning Configuration
fine_tuning:
  num_epochs: 4 # More epochs for better convergence
  learning_rate: 1.0e-4 # 0.0001
  batch_size: 32 # H200 can handle much larger batches
  max_length: 1024 # Longer sequences for better context
  data_repetition_factor: 100 # Repeat dataset to increase effective size
  save_strategy: "no" # Options: "no", "steps", "epoch"
  logging_steps: 10

# Alpha Sweep Configuration - M(α) = M_base + α·T where T = M_finetuned - M_base
# α interpretation: 0=base, 1=finetuned, >1=amplified, <0=opposite direction
# Range recommendations: [-3,3]=full symmetry analysis | [-0.5,3]=RECOMMENDED (40% faster) | [0,3]=practical (50% faster)
alpha_sweep:
  alpha_min: -0.5 # For fast iteration, try -0.5 or 0
  alpha_max: 1.5 # Positive range is where most interesting effects occur
  num_samples: 100 # H200 power - ultra high resolution curves
  enable_squaring_test: true # Test M(2α) for self-inverse properties
  threshold: 0.15 # Zero-crossing detection threshold
  sampling_strategy: "adaptive" # Sampling Strategy: "uniform" (100% samples) | "adaptive" (40-60%, multi-resolution) | "bayesian" (10-20%, GP-based, 80-90% speedup!)
  adaptive_coarse_samples: 60
  # adaptive_refine_factor: 3
  # adaptive_curvature_threshold: 0.5
  # bayesian_n_initial: 10 | bayesian_acquisition: "ei" | NOTE: Requires `pip install -e ".[bayesian]"`

  # Gradient Analysis: Auto-compute dL/dα and find critical points (minima, maxima, inflection)
  # enable_gradient_analysis: false | gradient_smooth_sigma: 0.5 | gradient_threshold: 0.01 | curvature_threshold: 0.001

# 2D Composition Configuration (Multi-Task Interaction)
composition_2d:
  enable: true # Set true to run 2D composition experiment
  alpha_min: -2.0
  alpha_max: 2.0
  beta_min: -2.0
  beta_max: 2.0
  num_samples_per_dim: 30 # 60×60 = 3,600 evaluations - publication quality heatmap

# Hardware Optimization & Use Case Settings
# Current (H200): batch_size=32, max_length=1024, num_samples=200, data_repetition=100×6epochs, 2D=60×60
# Smaller GPUs (3090/4090): batch_size=16, max_length=512, num_samples=150, data_repetition=100×2epochs, 2D=30×30
#
# Speed optimization (for development):
# • Alpha range: [-0.5,3] or [0,3] saves 40-50% | composition_2d.enable=false skips 2D sweep
# • Sampling: adaptive saves 40-60% | bayesian saves 80-90% (best for finding optimal α fast)
#
# Use case presets:
# • Pure research: α∈[-3,3], 300 samples, uniform | Research+speed: α∈[-0.5,3], 200, adaptive (RECOMMENDED)
# • Practical: α∈[0,2], 150, adaptive | Quick test: α∈[0,1.5], 50, bayesian | Optimal α search: any range, 30-50, bayesian
