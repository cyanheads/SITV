# SITV Experiment Configuration
# Configuration for Self-Inverse Task Vector experiments
# H200-OPTIMIZED: High-resolution research configuration
# Hardware: H200 SXM (141GB VRAM, 188GB RAM, 12 vCPU)

# Model Configuration
model:
  name: "google/gemma-3-4b-it"
  device: null # null = auto-detect (cuda/mps/cpu)

# Task Configuration
task:
  name: "sentiment_positive" # Options: sentiment_positive, sentiment_negative, instruction_following, qa_factual

# Evaluation Configuration
evaluation:
  general_dataset: "mixed_domain" # Options: mixed_domain, wikitext, coding, common_knowledge
  # General evaluation measures how task vectors affect broad language modeling capability
  # Task-specific evaluation measures performance on the trained task

# Output Configuration
output:
  dir: "outputs"
  analysis_only: false # Set true to skip fine-tuning and load saved models

# Fine-Tuning Configuration
fine_tuning:
  num_epochs: 6 # More epochs for better convergence
  learning_rate: 5.0e-5 # 0.00005
  batch_size: 128 # H200 can handle much larger batches
  max_length: 1024 # Longer sequences for better context
  data_repetition_factor: 25 # Reduced repetition, more unique passes via epochs
  save_strategy: "no" # Options: "no", "steps", "epoch"
  logging_steps: 10

# Alpha Sweep Configuration (1D Loss Landscape)
alpha_sweep:
  alpha_min: -3.0
  alpha_max: 3.0
  num_samples: 300 # H200 power - ultra high resolution curves
  enable_squaring_test: true # Test M(2α) for self-inverse properties
  threshold: 0.1 # Zero-crossing detection threshold

# 2D Composition Configuration (Multi-Task Interaction)
composition_2d:
  enable: true # Set true to run 2D composition experiment
  alpha_min: -2.0
  alpha_max: 2.0
  beta_min: -2.0
  beta_max: 2.0
  num_samples_per_dim: 60 # 60×60 = 3,600 evaluations - publication quality heatmap

# Hardware Optimization Notes (H200 Optimized):
# - Batch size 128: Fully utilizing 141GB VRAM
# - Max length 1024: Extended context understanding
# - Alpha samples 300: Ultra high resolution loss curves
# - 2D samples 60×60: Publication quality heatmaps (3,600 points)
# - Data repetition 25× with 6 epochs: Better training diversity
#
# For smaller GPUs (e.g., RTX 3090, 4090), reduce:
# - batch_size to 16-32
# - max_length to 512
# - num_samples to 150
# - num_samples_per_dim to 30
# - data_repetition_factor to 100, num_epochs to 2
