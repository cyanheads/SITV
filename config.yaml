# SITV Experiment Configuration - Self-Inverse Task Vector experiments

# Model Configuration
model:
  name: "google/gemma-3-4b-it" # Options: google/gemma-3-4b-it
  device: null # null = auto-detect (cuda/mps/cpu)
  # Supported models:
  # - google/gemma-3-4b-it (4B params, early 2025) - Fast, previously tested
  #
  # Model-specific notes:
  # Gemma models: Instruction-tuned variant recommended (_it suffix)

# Task Configuration
task:
  name: "sentiment_positive" # Options: sentiment_positive, sentiment_negative, instruction_following, qa_factual

# Evaluation Configuration
evaluation:
  general_dataset: "combined" # Options: mixed_domain, wikitext, coding, common_knowledge, combined
  # General evaluation measures how task vectors affect broad language modeling capability
  # Task-specific evaluation measures performance on the trained task
  # Use "combined" to evaluate on all general datasets together (120 examples total)

  # Performance Optimization (NEW)
  batch_size: 32 # Number of texts per forward pass (8-16 recommended, higher = faster but more memory)
  enable_mixed_precision: true # Use FP16/BF16 for 1.5-2x speedup (BF16 auto-selected on Ampere+ GPUs)
  max_length: 1024 # Maximum sequence length for evaluation tokenization

# Output Configuration
output:
  dir: "outputs"
  analysis_only: false # Set true to skip fine-tuning and load saved models

# Fine-Tuning Configuration
fine_tuning:
  num_epochs: 4 # More epochs for better convergence
  learning_rate: 1.0e-4 # 0.0001
  batch_size: 32 # H200 can handle much larger batches
  max_length: 1024 # Longer sequences for better context
  data_repetition_factor: 100 # Repeat dataset to increase effective size
  save_strategy: "no" # Options: "no", "steps", "epoch"
  logging_steps: 10

# Alpha Sweep Configuration - M(α) = M_base + α·T where T = M_finetuned - M_base
# α interpretation: 0=base, 1=finetuned, >1=amplified, <0=opposite direction
# Range recommendations: [-3,3]=full symmetry analysis | [-0.5,3]=RECOMMENDED (40% faster) | [0,3]=practical (50% faster)
alpha_sweep:
  alpha_min: -0.5 # For fast iteration, try -0.5 or 0
  alpha_max: 1.5 # Positive range is where most interesting effects occur
  num_samples: 100 # H200 power - ultra high resolution curves
  enable_squaring_test: true # Test M(2α) for self-inverse properties
  threshold: 0.20 # Zero-crossing detection threshold
  sampling_strategy: "uniform" # Sampling Strategy: "uniform" (100% samples) | "adaptive" (40-60%, multi-resolution) | "bayesian" (10-20%, GP-based, 80-90% speedup!)
  # adaptive_coarse_samples: 60
  # adaptive_refine_factor: 3
  # adaptive_curvature_threshold: 0.5
  # bayesian_n_initial: 10 | bayesian_acquisition: "ei" | NOTE: Requires `pip install -e ".[bayesian]"`

  # Gradient Analysis: Auto-compute dL/dα and find critical points (minima, maxima, inflection)
  # enable_gradient_analysis: false | gradient_smooth_sigma: 0.5 | gradient_threshold: 0.01 | curvature_threshold: 0.001

# 2D Composition Configuration (Multi-Task Interaction)
composition_2d:
  enable: true # Set true to run 2D composition experiment
  alpha_min: -2.0
  alpha_max: 2.0
  beta_min: -2.0
  beta_max: 2.0
  num_samples_per_dim: 30 # 60×60 = 3,600 evaluations - publication quality heatmap
  enable_analysis: true # Auto-run composition analysis after 2D sweep (interaction detection, predictions)

# 3D Composition Configuration (Three-Task Interaction)
# Explores L(M_base + α·T1 + β·T2 + γ·T3)
composition_3d:
  enable: true # Set true to run 3D composition experiment (WARNING: computationally expensive!)
  task_1: "sentiment_negative" # First task vector
  task_2: "sentiment_positive" # Second task vector
  task_3: "instruction_following" # Third task vector
  alpha_min: -1.0
  alpha_max: 1.0
  beta_min: -1.0
  beta_max: 1.0
  gamma_min: -1.0
  gamma_max: 1.0
  num_samples_per_dim: 10 # 10×10×10 = 1,000 evaluations (CAREFUL: scales cubically!)
  # Recommended settings:
  # - Quick test: 5×5×5 = 125 evaluations (~ 10-20 min)
  # - Standard: 10×10×10 = 1,000 evaluations (~ 1-3 hours)
  # - High-res: 20×20×20 = 8,000 evaluations (~ 8-24 hours)

# Riemannian Geometry Configuration (TRUE GEOMETRIC STRUCTURE DETECTION)
# Enables proper Riemannian geometry on parameter manifolds to detect curvature and structure
# Based on Fisher Information Matrix as metric tensor with varying-metric geodesics
#
# NEW CAPABILITIES (v0.13.0):
# - Finite difference Christoffel symbol computation (detects metric variation)
# - Proper geodesic equation integration with acceleration term
# - Metric recomputation every N steps along geodesic path
# - Can now detect if neural network parameter spaces are curved!
#
# RESEARCH QUESTION: Do task vector paths curve in Fisher geometry?
# This configuration will help answer whether neural networks have rich geometric structure
# like an "anthill" with intrinsic curvature, or are approximately flat Euclidean spaces.
geometry:
  enabled: true # Set true to enable Riemannian geometry features
  metric_type: "fisher_diagonal" # Options: euclidean | fisher_diagonal | fisher_kfac | fisher_full
  cache_metric: false # MUST be false when recompute_metric_every > 0 (to allow varying metric)
  parallel_transport: false # Use parallel transport for task vectors (experimental, not fully tested)

  # Fisher Information Matrix Approximation
  fisher_approximation:
    sampling_strategy: "subset" # Options: full | subset | batch
    num_samples: 1000 # Number of samples for FIM computation (if subset)
    block_size: 256 # Block size for KFAC approximation
    eigenvalue_floor: 1.0e-6 # Regularization for numerical stability

  # Geodesic Integration (uses geodesics instead of straight lines)
  geodesic_integration:
    enabled: true # Use geodesic interpolation M_base + αT → exp_M(α·v)
    num_steps: 20 # Runge-Kutta integration steps (reduced from 100 for speed)
    tolerance: 1.0e-6 # Integration error tolerance
    step_size_control: false # Adaptive step size (slower but more accurate)
    max_iterations: 1000 # Maximum solver iterations
    recompute_metric_every: 5 # Recompute Fisher/Christoffel every N steps (0=never, recommended: 4-10)
    metric_epsilon: 1.0e-3 # Finite difference epsilon for Christoffel computation

    # COMPUTATIONAL COST with recompute_metric_every > 0:
    # - Each metric recomputation requires full Fisher computation (~2-3s for 4B model)
    # - With num_steps=20, recompute_every=5 → 4 recomputations per alpha
    # - Expected time per alpha: 10-15 seconds (vs 2s without recomputation)
    # - For 100 alphas: ~20-25 minutes (vs 3 minutes without recomputation)
    #
    # VALIDATION TEST SETTINGS (gemma-2-2b-it):
    # - num_steps: 10
    # - recompute_every: 2
    # - num_samples: 10 alphas only
    # - Expected: ~5-10 minutes total
    #
    # If Christoffel symbols are non-zero → CURVATURE DETECTED!
    # If geodesic deviates from straight line → GEOMETRIC STRUCTURE EXISTS!

  # Christoffel Symbol Computation (curvature detection)
  # v0.14.2 - Optimized parameter filtering for faster curvature detection
  christoffel_computation:
    skip_vision_tower: true # Skip vision_tower parameters (not modified during text fine-tuning)
    skip_frozen: true # Skip parameters where requires_grad=False
    num_samples: 30 # Number of data samples for Fisher computation (balanced speed/quality)
    parameter_sample_fraction: 1.0 # Fraction of parameters to compute (1.0 = all, 0.1 = 10%)
    max_parameters: null # Maximum number of parameters to process (null = unlimited)

    # PERFORMANCE NOTES:
    # - skip_vision_tower: true reduces computation from ~21 min to ~2-3 min (90% speedup!)
    # - skip_frozen: true filters out frozen parameters
    # - num_samples: 30 balances speed and quality (20=fast/noisy, 50=research-quality, 100=original)
    # - parameter_sample_fraction: 0.1 samples 10% of parameters for statistical curvature detection
    # - max_parameters: 50 limits computation to first 50 parameters for quick testing

  # Symmetry Analysis (detect rotation/permutation/scaling symmetries)
  # v0.14.0 - Fully implemented with quotient space projection
  symmetry_analysis:
    enabled: true # Perform symmetry group detection
    detect_rotations: true # Test for rotation invariance
    detect_permutations: true # Test for neuron permutation symmetries
    detect_scaling: true # Test for layer-wise scaling symmetries
    quotient_space: true # Work in canonical parameter space (removes redundancy)
    symmetry_tolerance: 0.01 # Tolerance for L(R·θ) ≈ L(θ)

  # Curvature Analysis (compute sectional/Ricci/scalar curvature)
  # v0.13.0 - Fully implemented with statistical estimation
  curvature_analysis:
    enabled: true # Compute Riemannian curvature tensors
    compute_sectional: true # Sectional curvature K(X,Y) (fast, recommended)
    compute_ricci: false # Ricci curvature (expensive, optional)
    compute_scalar: false # Scalar curvature (expensive, optional)
    num_tangent_samples: 10 # Random tangent vectors for curvature estimation

# Hardware Optimization & Use Case Settings
# Current (H200): batch_size=32, max_length=1024, num_samples=200, data_repetition=100×6epochs, 2D=60×60
# Smaller GPUs (3090/4090): batch_size=16, max_length=512, num_samples=150, data_repetition=100×2epochs, 2D=30×30
#
# Speed optimization (for development):
# • Alpha range: [-0.5,3] or [0,3] saves 40-50% | composition_2d.enable=false skips 2D sweep
# • Sampling: adaptive saves 40-60% | bayesian saves 80-90% (best for finding optimal α fast)
#
# Use case presets:
# • Pure research: α∈[-3,3], 300 samples, uniform | Research+speed: α∈[-0.5,3], 200, adaptive (RECOMMENDED)
# • Practical: α∈[0,2], 150, adaptive | Quick test: α∈[0,1.5], 50, bayesian | Optimal α search: any range, 30-50, bayesian
