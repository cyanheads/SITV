# SITV Experiment Configuration - Self-Inverse Task Vector experiments

# Model Configuration
model:
  name: "google/gemma-3-4b-it" # Options: google/gemma-3-4b-it
  device: null # null = auto-detect (cuda/mps/cpu)
  # Supported models:
  # - google/gemma-3-4b-it (4B params, early 2025) - Fast, previously tested
  #
  # Model-specific notes:
  # Gemma models: Instruction-tuned variant recommended (_it suffix)

# Task Configuration
task:
  name: "sentiment_negative" # Options: sentiment_positive, sentiment_negative, instruction_following, qa_factual

# Evaluation Configuration
evaluation:
  general_dataset: "combined" # Options: mixed_domain, wikitext, coding, common_knowledge, combined
  # General evaluation measures how task vectors affect broad language modeling capability
  # Task-specific evaluation measures performance on the trained task
  # Use "combined" to evaluate on all general datasets together (120 examples total)

  # Performance Optimization (NEW)
  batch_size: 32 # Number of texts per forward pass (8-16 recommended, higher = faster but more memory)
  enable_mixed_precision: true # Use FP16/BF16 for 1.5-2x speedup (BF16 auto-selected on Ampere+ GPUs)
  max_length: 1024 # Maximum sequence length for evaluation tokenization

# Output Configuration
output:
  dir: "outputs"
  analysis_only: false # Set true to skip fine-tuning and load saved models

# Fine-Tuning Configuration
fine_tuning:
  num_epochs: 4 # More epochs for better convergence
  learning_rate: 1.0e-4 # 0.0001
  batch_size: 32 # H200 can handle much larger batches
  max_length: 1024 # Longer sequences for better context
  data_repetition_factor: 100 # Repeat dataset to increase effective size
  save_strategy: "no" # Options: "no", "steps", "epoch"
  logging_steps: 10

# Alpha Sweep Configuration - M(α) = M_base + α·T where T = M_finetuned - M_base
# α interpretation: 0=base, 1=finetuned, >1=amplified, <0=opposite direction
# Range recommendations: [-3,3]=full symmetry analysis | [-0.5,3]=RECOMMENDED (40% faster) | [0,3]=practical (50% faster)
alpha_sweep:
  alpha_min: -0.5 # For fast iteration, try -0.5 or 0
  alpha_max: 1.5 # Positive range is where most interesting effects occur
  num_samples: 100 # H200 power - ultra high resolution curves
  enable_squaring_test: true # Test M(2α) for self-inverse properties
  threshold: 0.20 # Zero-crossing detection threshold
  sampling_strategy: "uniform" # Sampling Strategy: "uniform" (100% samples) | "adaptive" (40-60%, multi-resolution) | "bayesian" (10-20%, GP-based, 80-90% speedup!)
  # adaptive_coarse_samples: 60
  # adaptive_refine_factor: 3
  # adaptive_curvature_threshold: 0.5
  # bayesian_n_initial: 10 | bayesian_acquisition: "ei" | NOTE: Requires `pip install -e ".[bayesian]"`

  # Gradient Analysis: Auto-compute dL/dα and find critical points (minima, maxima, inflection)
  # enable_gradient_analysis: false | gradient_smooth_sigma: 0.5 | gradient_threshold: 0.01 | curvature_threshold: 0.001

# 2D Composition Configuration (Multi-Task Interaction)
composition_2d:
  enable: true # Set true to run 2D composition experiment
  alpha_min: -2.0
  alpha_max: 2.0
  beta_min: -2.0
  beta_max: 2.0
  num_samples_per_dim: 30 # 60×60 = 3,600 evaluations - publication quality heatmap

# Riemannian Geometry Configuration (NEW - Experimental)
# Enables proper Riemannian geometry on parameter manifolds instead of Euclidean vector operations
# Based on Fisher Information Matrix as metric tensor
geometry:
  enabled: true # Set true to enable Riemannian geometry features
  metric_type: "fisher_diagonal" # Options: euclidean | fisher_diagonal | fisher_kfac | fisher_full
  cache_metric: true # Cache Fisher matrix for reuse (recommended)
  parallel_transport: false # Use parallel transport for task vectors (experimental, not fully tested)

  # Fisher Information Matrix Approximation
  fisher_approximation:
    sampling_strategy: "subset" # Options: full | subset | batch
    num_samples: 1000 # Number of samples for FIM computation (if subset)
    block_size: 256 # Block size for KFAC approximation
    eigenvalue_floor: 1.0e-6 # Regularization for numerical stability

  # Geodesic Integration (uses geodesics instead of straight lines)
  geodesic_integration:
    enabled: true # Use geodesic interpolation M_base + αT → exp_M(α·v)
    num_steps: 100 # Runge-Kutta integration steps
    tolerance: 1.0e-6 # Integration error tolerance
    step_size_control: false # Adaptive step size (slower but more accurate)
    max_iterations: 1000 # Maximum solver iterations

  # Symmetry Analysis (detect rotation/permutation/scaling symmetries)
  # NOTE: Phase 4 - Not yet implemented
  symmetry_analysis:
    enabled: false # Perform symmetry group detection (NOT IMPLEMENTED YET)
    detect_rotations: true # Test for rotation invariance
    detect_permutations: true # Test for neuron permutation symmetries
    detect_scaling: true # Test for layer-wise scaling symmetries
    quotient_space: false # Work in canonical parameter space
    symmetry_tolerance: 0.01 # Tolerance for L(R·θ) ≈ L(θ)

  # Curvature Analysis (compute sectional/Ricci/scalar curvature)
  # NOTE: Phase 3 - Not yet implemented
  curvature_analysis:
    enabled: false # Compute Riemannian curvature tensors (NOT IMPLEMENTED YET)
    compute_sectional: true # Sectional curvature K(X,Y)
    compute_ricci: false # Ricci curvature (expensive)
    compute_scalar: false # Scalar curvature (expensive)
    num_tangent_samples: 10 # Random tangent vectors for curvature estimation

# Hardware Optimization & Use Case Settings
# Current (H200): batch_size=32, max_length=1024, num_samples=200, data_repetition=100×6epochs, 2D=60×60
# Smaller GPUs (3090/4090): batch_size=16, max_length=512, num_samples=150, data_repetition=100×2epochs, 2D=30×30
#
# Speed optimization (for development):
# • Alpha range: [-0.5,3] or [0,3] saves 40-50% | composition_2d.enable=false skips 2D sweep
# • Sampling: adaptive saves 40-60% | bayesian saves 80-90% (best for finding optimal α fast)
#
# Use case presets:
# • Pure research: α∈[-3,3], 300 samples, uniform | Research+speed: α∈[-0.5,3], 200, adaptive (RECOMMENDED)
# • Practical: α∈[0,2], 150, adaptive | Quick test: α∈[0,1.5], 50, bayesian | Optimal α search: any range, 30-50, bayesian
